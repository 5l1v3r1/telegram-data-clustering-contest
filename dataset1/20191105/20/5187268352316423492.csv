,src,lang,text
0,"











Investigadores atacam a Siri, Alexa e Google Home com raios laser
05 Nov 2019, 20:00 by Vítor M.
Apple, Google e Amazon têm no mercado colunas inteligentes que permitem executar várias ações, como abrir portas, acender luzes, ligar carros e muito mais. Estas colunas estão equipadas com microfones MEMS, que respondem à luz como se fosse som. Nesse sentido, e ainda desconhecendo a raiz da vulnerabilidade, investigadores dispararam lasers contra estas colunas e estas agiram como se estivessem a receber comandos de voz.
O assunto pode ser grave, porque facilmente um destes assistentes pode abrir portas de casas, fazer compras por sua ordem, estando a ser atacadas.





Colunas inteligentes da Apple, Google e Amazon vulneráveis aos lasers
A Siri, Alexa e Google Assistant são vulneráveis a ataques que usam lasers para injetar impulso inaudíveis. Além disso, os ataques podem mesmo ser no formato de comandos invisíveis nos dispositivos. Como resultado, estes assistentes virtuais podem abrir a porta de casa, abrir e ligar um carro, ligar as luzes ou até fazer visitas a sites sem que o proprietário autorize.
Segundo um grupo de investigadores, o ataque chama-se Dubbed Light Commands. Este funciona como ataque a vários serviços e dispositivos, incluindo contra o Facebook Portal.





Apesar de ainda não estar um “ataque apurado”, podem ser usados laser de baixa potência nestes sistemas ativados por voz, o que permite aos atacantes injetar comandos. Com efeito, os comandos podem ser mesmo disparados estando o atacante a uma distância de até 110 metros.
A importância destas colunas terem autenticação
Estes sistemas, por vezes, e em certos serviços, não exigem que os utilizadores se autentiquem. Dessa forma, o ataque pode ser realizado com frequência sem a necessidade de uma palavra-passe ou PIN.
Mesmo quando os sistemas exigem autenticação para certas ações, pode ser viável forçar o PIN, uma vez que muitos dispositivos não limitam o número de tentativas que um utilizador pode fazer. Entre outras coisas, os comandos baseados em luz podem ser enviados de um prédio para outro. Se o dispositivo estiver perto de uma janela, este pode ser alcançável.





O ataque explora uma vulnerabilidade em microfones que usam sistemas microeletromecânicos, ou MEMS. Os componentes microscópicos MEMS desses microfones respondem à luz sem querer, como se fosse som.
Apesar dos investigadores só terem testado o ataque em dispositivos com a Siri, Alexa, Google Assistant, Facebook Portal e um pequeno número de tablets e telefones, estes acreditam que todos os dispositivos que usam microfones MEMS são suscetíveis a ataques de comandos de luz.



Ataques por laser eficazes, mas com condicionantes
Os ataques baseados em laser têm várias limitações. Por um lado, o atacante deve ter linha de visão direta para o dispositivo alvo. Além disso, a luz, em muitos casos, tem de ser precisamente direcionada a uma parte muito específica do microfone.
Exceto nos casos em que um atacante usa um laser infravermelho, as luzes também são fáceis de ver por alguém que está por perto e tem linha de visão do dispositivo. Os dispositivos normalmente respondem com voz e sinais visuais ao executar um comando, um recurso que alertaria os utilizadores dentro do alcance de ouvido do dispositivo.





Apesar dessas restrições, os resultados são importantes por uma série de razões. A investigação não só apresenta um novo modo de ataque contra sistemas controlados por voz, como também mostra como realizar os ataques em ambientes semirrealistas. Além disso, os investigadores ainda não compreendem totalmente a física por trás da sua descoberta. Dessa forma, quando perceberem como realmente funcionam, poderão apurar mais o ataque.



Serviços sensíveis nas colunas não exigem autenticação
Por último, a investigação destaca os riscos que resultam quando dispositivos de controlo de voz e os periféricos aos quais eles se ligam realizam comandos confidenciais sem exigir uma palavra-passe ou PIN.
Descobrimos que os sistemas de comando de voz muitas vezes não possuem mecanismos de autenticação do utilizador, ou se os mecanismos estão presentes, eles são implementados incorretamente (por exemplo, permitindo a força bruta de PIN). Mostramos como um atacante pode usar comandos de voz com injeção de luz para desbloquear a porta dianteira protegida com fechadura inteligente, abrir portas de garagem, fazer compras em sites de comércio eletrónico à custa da vítima, ou mesmo localizar, desbloquear e iniciar vários veículos (por exemplo, Tesla e Ford) se os veículos estiverem ligados à conta Google do utilizador atacado.
Escreveram os investigadores num artigo intitulado Comandos de luz: Ataques de injeção de áudio baseados em laser em sistemas controlados por voz.



Instrumentos laser baratos podem causar grandes danos
O artigo descreve diferentes configurações usadas para realizar os ataques. Um deles é composto de um ponteiro laser simples (preço de 18 dólares por 3 unidades). Além desse, usaram também um driver laser Wavelength Electronics LD5CHA (de 339 dólares) e um amplificador de áudio Neoteck NTK059. Apesar de certas limitações, podem até usar outras lentes para focar o laser nos ataques de longo alcance.



Outro aparelho usou um laser infravermelho que é invisível ao olho humano para ataques mais furtivos. Uma terceira configuração contou com uma lanterna de fósforo excitado a laser Acebeam W30, de 500 lumens,para eliminar a necessidade de apontar com precisão uma luz numa parte específica de um microfone MEMS.
Num dos ataques, foi injetado com sucesso um comando através de uma janela de vidro. Esta estava a cerca de 70 metros de distância.



Nessa experiência, uma coluna foi posicionada próximo de uma janela no quarto andar de um prédio. O laser do atacante foi colocado numa plataforma dentro de uma torre próxima, localizada a cerca de 1,5 metros acima do nível do solo. O laser disparou um raio sobre o Google Home, que tem apenas microfones voltados para cima.
Numa experiência diferente, os investigadores usaram uma lente telefoto para focar o laser e atacar com sucesso um dispositivo a cerca de 100 metros de distância.
Fonte: Arstechnica


",pt,"











Researchers attack Siri, Alexa and Google Home with laser beams
05 Nov 2019, 20:00 by Vitor M.
Apple, Google and Amazon have smart speakers on the market that allow you to perform various actions, such as opening doors, turning on lights, starting cars, and more. These speakers are equipped with MEMS microphones, which respond to light as if it were sound. In this sense, while still unaware of the root of the vulnerability, investigators fired lasers at these columns and acted as though they were receiving voice commands.
The matter can be serious, because easily one of these assistants can open doors of houses, make purchases on your order, being attacked.





Smart columns from Apple, Google and Amazon vulnerable to lasers
Siri, Alexa and Google Assistant are vulnerable to attacks that use lasers to inject inaudible impulses. In addition, attacks can even be in the form of invisible commands on devices. As a result, these virtual assistants can open the door of the house, open and start a car, turn on the lights or even make website visits without the owner's permission.
According to a group of investigators, the attack is called Dubbed Light Commands. This acts as an attack on various services and devices, including against Facebook Portal.





Although not yet a “fine attack”, low power lasers can be used in these voice-activated systems, which allows attackers to inject commands. Indeed, the commands can even be fired from the attacker at a distance of up to 110 meters.
The importance of these columns having authentication
These systems sometimes and in certain services do not require users to authenticate. This way, the attack can often be performed without the need for a password or PIN.
Even when systems require authentication for certain actions, it may be feasible to enforce the PIN, as many devices do not limit the number of attempts a user can make. Among other things, light-based commands can be sent from one building to another. If the device is near a window, it may be reachable.





The attack exploits a vulnerability in microphones that use microelectromechanical systems, or MEMS. The MEMS microscopic components of these microphones unintentionally respond to light like sound.
Although investigators have only tested the attack on devices with Siri, Alexa, Google Assistant, Facebook Portal and a small number of tablets and phones, they believe that all devices that use MEMS microphones are susceptible to lightning strike attacks.



Effective but conditioning laser attacks
Laser-based attacks have several limitations. For one thing, the attacker must have direct line of sight to the target device. In addition, light in many cases must be precisely directed at a very specific part of the microphone.
Except in cases where an attacker uses an infrared laser, the lights are also easy to see by someone who is nearby and has a line of sight on the device. Devices typically respond with voice and visual signals when executing a command, a feature that would alert users within earshot of the device.





Despite these restrictions, results are important for a number of reasons. The investigation not only introduces a new mode of attack against voice-controlled systems, but also shows how to conduct attacks in semi-realistic environments. In addition, researchers still do not fully understand the physics behind their discovery. That way, when they realize how they really work, they can further refine the attack.



Sensitive services on columns do not require authentication
Finally, the research highlights the risks that result when voice control devices and the peripherals to which they connect perform confidential commands without requiring a password or PIN.
We have found that voice command systems often do not have user authentication mechanisms, or if the mechanisms are present, they are implemented incorrectly (for example, allowing brute force PIN). We show how an attacker can use light-injected voice commands to unlock the smart lock-protected front door, open garage doors, shop at e-commerce sites at the victim's expense, or even locate, unlock and launch multiple vehicles ( for example, Tesla and Ford) if the vehicles are linked to the attacked user's Google account.
The researchers wrote in an article titled Light Commands: Laser-based audio injection attacks on voice-controlled systems.



Cheap laser instruments can do a lot of damage
The article describes different configurations used to perform the attacks. One is made up of a simple laser pointer (priced at $ 18 for 3 units). In addition, they also used a Wavelength Electronics LD5CHA laser driver ($ 339) and a Neoteck NTK059 audio amplifier. Despite certain limitations, they may even use other lenses to focus the laser on ranged attacks.



Another device used an infrared laser that is invisible to the human eye for more stealth attacks. A third configuration featured a 500-lumen Acebeam W30 laser-excited phosphor flashlight to eliminate the need to precisely point a light on a specific part of a MEMS microphone.
In one attack, a command was successfully injected through a glass window. This was about 70 meters away.



In this experiment, a column was positioned near a window on the fourth floor of a building. The attacker's laser was placed on a platform within a nearby tower, located about 1.5 meters above ground level. The laser fired a beam over Google Home, which has only upward-facing microphones.
In a different experiment, the researchers used a telephoto lens to focus the laser and successfully attack a device about 100 meters away.
Source: Arstechnica


"
