<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.techspot.com/news/82625-siri-alexa-google-home-can-hacked-laser.html"/>
    <meta property="og:site_name" content="TechSpot"/>
    <meta property="article:published_time" content="2019-11-05T17:12:57+00:00"/>
    <meta property="og:title" content="Siri, Alexa, and Google Home can be hacked with a laser"/>
    <meta property="og:description" content="For the last few years, tech giants like Amazon, Google, Apple, and others have been busy building smart devices that can be controlled by voice. Naturally, a lot of people have questioned the privacy and security implications, and even more&amp;hellip;"/>
  </head>
  <body>
    <article>
      <h1>Siri, Alexa, and Google Home can be hacked with a laser</h1>
      <h2>Your smart speakers are a little vulnerable right now</h2>
      <address><time datetime="2019-11-05T17:12:57+00:00">05 Nov 2019, 17:12</time> by <a rel="author">Adrian Potoroaca</a></address>
      <figure>
        <iframe src="https://www.youtube.com/embed/ORji7Tz5GiI" width="560" height="390" data-service="Youtube" scrolling="no"/>
      </figure>
      <h4>In brief: Researchers have found a novel way to hack into smart devices that uses lasers. While that seems unimaginable, it turns out a lot of devices have MEMS (micro-electro-mechanical systems) microphones that can be used to send commands from hundreds of feet away, so long as there's a clear line of sight.</h4>
      <p>For the last few years, tech giants like Amazon, Google, Apple, and others have been busy building smart devices that can be controlled by voice. Naturally, a lot of people have questioned the privacy and security implications, and even more so after news broke that Amazon and Apple used <a href="https://www.techspot.com/news/79610-thousands-amazon-workers-listening-alexa-conversations.html">thousands of workers</a> to <a href="https://www.techspot.com/news/81170-apple-qc-workers-often-hear-bits-private-conversations.html">listen</a> to digital assistant conversations. In the case of Amazon, Alexa transcripts can dwell on its servers long <a href="https://www.techspot.com/news/80790-alexa-transcripts-may-persist-even-after-deleted-amazon.html">after a user requested their deletion</a>.</p>
      <p>Recently, researchers at the University of Michigan and at the University of Electro-Communications of Tokyo have <a href="https://lightcommands.com/20191104-Light-Commands.pdf">discovered</a> a new type of attack that once again proves that smart devices come with significant privacy risks. They found that by pointing a powerful laser at a mobile device or a smart speaker and quickly varying its intensity made their sensitive MEMS (micro-electro-mechanical systems) microphones pick it up as if it were sound.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/EtzP-mCwNAs" width="893" height="502" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>This photoacoustic effect essentially allowed the researchers to beam "light commands" to voice-controlled hardware from as far away as 361 feet (110m). And since smart speakers are built for convenience, more often that not there's no additional security measures in place before they can receive those commands. That means an attacker can use digital assistants to make online purchases, unlock door locks, garage doors, or locate a connected car such as a Tesla.</p>
      <p>The devices used in the experiments include Google Home, Amazon's Echo lineup, Facebook Portal, iPhone XR, iPad 6th gen, Google Pixel 2 and Samsung's Galaxy S9.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/iK2PtdQs77c" width="893" height="502" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>It's worth noting there are some limitations, such as the need to precisely target a device's microphone and a clear line of sight. However, the researchers note that while such an attack isn't easy to pull off, the parts needed can cost as little as $600 and are readily available on Amazon. And judging by the demo videos above, this attack method works surprisingly well even when the laser beam has to pass through a window.</p>
      <p>The good news is there are some protections in our devices that can prevent light commands from working, such as requiring a device unlock for advanced commands and wake-up words that carry a user's voice signature. And even as companies are doing everything in their power to <a href="https://www.techspot.com/news/82631-opinion-microsoft-cortana-pivot-highlights-evolving-role-voice.html">make voice assistants more useful</a>, adoption rates <a href="https://www.techspot.com/news/80542-voice-assistant-adoption-rates-lower-than-previously-thought.html">aren't that high</a>.</p>
      <p>The researchers think manufacturers can greatly improve security by using multiple microphones and shielding them from light. In any case, it will be interesting to see how tech giants respond to the findings.</p>
      <related>
        <h4>Related Reads</h4>
        <a href="https://www.techspot.com/news/80704-jetson-pentagon-laser-can-identify-people-heartbeat.html"/>
        <a href="https://www.techspot.com/news/82631-opinion-microsoft-cortana-pivot-highlights-evolving-role-voice.html"/>
        <a href="https://www.techspot.com/news/80641-usaf-directed-energy-weapon-thor-can-down-swarms.html"/>
        <a href="https://www.techspot.com/news/82086-xbox-one-can-now-controlled-google-assistant.html"/>
      </related>
    </article>
  </body>
</html>