,src,lang,text
0,"











Deepfakes, Revenge Porn, And The Impact On Women
01 Nov 2019 by Chenxi Wang
Imagine seeing yourself in a sexually explicit video in which you have never participated. This is a distinct possibility today for a female celebrity or a regular woman living in the age of Deepfakes. Deepfake is a technique used to manipulate human images based on artificial intelligence. It can superimpose images and videos onto source footage using a variety of machine learning techniques. What sets Deepfake images or videos apart from other modified images is that the former looks strikingly authentic. 
Earlier this year, a Deepfake video that went viral showed comedian Bill Hader’s face being seamlessly transformed into that of Tom Cruise. The footage appeared so authentic that it created a sensation on the Internet.  



Ctrl shift face YouTube channel


Examples like this abound. A recent report by DeepTrace found that the number of Deepfake videos on the Internet almost doubled from 2018 to 2019. 
One reason that fuels the rise of the Deepfake footage is that many implementations of Deepfake algorithms are open-source, easily accessible to anyone with basic programming skills and a reliable Internet connection. The popular code repository created by the anonymous /r/ is available on Github, free for all to use. This dramatically lowers the barrier for non-experts to utilize the technology. 
The most popular algorithm capable of generating Deepfake images is Generative Adversarial Networks (GAN), created by Ian Goodfellow (Formerly of Google). GAN comprises of two neural networks engaged in a competition with each other. One network is the “generative” model that aims to generate similar data to the original training set. The other is the “discriminatory” model whose goal is to classify whether a particular data set is synthetic or original.
As the two models iterate and compete with each other, if the synthetic data set produced by the generative model evades the detection of the discriminatory model, the data must be strikingly similar to the original data set. As a result, it appears authentic to the human eyes. 
In October 2018, Christie, the auction house, sold an oil painting generated by GAN for $435,000, nearly 45 times its highest estimate. The painting was created by a group of French students, using a dataset of approximately 15,000 portraits from the 14th to the 20th century. 
Even though AI techniques like Deepfake have many applications, including photo editing, image repair, and 3D transformation, unfortunately, their main application today is generating sexually explicit videos for cyber exploitation. According to the Deeptrace report, 96% of the Deepfake videos on the Internet are pornographic videos. Not surprisingly, the main victims of the fake videos are women, whose images and likenesses are used without their consent, and often without their knowledge. 
In addition to violating the victim’s rights to her own images and privacy, a major concern is Deepfakes’ potential impact on revenge porn. As we saw in the case with Representative Katie Hill, revenge porn can end someone’s political career. With technologies like Deepfake, revenge porn or cyber smear campaigns can take on a whole new dimension and potentially impact a wider population of victims. 
Modified or photoshopped images are certainly nothing new, but the difference between Deepfakes and these prior image manipulation techniques is that there is no reliable way to detect Deepfakes today. In fact, the technology used to detect Deepfaks can be used to generate them. “As of now, we lack automated ways to detect Deepfakes in a reliable and scalable fashion”, Dawn Song, a Computer Science Professor at the University of California Berkeley said, “It will be an arms race between those that create Deepfakes and those seek to detect them.” 
In this arms race, it is clear that the entity that has the most data will win - the more data that you have, the more accurate and sophisticated a model you can build. To that end, companies like Google, Facebook, and Twitter have a critical role to play. In September, Facebook, in partnership with Microsoft and a list of academic institutions, announced a $10 million research effort to help build a better Deepfakes detector. Google recently contributed over 3,000 Deepfake videos as training data to the FaceForensics benchmark project, supported by the European Research Council, in an effort to bolster Deepfake detection.



FaceForensics Benchmark Project


As a growing number of Deepfake creation communities emerge around the world, along with the increasingly commoditized creation apps and services, we expect that Deepfake content will proliferate rapidly, expanding from sexually explicit content to fake news, political speeches, and commercial fraud. 
Regulators are also concerned. Early October, California Gov. Gavin Newsom signed into legislation AB730, which makes it illegal to create or distribute fake videos, images or audio of politicians within 60 days of an election. American Civil Liberties Union (ACLU) and Electronic Frontier’s Foundation (EFF) have already disagreed with the law, arguing that it could negatively impact free speech. 
Should we establish laws to regulate synthetic data? Should we regulate the generation of such data or just the usage of it? Who owns the intellectual property (IP) of the synthetic data? Does the algorithm own it or is it the writer of the algorithm who owns the IP? These are all interesting and challenging questions for lawmakers, who by and large have not even begun to consider the societal impact of AI technologies behind phenomena such as Deepfakes.   


",en,"











Deepfakes, Revenge Porn, And The Impact On Women
01 Nov 2019 by Chenxi Wang
Imagine seeing yourself in a sexually explicit video in which you have never participated. This is a distinct possibility today for a female celebrity or a regular woman living in the age of Deepfakes. Deepfake is a technique used to manipulate human images based on artificial intelligence. It can superimpose images and videos onto source footage using a variety of machine learning techniques. What sets Deepfake images or videos apart from other modified images is that the former looks strikingly authentic. 
Earlier this year, a Deepfake video that went viral showed comedian Bill Hader’s face being seamlessly transformed into that of Tom Cruise. The footage appeared so authentic that it created a sensation on the Internet.  



Ctrl shift face YouTube channel


Examples like this abound. A recent report by DeepTrace found that the number of Deepfake videos on the Internet almost doubled from 2018 to 2019. 
One reason that fuels the rise of the Deepfake footage is that many implementations of Deepfake algorithms are open-source, easily accessible to anyone with basic programming skills and a reliable Internet connection. The popular code repository created by the anonymous /r/ is available on Github, free for all to use. This dramatically lowers the barrier for non-experts to utilize the technology. 
The most popular algorithm capable of generating Deepfake images is Generative Adversarial Networks (GAN), created by Ian Goodfellow (Formerly of Google). GAN comprises of two neural networks engaged in a competition with each other. One network is the “generative” model that aims to generate similar data to the original training set. The other is the “discriminatory” model whose goal is to classify whether a particular data set is synthetic or original.
As the two models iterate and compete with each other, if the synthetic data set produced by the generative model evades the detection of the discriminatory model, the data must be strikingly similar to the original data set. As a result, it appears authentic to the human eyes. 
In October 2018, Christie, the auction house, sold an oil painting generated by GAN for $435,000, nearly 45 times its highest estimate. The painting was created by a group of French students, using a dataset of approximately 15,000 portraits from the 14th to the 20th century. 
Even though AI techniques like Deepfake have many applications, including photo editing, image repair, and 3D transformation, unfortunately, their main application today is generating sexually explicit videos for cyber exploitation. According to the Deeptrace report, 96% of the Deepfake videos on the Internet are pornographic videos. Not surprisingly, the main victims of the fake videos are women, whose images and likenesses are used without their consent, and often without their knowledge. 
In addition to violating the victim’s rights to her own images and privacy, a major concern is Deepfakes’ potential impact on revenge porn. As we saw in the case with Representative Katie Hill, revenge porn can end someone’s political career. With technologies like Deepfake, revenge porn or cyber smear campaigns can take on a whole new dimension and potentially impact a wider population of victims. 
Modified or photoshopped images are certainly nothing new, but the difference between Deepfakes and these prior image manipulation techniques is that there is no reliable way to detect Deepfakes today. In fact, the technology used to detect Deepfaks can be used to generate them. “As of now, we lack automated ways to detect Deepfakes in a reliable and scalable fashion”, Dawn Song, a Computer Science Professor at the University of California Berkeley said, “It will be an arms race between those that create Deepfakes and those seek to detect them.” 
In this arms race, it is clear that the entity that has the most data will win - the more data that you have, the more accurate and sophisticated a model you can build. To that end, companies like Google, Facebook, and Twitter have a critical role to play. In September, Facebook, in partnership with Microsoft and a list of academic institutions, announced a $10 million research effort to help build a better Deepfakes detector. Google recently contributed over 3,000 Deepfake videos as training data to the FaceForensics benchmark project, supported by the European Research Council, in an effort to bolster Deepfake detection.



FaceForensics Benchmark Project


As a growing number of Deepfake creation communities emerge around the world, along with the increasingly commoditized creation apps and services, we expect that Deepfake content will proliferate rapidly, expanding from sexually explicit content to fake news, political speeches, and commercial fraud. 
Regulators are also concerned. Early October, California Gov. Gavin Newsom signed into legislation AB730, which makes it illegal to create or distribute fake videos, images or audio of politicians within 60 days of an election. American Civil Liberties Union (ACLU) and Electronic Frontier’s Foundation (EFF) have already disagreed with the law, arguing that it could negatively impact free speech. 
Should we establish laws to regulate synthetic data? Should we regulate the generation of such data or just the usage of it? Who owns the intellectual property (IP) of the synthetic data? Does the algorithm own it or is it the writer of the algorithm who owns the IP? These are all interesting and challenging questions for lawmakers, who by and large have not even begun to consider the societal impact of AI technologies behind phenomena such as Deepfakes.   


"
