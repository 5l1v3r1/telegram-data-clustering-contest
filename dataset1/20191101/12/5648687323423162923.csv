,src,lang,text
0,"











Update
Filter und Fakten
01 Nov 2019, 12:33 by Kathrin Passig

Soziale Netzwerke sollen Radikalisierung fördern und zu Parallelwelten führen. Gibt es dafür wissenschaftliche Belege?

Vor acht Jahren ist der Begriff der „Filterblase“ in den Diskussionen um Internetangelegenheiten aufgetaucht und seitdem nicht mehr verschwunden. Es ging damals vor allem personalisierte Suchergebnisse zum Beispiel bei Google, die die Vorlieben einer Person aus deren bisherigem Nutzungsverhalten abzuleiten versuchen. Dadurch bekomme man, so lautete der Vorwurf, nur noch Informationen vorgesetzt, die eine schon vorhandene Meinung weiter verstärken. Das deutsche Wort „Blase“ klingt fragil, nach etwas, das schnell wieder platzt. Gemeint ist aber ein robuster Raum, der von den Ansichten anderer Menschen abschottet.
Seit den US-Wahlen von 2016 geht es, wenn das Wort Filterblase fällt, kaum noch um personalisierte Google-Ergebnisse. Die Diskussion hat sich auf die Vorgänge in sozialen Netzwerken verlagert. Gleichgesinnte, so heißt es, bestärken einander in ihren Meinungen, fallen auf falsche Nachrichten herein, bekommen von anderen Lebensweisen nichts mehr mit und entfernen sich so immer weiter von anderen Gruppen. Sortier- und Empfehlungssysteme, die bei Facebook bestimmte Beiträge in der Timeline sichtbarer machen und bei Youtube zu ähnlichen Videos führen, unterstützen diese Aufspaltung in Parallelwelten.
So lautet jedenfalls die Theorie.
Eine Studie aus dem Jahr 2018 (Shore, Baek und Dellarocas) ergab jedoch, dass die überwiegende Mehrheit der aktiven Twitter-Nutzerinnen und -Nutzer dort im Widerspruch zur Filterblasentheorie Nachrichten schreibt, die politisch nicht extremer sind als diejenigen Nachrichten, die sie zur Kenntnis nimmt, sondern moderater. Die typische Twitter-Timeline enthält Links zu Nachrichtenartikeln aus dem gesamten politischen Spektrum. Die Intensiv-Twitterer folgen politisch noch diverseren Accounts als die Normalnutzer. An Kontakt zu anderen Sichtweisen mangelte es keiner der beiden Gruppen.


Hier schreibt Kathrin Passig jede Woche über Themen des digitalen Zeitalters. Sie ist Mitbegründerin des Blogs „Techniktagebuch“. www.kathrin.passig.de© Norman Posselt

Eine weitere 2019 erschienene Studie (Becker, Porter und Centola) widmet sich der Frage, wie die Beobachtung der „Wisdom of Crowds“ zur Filterblasentheorie passt. Die Idee von der „Wisdom of Crowds“ ist älter als das Netz und beschreibt, dass Menschen in der Summe ihrer Meinungen zu besseren Zukunftsvorhersagen oder Schätzwerten gelangen als Einzelne, selbst wenn diese Einzelnen Fachleute sind, weil sich die individuellen Irrtümer gleichmäßig verteilen.
Nach der Filterblasentheorie müsste die Situation in sozialen Netzwerken diesen Effekt verhindern, und die Irrtümer müssten in eine bestimmte Richtung ausschlagen. Die Studie – die allerdings in einer experimentellen Umgebung und nicht in einem realen sozialen Netzwerk durchgeführt wurde – kommt zum entgegengesetzten Schluss: Selbst wenn zwei politisch polarisierte Gruppen gar nicht direkt miteinander kommunizieren, führt schon der Austausch innerhalb der jeweiligen Gruppen zu korrekteren Ergebnissen bei Faktenfragen und reduziert die Radikalisierung.
Eine dritte Studie aus dem Jahr 2019 (Nguyen und Vu) untersuchte die Frage, ob Menschen, die ihre Politiknachrichten vorwiegend aus sozialen Netzwerken beziehen, dadurch stärker radikalisiert werden als solche, die sich auf Radio, Fernsehen, Zeitungen oder das Netz außerhalb sozialer Netzwerke verlassen. Zu diesem Zweck analysierten die Autoren die in der „Eurobarometer“-Umfrage von 2016 abgefragten Ansichten über die EU. Wie sich herausstellte, hat die Wahl der Nachrichtenquelle keinen signifikanten Einfluss.
Die Vorstellung, dass die Meinungen anderer Leute irgendwie durch eine technische Neuerung verursacht werden, ist attraktiv, aber nicht so leicht zu belegen. Dass die falschen und empörenden Meinungen anderer Leute in den vergangenen Jahren überhaupt zum Debattenthema geworden sind, hat mit ihrer gestiegenen Sichtbarkeit zu tun. Man kann den sozialen Netzwerken aber nicht vorwerfen, dass sie hermetisch abgeschottete Räume erzeugen und dass sie uns gleichzeitig mit der Existenz der darin vertretenen Ansichten konfrontieren. Zuverlässig funktionierende Filterblasen würden die Welt friedlicher und einmütiger wirken lassen, nicht umgekehrt.
Selbst wenn ich mich im Netz einer Gruppe aus hundertprozentig konformen Befürwortern des großen ß in der Typographie oder Gegnern einer Fahrradhelmpflicht anschließe, wird es darin auch um Fragen des Sprachgebrauchs, der Politik und der Haustierhaltung gehen. In diesen Fragen wird nicht nur keine Einigkeit herrschen, ich werde dadurch auch von Lebensweisen und Ansichten erfahren, die mir bis dahin ganz unbekannt waren. Das ist lästig, aber leider unvermeidlich. Man hört, es sei sogar schon vor der Erfindung des Internets so ähnlich gewesen.


",de,"











update
Filters and facts
01 Nov 2019, 12:33 by Kathrin Passig

Social networks should promote radicalization and lead to parallel worlds. Is there scientific evidence for this?

Eight years ago, the term ""filterbladder"" surfaced in Internet-related discussions and has not disappeared since. At that time, it was mainly personalized search results, for example Google, that attempted to derive a person's preferences from their previous usage behavior. Thus one gets, so the reproach, only information prefixed, which reinforce an already existing opinion further. The German word ""bubble"" sounds fragile, something that quickly bursts again. What is meant, however, is a robust space that separates itself from other people's views.
Since the US election in 2016, when the word ""filter bubble"" comes up, it's hardly about personalized Google results. The discussion has shifted to the processes in social networks. Like-minded people, it is said, reinforce each other's opinions, fall for false news, have nothing to do with other ways of life, and move further away from other groups. Sorting and recommending systems, which make certain posts visible on Facebook in the timeline and lead to similar videos on Youtube, support this split in parallel worlds.
At least that's the theory.
However, a 2018 study (Shore, Baek, and Dellarocas) found that the overwhelming majority of Twitter active users, in contravention of filter bubble theory, write news that is not politically more extreme than the news they take note of but more moderate. The typical Twitter timeline contains links to news articles from across the political spectrum. The intensive Twitter follow politically even more diverse accounts than the normal users. There was no lack of contact with other perspectives.


Every week Kathrin Passig writes about topics of the digital age. She is co-founder of the blog ""Techniktagebuch"". www.kathrin.passig.de © Norman Posselt

Another study published in 2019 (Becker, Porter and Centola) deals with the question of how the observation of the ""Wisdom of Crowds"" fits filter-bubble theory. The idea of the ""Wisdom of Crowds"" is older than the Web and describes that people in the sum of their opinions get better predictions of the future or estimates than individuals, even if these individuals are professionals because the individual errors are evenly distributed.
According to the filter bubble theory, the situation in social networks would have to prevent this effect, and the errors would have to turn in a certain direction. The study - which was carried out in an experimental environment and not in a real social network - comes to the opposite conclusion: Even if two politically polarized groups do not communicate directly with each other, the exchange within the respective groups leads to more correct results in factual questions and reduces radicalization.
A third study from 2019 (Nguyen and Vu) examined whether people who derive their news primarily from social networks are more radicalized than those who rely on radio, television, newspapers or the network outside social networks , To this end, the authors analyzed the views on the EU queried in the 2016 Eurobarometer survey. As it turned out, the choice of news source has no significant impact.
The idea that other people's opinions are somehow caused by a technical innovation is attractive, but not so easy to prove. The fact that other people's false and outrageous opinions have become a debating topic in recent years has something to do with their increased visibility. But you can not blame the social networks for creating hermetically sealed spaces, and for confronting us with the existence of the views expressed in them. Reliably functioning filter bubbles would make the world appear more peaceful and unanimous, not the other way around.
Even if I join the network of a group of 100% compliant supporters of the big name in the typography or opponents of cycling helmets, it will also address issues of language usage, politics, and pet ownership. Not only will there be no consensus on these questions, but I will also learn about ways of life and views that until then were completely unknown to me. This is annoying, but unfortunately inevitable. It is said that it was similar even before the invention of the Internet.


"
