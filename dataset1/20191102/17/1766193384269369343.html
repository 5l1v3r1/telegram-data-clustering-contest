<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.heise.de/newsticker/meldung/Gaggle-Echtzeitueberwachung-von-US-Schuelern-mit-Kuenstlicher-Intelligenz-4574230.html"/>
    <meta property="og:site_name" content="heise online"/>
    <meta property="article:published_time" content="2019-11-02T17:15:00+00:00"/>
    <meta property="og:title" content="Gaggle: Echtzeitüberwachung von US-Schülern mit Künstlicher Intelligenz"/>
    <meta property="og:description" content="Der Dienstleister Gaggle überwacht die digitale Arbeit und Kommunikation von rund fünf Millionen Schülern in den USA im Kampf gegen Selbstmorde und Amokläufe."/>
  </head>
  <body>
    <article>
      <h1>Gaggle: Echtzeitüberwachung von US-Schülern mit Künstlicher Intelligenz</h1>
      <h2>Der Dienstleister Gaggle überwacht die digitale Arbeit und Kommunikation von rund fünf Millionen Schülern in den USA im Kampf gegen Selbstmorde und Amokläufe.</h2>
      <address><time datetime="2019-11-02T17:15:00+00:00">02 Nov 2019, 17:15</time> by <a rel="author">Stefan Krempl</a></address>
      <p>Schießereien, Massaker und Selbstmorde an US-Schulen erhöhen die Nachfrage nach technischen Lösungen, wie sie nach eigenen Angaben der in Illinois sitzende Anbieter Gaggle verfügbar macht. Die Firma setzt auf einen Mix aus einem proprietären System mit Künstlicher Intelligenz (KI) und menschlichen Begutachtern von als schädlich oder gefährlich erkannten Inhalten, um potenziell tödliche Taten zu verhindern. Zugleich soll das Programm dabei helfen, Kinder und Jugendliche zu "guten Bürgern" zu erziehen. Mit dem Ansatz wirft die Firma aber Fragen nicht nur zum Datenschutz der Betroffenen auf.</p>
      <h3>Tiefer Eingriff in Privatsphäre</h3>
      <p>Über 1400 US-Schulen mit rund fünf Millionen Auszubildenden glauben bereits den Versprechen von Gaggle und setzen die Software ein, um die digitale Arbeit und Kommunikation ihrer Schutzbefohlenen "in Echtzeit" zu überwachen. Das Programm klinkt sich in Googles G Suite und Microsoft Office 365 und damit in zwei der am weitesten verbreiteten Produktivitätspakete in der Cloud ein und analysiert den gesamten damit erzeugten Datenverkehr. Es bezieht zudem Nachrichten aus Twitter, Facebook, Instagram und anderen sozialen Netzwerken mit ein, die mit einer schulisch benutzten E-Mail-Adresse verknüpft sind.</p>
      <p>Gaggle gibt an, allein im Schuljahr 2018/19 722 Schüler von einem Suizid abgehalten zu haben. <a href="https://www.documentcloud.org/public/search/projectid:46514-Gaggle">Interne Dokumente</a> wie Hinweise für Prüfer von Inhalten, Fallberichte und Rechnungen aus 17 Schulbezirken, die das Online-Magazin BuzzFeed auf Basis des US-Informationsfreiheitsgesetzes <a href="https://www.buzzfeednews.com/article/carolinehaskins1/gaggle-school-surveillance-technology-education">erhalten und ausgewertet hat</a>, zeigen jedoch, wie tief das System in die Privatsphäre der Betroffenen eingreift und dass die Erfolgsmeldungen auf nur schwer nachprüfbaren Kriterien beruhen. Zudem wird deutlich, dass Schulbezirke mehr als 60.000 US-Dollar für den Einsatz von Gaggle pro Jahr bezahlen, die in knappen Etats an anderer Stelle fehlen.</p>
      <h3>Begriffe wie "schwul", "lesbisch", "queer" auf schwarzer Liste</h3>
      <p>Die Software mit dem eingebauten KI-Filter scannt demnach E-Mails, Dokumente, Chats und Kalendereinträge und gleicht sie mit einer schwarzen Liste ab, auf der sich neben Ausdrücken mit Bezügen zu Selbstverletzungen, Gewalt, Mobbing oder Drogen auch "vulgäre Ausdrücke" befinden. Letztere machen dem Bericht zufolge 80 Prozent der von dem System zur weiteren Verfolgung markierten Inhalte aus. Auch Begriffe wie "schwul", "lesbisch" oder "queer" sind auf der Liste.</p>
      <p>Dazu kommt ein "Anti-Pornografie-Scanner", über dessen Trainingsdaten Gaggle aufgrund der sensiblen Thematik und geschützter Informationen rund um die Eigenlösung keine Auskunft geben wollte. Stellt ein Analyst der Firma fest, dass ein Schüler pornografische Inhalte selbst erstellt hat, wird die Datei automatisch an das National Center for Missing and Exploited Children (NCMEC) weitergeleitet für einen weiteren Abgleich mit der dortigen Datenbank für Bilder mit sexuellen Missbrauchsdarstellungen von Kindern.</p>
      <h3>Schüler nach "Regelverletzung" in drei Risikogruppen eingeteilt</h3>
      <p>Schlägt die Software Alarm, übergibt sie einschlägiges Material an einen von 125 "Sicherheitsrepräsentanten 1. Grades". Erkennen auch diese darin eine immanente Bedrohung für Schüler, schaut noch ein Mitglied einer 25-köpfigen Kerngruppe an besonders geschulten Prüfern darüber. Bleibt es bei der Einschätzung, werden umgehend Zuständige bei der Ausbildungsstätte oder bei deren Nichterreichbarkeit bei der örtlichen Polizei informiert.</p>
      <p>Generell speist Gaggle Erkenntnisse aus der Kommunikationsüberwachung in ein "Dashboard" für das "Sicherheitsmanagement" ein, auf das auch Schuladministratoren Zugriff haben. Daraus ersichtlich werden "Regelverletzungen" einzelner Schüler zusammen mit deren Ausweisnummern. Die Betroffenen werden dabei in drei Risikogruppen eingeteilt, die von hohen bis zu niedrigen Bedenken mit nur "fragwürdigen Inhalten" reichen.</p>
      <h3>Fehlalarme: "Gedichtsammlung" als gefährlich eingestuft</h3>
      <p>Das System befolgt auf dieser Basis eine "Three Strikes"-Regel, wie sie ähnlich auch aus dem <a href="https://www.heise.de/meldung/EU-Datenschutzbeauftragter-gegen-Internetsperren-bei-Copyright-Verstoessen-198019.html">Umgang mit Urheberrechtsverletzungen mit Konsequenzen bis hin zu Netzsperren bekannt geworden ist</a>. Demnach werden auch geringfügige Regelverstöße an eine Schule gemeldet, wenn sie wiederholt auftreten und ein Beobachteter etwa dreimal das Wort "Fuck" benutzt. Betroffene verlieren damit gewisse Nutzerprivilegien, bis ein Schulvertreter sie wieder freischaltet.</p>
      <details>
        <summary>Software meldet Selbstmordabsichten</summary>
        <figure>
          <img src="https://www.heise.de/imgs/18/2/7/8/0/4/3/0/urn-newsml-dpa-com-20090101-141209-99-04221_large_4_3-afb5de68b74e74b4-6e7cb64064940b42.jpeg"/>
          <figcaption>
            <cite>(Bild: dpa, Marc Tirl/Symbolbild)</cite>
          </figcaption>
        </figure>
        <p>Schulcomputer in den USA können eine Software erhalten, die per KI aus dem Surfverhalten der Schüler Rückschlüsse auf mögliche Selbstmordabsichten ziehen soll.</p>
        <ul>
          <li>
            <a href="https://www.heise.de/meldung/Internetfilter-an-US-Schulen-soll-Suizidabsichten-erkennen-4146161.html">Internetfilter an US-Schulen soll Suizidabsichten erkennen</a>
          </li>
        </ul>
      </details>
      <p>Im jüngsten Schuljahr will das Unternehmen mithilfe des Programms 52.000 Hinweise auf Absichten für Selbstmorde und -verletzungen ausgemacht haben. Davon sollen 6000 so dringlich gewesen sein, dass man die Schule unverzüglich darüber in Kenntnis gesetzt habe. Aus den Papieren geht aber hervor, dass die Software auch Dateien mit Titeln wie "Gedichtsammlung" oder "erzählerischer Aufsatz" als brandgefährlich eingeschätzt und so Fehlalarme ausgelöst hat.</p>
      <h3>Opt-out von Einzelnen schließt Schule von Softwarenutzung aus</h3>
      <p>Gaggle "empfiehlt" Schulbezirken nach eigenem Bekunden, die Erlaubnis von Eltern und Schülern einzuholen, bevor sie die Software der Firma verwenden. Sollten sich einzelne Betroffene später für ein Opt-out entscheiden, könnten sie die von der Schule angebotenen Softwareprodukte und E-Mail-Dienste nicht mehr in Anspruch nehmen und müssten mit eigenen Alternativen arbeiten. Von einer echten freiwilligen Nutzung des Dienstes lässt sich damit nicht ausgehen.</p>
      <p>Experten sehen den Einsatz des Systems kritisch. Schulen seien zwar schon immer "Trainingsstätten für die Persönlichkeit" gewesen, erklärte Sarah Igo, Geschichtsprofessorin an der Vanderbilt-Universität, gegenüber BuzzFeed. Unter der ständigen Aufsicht von Diensten wie Gaggle brächten den Heranwachsenden auch vielleicht das ein oder andere bei, "aber nicht das, was sie lernen sollen". Es handle sich offenbar um eine "Lösung auf der Suche nach einem Problem", konstatierte Sarah Roberts, Expertin für "Content Moderation" an der University of California in Los Angeles. Das Ganze funktioniere nur, "wenn wir akzeptieren, dass unsere Kinder bis auf Weiteres in einem digitalen System gefangen sein sollten schon letztlich von der Zeit an, in denen sie empfindungsfähig werden." (<a href="mailto:tiw@heise.de">tiw</a>)</p>
    </article>
  </body>
</html>