,src,lang,text
0,"











[Tech] 엔비디아, AI 추론 벤치마크 테스트서 새로운 기록 달성
07 Nov 2019, 03:18 by 이두현 기자



엔비디아가 데이터센터와 네트워크 엣지(edge)에서 AI 추론 워크로드 성능을 측정하는 새로운 벤치마크 테스트에서 가장 빠른 속도를 달성했다.
업계 최초의 독립적인 AI 추론 벤치마크 테스트인 이번 MLPerf Inference 0.5의 결과는 데이터센터용 엔비디아 튜링(Turing) GPU와 엣지 컴퓨팅용 엔비디아 젯슨 자비에(Jetson Xavier) 시스템온칩(SoC)의 우수한 성능을 명확히 보여준다.
다양한 폼 팩터와 네 가지 추론 시나리오를 적용하는 MLPerf의 다섯 가지 추론 벤치마크는 이미지 분류, 객체 감지 및 변환과 같은 기존 AI 애플리케이션을 포함한다.
엔비디아는 서버 및 오프라인을 포함한 데이터센터 시나리오 상의 5개 벤치마크에서 모두 1위를 차지했으며, 튜링 GPU는 상용 프로세서 중 프로세서당 최고 성능을 달성했다.[1] 젯슨 자비에는 엣지 중심의 시나리오(단일 및 멀티 스트림)에서 상용화된 엣지 및 모바일 SoC 가운데 최고 성능을 기록했다.[2]
이안 벅(Ian Buck) 엔비디아 가속 컴퓨팅 담당 부사장겸 총괄은 “AI는 현재 연구 단계에서 실제 애플리케이션을 위해 대규모로 구축되는 단계로 접어들면서 전환점을 맞이하고 있다. AI 추론은 컴퓨팅의 중대한 과제이다. 엔비디아는 프로그래밍이 가능한 업계 최고 수준의 가속기, 쿠다-X(CUDA-X) AI 알고리즘 스위트, 그리고 AI 컴퓨팅에 대한 전문지식을 결합하여 데이터센터에 점점 복잡해지고 있는 AI 모델을 원활하게 구축할 수 있도록 돕고 있다”고 설명했다.
엔비디아는 다양한 AI 워크로드에 걸쳐 컴퓨팅 플랫폼의 프로그래밍 가능성과 성능을 입증하면서, 모든 다섯 가지 MLPerf 벤치마크에 결과를 제출한 유일한 기업이었다.
엔비디아 GPU는 알리바바 클라우드(Alibaba Cloud), 아마존웹서비스(AWS), 구글 클라우드 플랫폼, 마이크로소프트 애저(Azure) 및 텐센트(Tencent) 등을 포함한 세계 최대 클라우드 인프라에서 대규모 추론 워크로드를 가속화하는데 활용되고 있다. 또한, 월마트(Walmart), 프록터앤드갬블(Procter & Gamble)과 같은 세계 선도 기업들이 엔비디아의 EGX 엣지 컴퓨팅 플랫폼과 AI 추론 기능을 이용하여 최첨단 AI 워크로드를 실행하고 있다.
엔비디아의 모든 MLPerf 결과는 엔디비아 텐서RT 6(TensorRT 6)를 기반으로 달성됐다. 엔비디아 텐서RT 6는 고성능 딥러닝 추론 소프트웨어로 데이터센터부터 엣지에 이르는 프로덕션 환경에서 AI 애플리케이션을 쉽게 배포 및 최적화한다. 새로운 텐서RT 최적화는 깃허브(Github) 리포지토리에서 오픈소스로도 이용할 수 있다.


",ko,"











NVIDIA hits new record with AI inference benchmark test book
07 Nov 2019, 03:18 by Lee Doo-hyun



NVIDIA has achieved the fastest speed in a new benchmark test that measures AI inference workload performance at the data center and network edge.
MLPerf Inference 0.5, the industry's first independent AI inference benchmark test, demonstrates the superior performance of NVIDIA Turing GPUs for data centers and NVIDIA Jetson Xavier System-on-Chip (SoC) for edge computing. .
Applying various form factors and four inference scenarios, MLPerf's five inference benchmarks include existing AI applications such as image classification, object detection, and transformation.
NVIDIA ranked first in all five benchmarks in data center scenarios, including server and offline, and Turing GPUs achieved the highest performance per processor among commercial processors. Jetson Xavier recorded the highest performance among commercially available edge and mobile SoCs in edge-driven scenarios (single and multi-stream).
Ian Buck, vice president and general manager of NVIDIA Accelerated Computing, said: “AI is currently at a turning point as we move from research to large scale deployments for real-world applications. AI inference is a significant task in computing. NVIDIA combines industry-leading programmable accelerators, CUDA-X AI algorithm suites, and AI computing expertise to help you build increasingly complex AI models in your data center. Is there. ”
NVIDIA was the only company to submit results to all five MLPerf benchmarks, demonstrating the programmability and performance of computing platforms across a variety of AI workloads.
NVIDIA GPUs are used to accelerate large-scale inference workloads on the world's largest cloud infrastructure, including Alibaba Cloud, Amazon Web Services (AWS), Google Cloud Platform, Microsoft Azure, and Tencent. . In addition, world-leading companies such as Walmart and Procter & Gamble are running state-of-the-art AI workloads using NVIDIA's EGX edge computing platform and AI inference capabilities.
All of NVIDIA's MLPerf results were based on NVIDIA TensorRT 6. NVIDIA TensorRT 6 is high-performance deep learning inference software that makes it easy to deploy and optimize AI applications in production environments, from the data center to the edge. The new TensorRT optimizations are also available as open source in the Github repository.


"
