<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.heise.de/newsticker/meldung/Google-Entwicklerin-Maschinelles-Lernen-braucht-bessere-Lehrer-4578815.html"/>
    <meta property="og:site_name" content="heise online"/>
    <meta property="article:published_time" content="2019-11-06T07:07:00+00:00"/>
    <meta property="og:title" content="Google-Entwicklerin: Maschinelles Lernen braucht bessere Lehrer"/>
    <meta property="og:description" content="Wer maschinelles Lernen sicher machen möchte, muss sich vor allem Gedanken um das maschinelle Lehren machen, findet die Google-Entwicklerin Cassie Kozyrkov."/>
  </head>
  <body>
    <article>
      <h1>Google-Entwicklerin: Maschinelles Lernen braucht bessere Lehrer</h1>
      <h2>Wer maschinelles Lernen sicher machen möchte, muss sich vor allem Gedanken um das maschinelle Lehren machen, findet die Google-Entwicklerin Cassie Kozyrkov.</h2>
      <address><time datetime="2019-11-06T07:07:00+00:00">06 Nov 2019, 07:07</time> by <a rel="author">Sylvester Tremmel</a></address>
      <p>Künstliche Intelligenz ist Menschen überlegen, erklärte Cassie Kozyrkov, Googles Chief Decision Scientist, bei ihrer Präsentation zur Web-Summit-Konferenz in Lissabon. Das wäre aber kein Problem, KI sei ein Werkzeug wie jedes andere auch. Und wie jedes (nützliche) Werkzeug übertreffe es Menschen in seinem Anwendungsgebiet. Auch ein Hammer sei besser beim Nägel-in-die-Wand-Schlagen als Menschen. Angst oder Bedenken gegenüber KI-Systemen, die sich häufig aus Science-Fiction-Szenarios speisen, wären fehlgeleitet. Das Problem, so Kozyrkov, seien nicht KI-Systeme außerhalb menschlicher Kontrolle, das Problem seien KI-Systeme, mit denen Menschen nicht verantwortungsvoll umgehen – ein grundsätzliches Problem jedweder Technologie.</p>
      <p>Kozyrkov sieht <a href="https://www.heise.de/thema/K%C3%BCnstliche-Intelligenz">KI-Systeme</a> als Werkzeuge zum Programmieren. Sie erlauben es einer Maschine, Dinge über Beispiele beizubringen, statt über Instruktionen. Anders als beim klassischen Schreiben von Code sind dadurch nicht "10.000 Zeilen Code" erforderlich, in denen ein Entwickler sorgfältig und in Handarbeit beschreibt, wie eine gegebene Aufgabe zu lösen ist. Stattdessen wären es idealisiert zwei Zeilen: "Optimiere dies auf jenem Datensatz. Los!" Daher komme das enorme Potenzial von KI, weil es häufig einfacher sei, Verhalten über Beispiele zu beschreiben, als über explizite Instruktionen. Auch Menschen nutzen deshalb gerne Vormachen und Nachahmung, um einander etwas beizubringen.</p>
      <h3>Eine Frage der Verantwortung</h3>
      <p>Allerdings müsse in diese idealisierten zwei Zeilen Code die gleiche Sorgfalt gesteckt werden, wie in die 10.000 expliziten Instruktionen, sagt Kozyrkov. Angesichts der vordergründigen Einfachheit, mit der sich KI-Systeme trainieren lassen, ist es verführerisch, nicht so sorgfältig vorzugehen und sich allerhand Probleme einzuhandeln. Nicht weil KI-Systeme fehlerhaft wären, oder sich verselbstständigen würden, sondern weil die lehrenden Menschen ihrer Verantwortung nicht gerecht werden und sich nicht ausreichend Gedanken über Optimierungsziel und Trainingsdaten machen.</p>
      <p>Um diese Verantwortung wahrzunehmen, sei es an der Zeit, sich auf maschinelles Lehren zu konzentrieren, also was genau einem lernfähigen System beigebracht wird. Kozyrkov präsentierte dazu vier Prinzipien für "sichere und effektive KI": Erstens müsse man "weise" auswählen, worauf überhaupt optimiert werden soll. Zweitens müssten die genutzten Trainingsdaten für dieses Optimierungsziel relevant sein und drittens müsse das Erlernte mittels gut gestellter Examen überprüft werden. Nichtsdestotrotz müsse man immer mit Fehlern rechnen, weswegen umsichtig Sicherungsmechanismen eingezogen werden müssten, um Fehler zu erkennen, beziehungsweise deren Auswirkungen abfangen zu können. (<a href="mailto:axk@heise.de">axk</a>)</p>
    </article>
  </body>
</html>