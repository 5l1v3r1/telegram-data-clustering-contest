,src,lang,text
0,"











Google-Entwicklerin: Maschinelles Lernen braucht bessere Lehrer
Wer maschinelles Lernen sicher machen möchte, muss sich vor allem Gedanken um das maschinelle Lehren machen, findet die Google-Entwicklerin Cassie Kozyrkov.
06 Nov 2019, 07:07 by Sylvester Tremmel
Künstliche Intelligenz ist Menschen überlegen, erklärte Cassie Kozyrkov, Googles Chief Decision Scientist, bei ihrer Präsentation zur Web-Summit-Konferenz in Lissabon. Das wäre aber kein Problem, KI sei ein Werkzeug wie jedes andere auch. Und wie jedes (nützliche) Werkzeug übertreffe es Menschen in seinem Anwendungsgebiet. Auch ein Hammer sei besser beim Nägel-in-die-Wand-Schlagen als Menschen. Angst oder Bedenken gegenüber KI-Systemen, die sich häufig aus Science-Fiction-Szenarios speisen, wären fehlgeleitet. Das Problem, so Kozyrkov, seien nicht KI-Systeme außerhalb menschlicher Kontrolle, das Problem seien KI-Systeme, mit denen Menschen nicht verantwortungsvoll umgehen – ein grundsätzliches Problem jedweder Technologie.
Kozyrkov sieht KI-Systeme als Werkzeuge zum Programmieren. Sie erlauben es einer Maschine, Dinge über Beispiele beizubringen, statt über Instruktionen. Anders als beim klassischen Schreiben von Code sind dadurch nicht ""10.000 Zeilen Code"" erforderlich, in denen ein Entwickler sorgfältig und in Handarbeit beschreibt, wie eine gegebene Aufgabe zu lösen ist. Stattdessen wären es idealisiert zwei Zeilen: ""Optimiere dies auf jenem Datensatz. Los!"" Daher komme das enorme Potenzial von KI, weil es häufig einfacher sei, Verhalten über Beispiele zu beschreiben, als über explizite Instruktionen. Auch Menschen nutzen deshalb gerne Vormachen und Nachahmung, um einander etwas beizubringen.
Eine Frage der Verantwortung
Allerdings müsse in diese idealisierten zwei Zeilen Code die gleiche Sorgfalt gesteckt werden, wie in die 10.000 expliziten Instruktionen, sagt Kozyrkov. Angesichts der vordergründigen Einfachheit, mit der sich KI-Systeme trainieren lassen, ist es verführerisch, nicht so sorgfältig vorzugehen und sich allerhand Probleme einzuhandeln. Nicht weil KI-Systeme fehlerhaft wären, oder sich verselbstständigen würden, sondern weil die lehrenden Menschen ihrer Verantwortung nicht gerecht werden und sich nicht ausreichend Gedanken über Optimierungsziel und Trainingsdaten machen.
Um diese Verantwortung wahrzunehmen, sei es an der Zeit, sich auf maschinelles Lehren zu konzentrieren, also was genau einem lernfähigen System beigebracht wird. Kozyrkov präsentierte dazu vier Prinzipien für ""sichere und effektive KI"": Erstens müsse man ""weise"" auswählen, worauf überhaupt optimiert werden soll. Zweitens müssten die genutzten Trainingsdaten für dieses Optimierungsziel relevant sein und drittens müsse das Erlernte mittels gut gestellter Examen überprüft werden. Nichtsdestotrotz müsse man immer mit Fehlern rechnen, weswegen umsichtig Sicherungsmechanismen eingezogen werden müssten, um Fehler zu erkennen, beziehungsweise deren Auswirkungen abfangen zu können. (axk)


",de,"











Google Developer: Machine learning needs better teachers
If you want to make machine learning safe, you have to think about machine teaching, says Google developer Cassie Kozyrkov.
06 Nov 2019, 07:07 by Sylvester Tremmel
Artificial intelligence is superior to humans, said Cassie Kozyrkov, Google's chief decision scientist, at her presentation at the Web Summit conference in Lisbon. That would be no problem, AI is a tool like any other. And like any (useful) tool, it surpasses people in its field of application. Even a hammer would be better at nails-in-the-wall beating than humans. Fear or misgivings about AI systems, which often feed on sci-fi scenarios, would be misguided. According to Kozyrkov, the problem is not AI systems beyond human control, the problem is AI systems that people do not deal with responsibly - a fundamental problem of any technology.
Kozyrkov sees AI systems as tools for programming. They allow a machine to teach things about examples rather than instructions. Unlike traditional code writing, this does not require ""10,000 lines of code"" in which a developer painstakingly and cautiously describes how to solve a given task. Instead, it would be idealized two lines: ""Optimize this on that record. Therefore, the enormous potential of AI comes, because it is often easier to describe behavior through examples than through explicit instructions. That's why people like to use their stuff and imitation to teach each other something.
A question of responsibility
However, the same care must be taken in these idealized two lines of code as in the 10,000 explicit instructions, says Kozyrkov. Given the superficial simplicity with which AI systems can be trained, it is tempting not to be so careful and to deal with all sorts of problems. Not because AI systems are flawed or self-reliant, but because the educating people do not live up to their responsibilities and do not think sufficiently about optimization goals and training data.
To take on this responsibility, it is time to focus on machine teaching, which is exactly what a learning system is taught. Kozyrkov presented four principles for ""safe and effective AI"": First, one must choose ""wise"", what should be optimized at all. Secondly, the training data used would have to be relevant for this optimization goal, and thirdly, what has been learned must be checked by means of well-passed exams. Nevertheless, one must always count on errors, which is why prudent security mechanisms would have to be called in to detect errors or to be able to intercept their effects. (Axk)


"
