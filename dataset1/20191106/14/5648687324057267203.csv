,src,lang,text
0,"











Smart Speaker
Schwachstelle in Smart Speakern: Forschende steuern Alexa, Siri und Google Assistant mit Licht
06 Nov 2019, 14:49 by Tanja Banner

Smart Speaker sind beliebt - doch nun haben Forschende in Alexa, Siri und Google Assistant eine Sicherheitslücke entdeckt, die Besitzer von Smart Speakern aufhorchen lassen sollte.

Sprach-Assistenten wie Siri, Alexa oder Google Assistant sind beliebt. Der Nutzer spricht ein Kommando - und das Gerät führt es aus. Das reicht von einfachen Aufgaben wie dem Abspielen eines Lieds oder dem Programmieren der Erinnerungsfunktion bis hin zum Steuern externer Geräte wie Lampen, Türschlösser oder Garagentore.
Vor diesem Hintergrund ist eine Schwachstelle, die Forschende in Mikrofonen entdeckt haben, besonders heikel: Ihnen ist es gelungen, verschiedene Sprachassistenten zu hacken - und zwar nicht per Sprachkommandos, sondern mit Lichtsignalen. „Light Commands“ haben Forschende der University of Michigan und der University of Electro Communications in Tokio die Schwachstelle genannt, die sie gefunden haben.
Alexa, Siri und Google Assistant reagieren auf Lichtsignale
Im Versuch ist es den Forschenden gelungen, Siri, Google Assistant* und Alexa mit Hilfe eines Laserstrahls auszutricksen. Das gelang in unterschiedlichen Szenarien: aus nächster Nähe, über eine Distanz von 110 Metern und sogar über eine lange Distanz und durch ein geschlossenes Fenster.
„Es ist möglich, Mikrofone dazu zu bringen, auf Licht so zu reagieren, als ob es Klang wäre“, erklärt der Forscher Takeshi Sugawara gegenüber dem Magazin „Wired“. „Das bedeutet, dass jedes Gerät, das auf Klang reagiert, auch auf Lichtkommandos reagiert.“






Wie funktioniert der Hacker-Angriff auf Alexa, Siri und Google Assistant?
Die MEMS-Mikrofone, die in Smart Speakern und Smartphones verbaut sind, verwandeln Klang in elektrische Signale. Allerdings lässt sich dieser Mechanismus durch Licht austricksen: Der Laserstrahl bringt ein Bauteil des Mikrofons zum Schwingen, wodurch die elektrischen Signale entstehen. So schicken die Forschenden unhörbare Signale an die Smart Speaker, die darauf reagieren.
Welche Gefahr besteht für Besitzer von Alexa, Siri und Google Assistant?
Mit Hilfe von „Light Commands“ könnten beispielsweise Garagentore geöffnet werden, die mit dem Smart Speaker verbunden sind, warnen die Forschenden. Genau dieses Szenario hatten sie in ihren Tests ausprobiert. Aber auch nicht autorisierte Online-Einkäufe, das Öffnen von Haustüren oder das Entriegeln und Starten von Fahrzeugen seien so möglich.



Wie kann man den Angriff auf Alexa, Siri und Google Assistant verhindern?
Um über längere Strecken die richtige Stelle von Alexa, Siri oder Google Assistant mit dem Laser zu treffen, benötigten die Forschenden unter anderem ein Teleobjektiv oder ein Teleskop - der Angriff ist also nicht ganz günstig. Außerdem kann man ihn verhältnismäßig einfach verhindern: Der Smart Speaker darf einfach nicht von außen einsehbar sein - sollte also beispielsweise nicht am Fenster stehen.
Die Forschenden schlagen zudem Mechanismen vor, die den Zugriff aus der Ferne verhindern sollen: So könnte der Smart Speaker vor dem Ausführen eines kritischen Befehls eine Frage stellen und den Befehl erst nach erhaltener Antwort ausführen. Auch möglich sei es, verschiedene Mikrofone zu nutzen: Der Smart Speaker reagiert dann nur, wenn er das Kommando über mehrere Mikrofone empfangen hat.



Smart Speaker: Beliebt und umstritten
Smart Speaker sind beliebt und weit verbreitet: Es gibt Statistiken, wonach jeder vierte US-Amerikaner einen smarten Lautsprecher besitzt. Amazon hat nach eigenen Angaben weltweit fast 120 Millionen Geräte mit Alexa-Unterstützung verkauft - Stand Anfang 2019. Mittlerweile dürften es deutlich mehr sein.
Aber Siri, Alexa und Co. sind auch umstritten: Sie hören jederzeit Gespräche mit*, um bei Bedarf reagieren zu können, kritisieren Datenschützer. Für Empörung sorgten vor einiger Zeit die Enthüllungen, dass Amazon teilweise Mitarbeiter die aufgezeichneten Befehle an Alexa anhören und abtippen lässt - um die Spracherkennung zu verbessern. Auch bei Apples Siri wurde ein entsprechendes Vorgehen bekannt. Mittlerweile haben beide Unternehmen Fehler zugegeben.
Von Tanja Banner

*fr.de ist Teil der bundesweiten Ippen-Digital-Zentralredaktion.



",de,"











Smart Speaker
Vulnerability in Smart Speakers: Researchers Control Alexa, Siri and Google Assistant with Light
06 Nov 2019, 14:49 by Tanja Banner

Smart speakers are popular - but now researchers in Alexa, Siri and Google Assistant have discovered a security hole that should make owners of smart speakers sit up and take notice.

Speech assistants like Siri, Alexa or Google Assistant are popular. The user speaks a command - and the device executes it. This ranges from simple tasks such as playing a song or programming the reminder function to controlling external devices such as lamps, door locks or garage doors.
Against this background, a weak spot that researchers have discovered in microphones is particularly tricky: they have succeeded in hacking various language assistants - not with voice commands, but with light signals. ""Light Commands"" researchers from the University of Michigan and the University of Electro Communications in Tokyo have called the vulnerability they have found.
Alexa, Siri and Google Assistant respond to light signals
In the experiment, the researchers managed to outsmart Siri, Google Assistant * and Alexa with the help of a laser beam. This succeeded in different scenarios: up close, over a distance of 110 meters and even over a long distance and through a closed window.
""It's possible to make microphones react to light as if it were sound,"" says researcher Takeshi Sugawara to Wired magazine. ""This means that any device that responds to sound also responds to light commands.""






How does the hacker attack on Alexa, Siri and Google Assistant?
The MEMS microphones, which are installed in smart speakers and smartphones, transform sound into electrical signals. However, this mechanism can be outsmarted by light: The laser beam causes a component of the microphone to vibrate, resulting in the electrical signals. The researchers send inaudible signals to the smart speakers who respond.
What's the risk for Alexa, Siri, and Google Assistant owners?
With the help of ""Light Commands"", for example, garage doors that are connected to the smart speaker could be opened, the researchers warn. It was exactly this scenario they had tried in their tests. But even unauthorized online shopping, the opening of doors or the unlocking and starting of vehicles are so possible.



How to prevent the attack on Alexa, Siri and Google Assistant?
In order to hit the right spot of Alexa, Siri or Google Assistant with the laser for longer distances, the researchers needed, among other things, a telephoto lens or a telescope - so the attack is not very favorable. In addition, it can be relatively easy to prevent: The Smart Speaker may simply not be visible from the outside - so for example should not be at the window.
The researchers also propose mechanisms to prevent remote access: So the smart speaker could ask a question before executing a critical command and execute the command only after receiving the answer. It is also possible to use different microphones: The Smart Speaker responds only if he has received the command of several microphones.



Smart Speaker: Popular and controversial
Smart speakers are popular and widespread: There are statistics that every fourth American has a smart speaker. Amazon claims to have sold nearly 120 million devices worldwide with Alexa support - as of early 2019. By now, it's likely to be more.
But Siri, Alexa and Co. are also controversial: they always hear conversations with *, to respond if necessary, criticize privacy advocates. Indignation caused some time ago, the revelations that Amazon partially employees listen to the recorded commands to Alexa and typing - to improve speech recognition. Even with Apple's Siri a corresponding procedure was known. Meanwhile, both companies have admitted mistakes.
By Tanja Banner

* fr.de is part of the nationwide Ippen-Digital-Zentralredaktion.



"
