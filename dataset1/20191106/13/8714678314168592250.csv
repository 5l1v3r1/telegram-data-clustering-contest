,src,lang,text
0,"











- Kontrollstudier ute av kontroll
06 Nov 2019, 13:12 by Christian Bø

- De siste årene har vi sett en stor vekst av kontrollstudier, som skal bevise effekten av reklame. Erfaringer viser derimot at det er noe alvorlig galt med denne metoden, skriver Christian Bø i Mindshare.

Vi lever i en verden hvor vi sjelden kan sette to streker under svaret. Ingen metode for måling av reklame-effekt er perfekt, alle har sine svakheter. Selv om vi i markedsavdelinger, byråer og medier skulle ønske det var vår kampanje som skapte effekten, må vi være ærlig og si at det ofte er helt andre årsaker til at våre KPI-er flytter seg. Været, sesongen, distribusjonen, produktet, embalasjen, prisen, word of mouth, makrofaktorer, redaksjonell omtale er alle faktorer som kan påvirke både salg og oppfatning.
Intensjonen for å måle effekt med kontroll/eksponert-studier er god, men den har store utfordringer. Metoden innebærer å sammenlikne responsen til to grupper mennesker – en gruppe som er eksponert for en annonse, og en som ikke er det. Dette gjøres i hovedsak ved å lage en kontrollgruppe som er lik den eksponerte gruppen, og sammenligne de to.
I utgangspunktet kan dette kan være en god måte å måle effekt på. Dersom den eksponerte gruppen har høyere oppmerksomhet, kjennskap, preferanse eller kjøpsintensjon enn kontrollgruppen ser vi en såkalt uplift, brandlift eller kampanjeeffekt.
Selv om metoden tilsynelatende virker god, ser vi på mottakersiden utallige eksempler på merkelige og åpenbart gale resultater. Blant annet finner vi eksempler på at reklameoppmerksomheten er større i en kontrollgrupp enn i den eksponerte gruppen, store utvalgsforskjeller, eller at kampanjer har skapt en signifikant økning for andre merker enn sitt eget. Det tyder på at noe er alvorlig galt med denne metoden.
En av årsakene til at kontroll/eksponert-studier i mange tilfeller ikke er egnet til å måle effekt, skyldes tekniske årsaker. Skillet mellom kontroll og eksponert er ofte basert på cookies, og vi vet godt forskjellen på cookies og mennesker. Cookies utløper, er utfordrende å koble på tvers av enheter og tar ikke høyde for eventuell samseeing. Det er derfor ingen garanti for at den eksponerte gruppen faktisk er eksponert.
Det er heller ingen garanti for at kontrollgruppen ikke er det. Det betyr at vi ikke med sikkerhet kan si at en person er eksponert og en annen person ikke er det – selv i digitale kanaler alene.
En annen utfordring er i hvilken grad man kan kontrollere for andelen som har sett reklamen i andre kanaler (spesielt offline). Mange mennesker i begge utvalgene kan ha blitt eksponert for kampanjen på både TV, print eller utendørsannonser.
Utfordringen er at vi ikke vet hvem som har blitt eksponert. Har de to gruppene blitt eksponert like mye? Eller har den ene gruppen blitt eksponert mer enn den andre? Når studier viser ekstremt høy oppmerksomhet i en gruppe som ikke har blitt eksponert for reklamen bør man stille seg disse spørsmålene.
Den største og viktigste utfordringen med bruk av kontroll/eksponert-studier er mangler i opprettelsen av kontrollgrupper. For å sammenligne to grupper trenger vi to identiske og randomiserte grupper, der den eneste forskjellen skal være om gruppen har blitt eksponert eller ikke.
Dette byr raskt på problemer når man kjøper en bestemt målgruppe, spesielt når det er basert på atferd eller interesse. Det er noe enklere ved kun demografiske variabler, men kan fortsatt være vanskelig.
Det er store forskjeller i annonsens relevans og en persons oppmerksomhet tilknyttet en bestemt produktkategori de er interessert i eller i kjøpsmodus for. Dersom man sammenligner dette med en kontrollgruppe som ikke har samme interesser eller er i samme fase i kjøpsprosesssen, vil du få ekstremt skjeve svar.
Nylig så jeg en studie som sammenlignet Norges befolkning med en eksponert gruppe som var tydelig overrepresentert av unge menn. Dette får åpenbart konsekvenser for oppmerksomhet og oppfatning av merkevaren. Skal man gjøre dette optimalt er man avhengig av å gjenskape en kontrollgruppe som er lik demografisk, er i samme fase i kjøpsprosessen og har de samme interessene. Det er mulig, men ofte veldig vanskelig.
Professor Fred Selnes, og Førsteamanuensis Auke Hunneman ved Handelshøyskolen BI, la nylig frem funn fra en stor amerikansk studie, gjennomført for Facebook-annonsering, og diskuterer skjevheter i kontroll/eksponert-studier. På Facebook har vi færre tekniske utfordringer pga. innlogging, det var annonser som kun ble vist på Facebook, og Facebook har også gode tall på både atferd, interesse og demografi slik at man i større grad kan gjenskape en lik kontrollgruppe. Allikevel fant forskerne utvalgsfeil i nesten alle studiene. Utvalget sto derfor for en overveldende andel av effekten, og ikke reklamen.
Vi skulle ønske vi kunne konkludere at reklamen skapte effekten, og jeg skulle ønske noen kunne bedre isolere for alle disse tre utfordringene. Mitt ønske er at vi som bransje blir enda flinkere til å måle effekt, men i praksis er dette mer komplekst enn dagens kontroll/eksponert-studier.
Her er mine råd for veien videre:
1. Vær forsiktig med å konkludere med denne metoden når kampanjen har hatt et høyt trykk i andre kanaler man ikke kan måle i sammenheng. Sannsynligheten for at det er andre faktorer som påvirker er stor.
2. Sammenlign med like og tidligere tester, det sier noe om kampanjen er bedre eller dårligere enn snittet med samme metode, og man kan trekke læringer fra.
3. Gjort riktig kan den i beste fall si noe om den enkelte kanals bidrag – de sier f.eks ingen ting om den totale oppmerksomhet for kampanjen.
4. Sørg for at den eksponerte gruppen og kontrollgruppen faktisk er identisk og randomisert.


",no,"











- Control studies out of control
06 Nov 2019, 13:12 by Christian Bø

- In recent years, we have seen a large growth in control studies, which will prove the effect of advertising. However, experience shows that there is something seriously wrong with this method, writes Christian Bø in Mindshare.

We live in a world where we can rarely put two lines under the answer. No method of measuring advertising effect is perfect, everyone has their weaknesses. Although in marketing departments, agencies and media we wish it was our campaign that created the effect, we have to be honest and say that there are often other reasons why our KPIs are moving. Weather, season, distribution, product, packaging, price, word of mouth, macro factors, editorial publicity are all factors that can affect both sales and perception.
The intention to measure efficacy with control / exposure studies is good, but it has major challenges. The method involves comparing the response to two groups of people - one that is exposed to an ad and one that is not. This is mainly done by creating a control group similar to the exposed group and comparing the two.
Basically, this can be a good way to measure impact. If the exposed group has higher attention, knowledge, preference or purchase intention than the control group, we see a so-called uplift, fire lift or campaign effect.
Although the method seems to work well, we see on the receiving end countless examples of strange and obviously wrong results. Among other things, we find examples of the fact that advertising attention is greater in a control group than in the exposed group, large sample differences, or that campaigns have created a significant increase for brands other than their own. This indicates that something is seriously wrong with this method.
One of the reasons why control / exposure studies are in many cases not suitable for measuring efficacy is due to technical reasons. The distinction between control and exposure is often based on cookies, and we know well the difference between cookies and people. Cookies expire, are challenging to connect across devices and do not take into account any co-existence. Therefore, there is no guarantee that the exposed group is actually exposed.
There is also no guarantee that the control group is not. This means that we cannot say with certainty that one person is exposed and another person is not - even in digital channels alone.
Another challenge is the degree to which one can control for the proportion who have seen the advertising in other channels (especially offline). Many people in both selections may have been exposed to the campaign on TV, print or outdoor advertising.
The challenge is that we do not know who has been exposed. Have the two groups been exposed equally? Or has one group been exposed more than the other? When studies show extremely high attention in a group that has not been exposed to advertising, these questions should be asked.
The biggest and most important challenge with the use of control / exposed studies is deficiencies in the creation of control groups. To compare two groups, we need two identical and randomized groups, the only difference being whether the group has been exposed or not.
This quickly presents problems when buying a particular target audience, especially when it is based on behavior or interest. It is somewhat simpler for only demographic variables, but can still be difficult.
There are major differences in the relevance of the ad and a person's attention to a particular product category they are interested in or in purchase mode for. If you compare this with a control group that does not have the same interests or is in the same phase of the buying process, you will get extremely skewed answers.
Recently, I saw a study comparing Norway's population with an exposed group that was clearly overrepresented by young men. This obviously has consequences for the brand's awareness and perception. To achieve this optimally, one must depend on recreating a control group that is similar to the demographic, is in the same phase of the purchasing process and has the same interests. It is possible, but often very difficult.
Professor Fred Selnes, and Associate Professor Auke Hunneman at BI Norwegian Business School, recently presented findings from a large American study conducted for Facebook advertising, discussing bias in control / exposure studies. On Facebook we have fewer technical challenges due to. login, there were ads that only appeared on Facebook, and Facebook also has good numbers on both behavior, interest and demographics so that one can more easily recreate a similar control group. Nevertheless, researchers found sample errors in almost all studies. The committee therefore accounted for an overwhelming share of the effect, and not the advertising.
We wish we could conclude that advertising created the effect, and I wish someone could better isolate for all these three challenges. My wish is that we as an industry become even better at measuring impact, but in practice this is more complex than today's control / exposure studies.
Here are my tips for the road ahead:
1. Be careful about concluding with this method when the campaign has had a high pressure on other channels that cannot be measured in context. The likelihood that there are other factors affecting is great.
2. Compare with the same and previous tests, it says if the campaign is better or worse than the average with the same method and you can deduct learnings.
3. Done correctly, at best, they can say something about each channel's contribution - for example, they say nothing about the overall awareness of the campaign.
4. Make sure the exposed group and control group are actually identical and randomized.


"
