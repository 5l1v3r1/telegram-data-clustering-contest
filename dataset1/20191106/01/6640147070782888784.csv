,src,lang,text
0,"











Intervista a Maria Yap: tra Realtà Aumentata e AI, il futuro di Adobe
06 Nov 2019, 01:08 by Andrea Ferrario



Durante l’Adobe MAX 2019 abbiamo intervistato Maria Yap, VP Digital Imaging, per capire meglio i trend attuali e futuri di Adobe e i suoi software dedicati ai Creators.
Tom’s: Molti servizi, oggigiorno, utilizzano il cloud. Non solo per immagazzinare i dati, ma anche per funzionare direttamente nel cloud. Possiamo aspettarci una simile integrazione, in futuro, anche per tutti i software Adobe?
Maria Yap: conosciamo bene i benefici del cloud, soprattutto quelli legati alle prestazioni, che permettono anche a chi possiede un dispositivo dalle basse prestazioni di accedere ad applicazioni molto impegnative. Ma tutti noi abbiamo anche dei dispositivi nelle nostre mani, che ci permettono di accedere al cloud, ma che sono anche molto potenti. I notebook e anche gli smartphone moderni hanno molta potenza da offrire.
Il problema principale del Cloud è oggi legato alla latenza delle connessioni. Non è tutto immediato, c’è sempre un ritardo, più o meno lungo, tra l’invio dei comandi e l’azione che ne consegue.
Il futuro sarà un mix delle due cose. Utilizzare la potenza del cloud ma anche quella dei dispositivi, sfruttando le caratteristiche di entrambi, per offrire l’esperienza migliore.
Tom’s: un focus di Adobe per questo Adobe Max 2019 è senza dubbio il mondo mobile, con la presentazione di Photoshop per iPad. Secondo noi, quello che oggi manca veramente nel mercato dei dispositivi mobile, smartphone e tablet, è un’applicazione per il montaggio video seria. Possiamo aspettarci una versione per tablet di Premiere Pro ?
Maria Yap: credo che l’evoluzione che abbiamo visto nel mondo dell’imaging potrà avvenire anche per il mondo dei video. Abbiamo, nella nostra suite, applicazioni molto potenti, e possiamo sfruttare il cloud per renderle disponibili sui vari dispositivi. Quello che abbiamo fatto con Photoshop, il nostro primo passo, è stato portare i file PSD, che possono essere molto pesanti, all’interno dei tablet per essere gestiti. Questo può accadere anche con i video, ma la mole di dati da gestire è molto più grande, quindi è una sfida maggiore, ma certamente possiamo fare molto bene anche in questo mercato.
Tom’s: quando viene realizzata un’applicazione per tablet o smartphone, solitamente quell’applicazione è solo una versione molto ridotta delle controparte per computer. Photoshop per iPad è forse la prima applicazione che cerca di non limitare le caratteristiche, anche se non è ancora identica a quella per desktop. Da cosa deriva questo approccio conservativo?
Maria Yap: solitamente quando portiamo un software in un nuovo dispositivo, vogliamo trovare il giusto bilanciamento tra portare il meglio che abbiamo, e sfruttare le potenzialità del dispositivo. Ci chiediamo quali sono i “superpoteri”, così li chiamiamo, di quel dispositivo. Ad esempio gli smartphone hanno delle fotocamera fenomenali oggigiorno, e quindi ci chiediamo come possiamo sfruttare al massimo la caratteristica peculiari di quel dispositivo.
Cosa cercano le persone da quel dispositivo? Quali sono le potenzialità del dispositivo? Come possiamo unire le due cose per offrire l’esperienza migliore ? E solitamente con la prima versione delle applicazioni cerchiamo di trovare il miglior bilanciamento, ma poi aggiungiamo velocemente ulteriori caratteristiche e funzioni.
Ricordiamoci che non tutti gli utenti sono power-user, anzi è proprio il contrario. Non vogliamo travolgere l’utente con troppi strumenti e funzioni. Iniziamo in maniera cauta, vediamo come l’utente usa l’applicazione, e in base al comportamento sviluppiamo le versioni future, aggiungendo funzioni e caratteristiche che possano offrire agli utenti tutti gli strumenti che richiedono e usano per la maggiore.
Tom’s: avete presentato Aero che porta le applicazioni Adobe nella Realtà Aumentata. Ma parliamo di realtà virtuale, avete progetti di sviluppare applicazioni per la VR?
Maria Yap: abbiamo alcuni progetti per la realtà virtuale, ad esempio Premiere Pro permette di gestire video VR. Tuttavia ci sono degli ostacoli, e il primo è sicuramente che non tutti si trovano a loro agio all’intero della realtà virtuale per via del motion sickness. Per ora la VR rimane una nicchia, accessibile solo a pochi. Noi abbiamo deciso di seguire il mondo dell’AR perché è più accessibile. Basta prendere il vostro smartphone e potete utilizzarla. Ogni artista di Photoshop può mettere a disposizione la propria creazione per la realtà aumentata, e ogni persona con uno smartphone può ammirarla. Noi pensiamo che sia una grande opportunità, e non vediamo l’ora di vedere a cosa porterà. In trent’anni di sviluppo quello che abbiamo imparato è che quando dai un nuovo strumento a un “Creator”, spesso è in grado di creare qualcosa di più emozionante di quanto avessimo mai immaginato.



Tom’s: abbiamo visto come Sensei, la vostra AI, possa velocizzare molte operazioni. Qual è il vostro scopo con Sensei? Non pensate che possa limitare la creatività degli artisti?
Maria Yap: l’AI rappresenta certamente un grande avanzamento tecnologico in grado di portare molta innovazione. Per molti anni, aggiungere nuove funzioni è significato realizzare nuovi algoritmi, un lavoro molto lungo e complesso. Quello che abbiamo visto è che combinare gli algoritmi di ieri con il machine learning di oggi ci permette di capire di più di una immagine. Ed è qui che diventa tutto interessante. Noi alleniamo il computer per atteggiarsi come un umano, per vedere come un essere umano. I PC vedono solo pixel e numeri, non distinguono tra oggetti, panorami o altro. Gli essere umani invece vedono e capiscono quello che hanno davanti, se si tratta di un ortaggio o una sedia. Questo è quello che l’AI può fare, capire cosa è rappresentato in quell’immagine e quindi compiere azioni di conseguenza. Ad esempio, come abbiamo visto, sapere che in quell’immagine è rappresentato un ortaggio, permette di selezionarlo in maniera estremamente più efficiente e precisa, cosa impossibile se si considerano solo le informazioni presenti nei singoli pixel.
L’Intelligenza Artificiale aiuta il computer a essere più umano e in grado di capire quello che vede. Detto questo, la creatività umana non può essere sostituita. L’AI vuole rendere i computer più umani per permette a noi di interagirci in maniera più immediata, intuitiva e veloce.
Tom’s: Come allenate Sensei? Utilizzate anche i dati degli utenti?
Maria Yap: l’approccio è quello della “classificazione”, alleniamo l’IA a capire il tipo di immagine, se si tratta di albero, un frutto, un’auto, etc. Prendiamo database di moltissime immagini che aziende mettono a disposizione. Non utilizziamo i dati degli utenti o le loro immagini. Se mai lo faremo, chiederemo esplicito consenso prima.
I dati che raccogliamo dagli utenti, in maniera anonima, è il modo in cui utilizzano i software. Ad esempio se dopo aver cliccato su un pulsante, selezionano una determinata voce di menù, o un altro pulsante. In questa maniera possiamo definire dei comportamenti e utilizzarli per migliorare i nostri software. Se ad esempio scopriamo che vengono fatte una serie di azioni in successione, possiamo rendere quelle azioni disponibili in un comando specifico così da velocizzare l’interazione con i nostri software.
Per tutte le novità da Adobe MAX, visita questa pagina


",it,"











Interview with Maria Yap: between Augmented Reality and AI, the future of Adobe
06 Nov 2019, 01:08 by Andrea Ferrario



During the Adobe MAX 2019 we interviewed Maria Yap, VP Digital Imaging, to better understand the current and future trends of Adobe and its software dedicated to Creators.
Tom’s: Many services today use the cloud. Not only to store data, but also to work directly in the cloud. Can we expect similar integration in the future for all Adobe software?
Maria Yap: we know well the benefits of the cloud, especially those related to performance, which also allow those who have a low performance device to access very demanding applications. But we all also have devices in our hands, which allow us to access the cloud, but which are also very powerful. Notebooks and even modern smartphones have a lot of power to offer.
The main problem of the Cloud is today linked to the latency of connections. It's not all immediate, there is always a delay, more or less long, between the sending of the commands and the action that follows.
The future will be a mix of the two. Use the power of the cloud but also that of the devices, exploiting the characteristics of both, to offer the best experience.
Tom's: an Adobe focus for this Adobe Max 2019 is undoubtedly the mobile world, with the Photoshop presentation for iPad. In our opinion, what is really missing in the mobile device market today, smartphones and tablets, is a serious video editing application. Can we expect a tablet version of Premiere Pro?
Maria Yap: I believe that the evolution we have seen in the world of imaging can also take place in the video world. We have very powerful applications in our suite, and we can use the cloud to make them available on various devices. What we did with Photoshop, our first step, was to bring PSD files, which can be very heavy, into tablets to be managed. This can also happen with videos, but the amount of data to manage is much larger, so it's a greater challenge, but we can certainly do very well in this market too.
Tom's: when a tablet or smartphone application is made, usually that application is only a very small version of the computer counterpart. Photoshop for iPad is perhaps the first application that tries not to limit the features, even if it is not yet identical to the desktop one. Where does this conservative approach come from?
Maria Yap: usually when we bring a software to a new device, we want to find the right balance between bringing the best we have, and exploiting the potential of the device. We ask ourselves what are the ""superpowers"", so we call them, of that device. For example, smartphones have phenomenal cameras these days, and so we wonder how we can make the most of the unique feature of that device.
What are people looking for from that device? What are the potentialities of the device? How can we combine the two to offer the best experience? And usually with the first version of the applications we try to find the best balance, but then we quickly add more features and functions.
Let us remember that not all users are power-users, rather it is just the opposite. We don't want to overwhelm the user with too many tools and features. Let's start cautiously, let's see how the user uses the application, and based on the behavior we develop future versions, adding features and features that can offer users all the tools they require and use the most.
Tom’s: you introduced Aero that brings Adobe applications in Augmented Reality. But let's talk about virtual reality, do you have plans to develop applications for VR?
Maria Yap: we have some projects for virtual reality, for example Premiere Pro allows you to manage VR videos. However, there are obstacles, and the first is surely that not everyone is comfortable with the whole of virtual reality because of the motion sickness. For now, VR remains a niche, accessible only to a few. We have decided to follow the world of AR because it is more accessible. Just take your smartphone and you can use it. Every Photoshop artist can make their own creation available for augmented reality, and every person with a smartphone can admire it. We think it's a great opportunity, and we can't wait to see what it brings. In thirty years of development what we have learned is that when you give a ""Creator"" a new tool, he is often able to create something more exciting than we ever imagined.



Tom’s: we have seen how Sensei, your AI, can speed up many operations. What is your purpose with Sensei? Don't you think it can limit the creativity of the artists?
Maria Yap: the AI certainly represents a great technological advancement capable of bringing much innovation. For many years, adding new features has meant making new algorithms, a very long and complex job. What we've seen is that combining yesterday's algorithms with today's machine learning allows us to understand more of an image. And it is here that everything becomes interesting. We train the computer to pose as a human, to look like a human being. PCs see only pixels and numbers, do not distinguish between objects, panoramas or other. Human beings, on the other hand, see and understand what they have before them, whether it is a vegetable or a chair. This is what the AI can do, understand what is represented in that image and then perform actions accordingly. For example, as we have seen, knowing that in that image a vegetable is represented, it allows to select it in an extremely more efficient and precise way, which is impossible if we consider only the information present in the single pixels.
Artificial Intelligence helps the computer to be more human and able to understand what it sees. That said, human creativity cannot be replaced. The AI wants to make computers more human to allow us to interact in a more immediate, intuitive and fast way.
Tom’s: How do you train Sensei? Do you also use user data?
Maria Yap: the approach is that of ""classification"", we train the AI to understand the type of image, whether it is a tree, a fruit, a car, etc. We take databases of many images that companies make available. We do not use user data or their images. If we ever do, we will ask for explicit consent first.
The data that we collect from users, anonymously, is the way in which they use the software. For example, if after clicking on a button, they select a specific menu item, or another button. In this way we can define behaviors and use them to improve our software. For example, if we find that a series of actions are performed in succession, we can make those actions available in a specific command so as to speed up the interaction with our software.
For all the news from Adobe MAX, visit this page


"
