,src,lang,text
0,"











Amazon Alexa, Apple Siri e Google Assistant possono essere hackerate con un laser
A quanto pare hackerare Amazon Alexa, Apple Siri e Google Assistant è davvero molto facile e basta l'utilizzo di un laser per farcela.
06 Nov 2019 by Simone Tagliaferri
A quanto pare gli assistenti vocali come Alexa di Amazon, Siri di Apple e Google Assistant sono facilissimi da hackerare. Per farlo basta puntare un laser sui loro microfoni, stando a un team di ricercatori dell'University of Electro-Communications di Tokyo e dell'University of Michigan.L'hack si chiama Light Commands e consente agli aggressori di dare comandi invisibili a distanza agli assistenti vocali. Puntando e modulando un laser sui microfoni MEMS (Microelectro-Mechanical Systems) i ricercatori hanno scoperto di poter dare comandi agli assistenti vocali usando la luce come se fosse suono.Gli autori dello studio hanno verificato di poter prendere il controllo di assistenti vocali posizionati a 110m di distanza dal laser, ma non si esclude che si possa fare anche da una distanza maggiore. In questo modo hanno dimostrato che in questi apparecchi l'autenticazione degli utenti è inesistente: chiunque può dargli ordini. Immaginate di aver collegato i sistemi di sicurezza della vostra abitazione a un assistente smart e capirete quale sia il pericolo di questa mancanza di controllo. La ricerca è stata condivisa con le compagnie Amazon, Apple, Google, Tesla e Ford e con gli enti ICS-CERT e FDA, con cui è stata concordata la pubblicazione per il 4 novembre 2019.





Fonte




",it,"











Amazon Alexa, Apple Siri and Google Assistant can be hacked with a laser
Apparently hacking Amazon Alexa, Apple Siri and Google Assistant is really very easy and just using a laser to do it.
06 Nov 2019 by Simone Tagliaferri
Apparently voice assistants like Amazon's Alexa, Apple's Siri and Google Assistant are very easy to hack. To do this, just place a laser on their microphones, according to a team of researchers from the University of Electro-Communications in Tokyo and the University of Michigan. The hack is called Light Commands and allows attackers to give invisible commands at a distance to the voice assistants. Focusing and modulating a laser on the MEMS microphones (Microelectro-Mechanical Systems) the researchers discovered they could give commands to the voice assistants using light as if it were sound. The authors of the study verified that they could take control of voice assistants positioned at 110m away from the laser, but it is not excluded that it can be done even from a greater distance. In this way, they have demonstrated that user authentication is non-existent in these devices: anyone can give them orders. Imagine that you have connected the security systems of your home to a smart assistant and you will understand the danger of this lack of control. The research was shared with the companies Amazon, Apple, Google, Tesla and Ford and with the ICS-CERT and FDA bodies, with which the publication was agreed for November 4, 2019.





Source




"
