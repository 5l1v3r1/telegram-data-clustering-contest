,src,lang,text
0,"











Inteligência Artificial considerada 'perigosa demais' agora está disponível ao público
Desenvolvida pela OpenAI, GPT-2 é capaz de analisar e completar texto fornecido por um humano, com resultados muito convincentes
06 Nov 2019, 20:13 by Rafael Rigues
Um sistema de inteligência artificial que foi considerado “perigoso demais” para ser lançado ao público finalmente está disponível a todo e qualquer interessado. Desenvolvido pela OpenAI, o GPT-2 funciona analisando um trecho de texto fornecido pelo usuário, a entrada, e produz uma saída relacionada.
Mas ao contrário de outros sistemas de geração de texto, que combinam frases e expressões pré-definidas com regras gramaticais rígidas, o GPT-2 é capaz de “improvisar” o texto de acordo com a entrada. E os resultados são assustadores, tanto do ponto de vista da consistência gramatical quanto do estilo, evitando os erros comuns que entregam rapidamente a origem de um texto criado por outros sistemas.
Por conta de sua precisão a OpenAI decidiu lançar o GPT-2 em partes, permitindo que pesquisadores estudassem e se acostumassem com sua capacidade aos poucos. O modelo completo usado para o treinamento da IA, conhecido como 1.5B, contém 1,5 bilhões de parâmetros. Inicialmente foram lançados 124 milhões deles, seguidos de 335 milhões e 774 milhões. Quanto mais parâmetros no modelo, mais preciso e “humano” é o texto.




Exemplo de texto gerado pela inteligência artificial GPT-2. O texto em negrito é o original fornecido pelo usuário,o restante foi gerado pela IA.

O medo a OpenAI é que sua IA seja usada para fins nefastos, como a produção de Fake News convincentes. O lançamento da ferramenta e de sua base de parâmetros foi feito justamente para auxiliar na pesquisa de ferramentas de detecção de texto sintético.
Um sistema de detecção criado pela OpenAI é capaz de identificar texto escrito pelo GPT-2 1.5B com precisão de 95%. Ainda assim, os criadores acreditam que isto não é “bom o bastante”, e que seu sistema precisa ser combinado a abordagens baseadas em metadados, julgamento humano e educação do público para que seja mais eficiente.
Por enquanto o GPT-2 só é capaz de gerar textos em inglês. É possível experimentar uma versão completa do sistema online, rodando com todos os 1,5 bihões de parâmetros. Mas cuidado, você vai se assustar.
Fonte: The Next Web







",pt,"











Artificial intelligence considered 'too dangerous' is now publicly available
Developed by OpenAI, GPT-2 is able to parse and complete text provided by a human, with very convincing results.
06 Nov 2019, 20:13 by Rafael Rigues
An artificial intelligence system that was deemed ""too dangerous"" to be released to the public is finally available to anyone and everyone. Developed by OpenAI, GPT-2 works by parsing a user-supplied piece of text, the input, and producing a related output.
But unlike other text generation systems, which combine predefined phrases and phrases with strict grammatical rules, GPT-2 is able to “improvise” text according to input. And the results are daunting from both a grammatical consistency and style standpoint, avoiding the common mistakes that quickly deliver the source of text created by other systems.
Due to its accuracy, OpenAI decided to release the GPT-2 in parts, allowing researchers to slowly study and get used to its ability. The complete model used for AI training, known as 1.5B, contains 1.5 billion parameters. Initially, 124 million were launched, followed by 335 million and 774 million. The more parameters in the model, the more accurate and “human” the text is.




Example of text generated by artificial intelligence GPT-2. The bold text is the original provided by the user, the rest was generated by AI.

The fear of OpenAI is that its AI will be used for harmful purposes, such as producing compelling Fake News. The launch of the tool and its parameter base was made precisely to assist in the search for synthetic text detection tools.
A detection system created by OpenAI is capable of identifying text written by GPT-2 1.5B with 95% accuracy. Still, the creators believe this is not “good enough,” and that their system needs to be combined with approaches based on metadata, human judgment, and public education to be more effective.
For now GPT-2 is only able to generate English texts. You can try a full version of the system online, running with all 1.5 billion parameters. But beware, you will be scared.
Source: The Next Web







"
