,src,lang,text
0,"











Sicherheitsforscher befehligen Alexa, Siri & Co. via Laserstrahl
Angreifer könnten unter Umständen Sprachassistenten in einem Lichtstrahl codierte Befehle unterschieben und so etwa ein smartes Türschloss öffnen.
06 Nov 2019, 10:44 by Dennis Schirrmacher
Die Sprachassistenzsysteme von unter anderem Amazon, Apple und Google sind für Attacken mit einem Laserstrahl anfällig. Dabei könnten Angreifer beispielsweise smarte Lautsprecher und Smartphones ins Visier nehmen und mit einem Lichtstrahl die Ausführung von Befehlen anstoßen.
Klappt das, könnten sie so eventuell in ein Smart Home eingebundene Geräte steuern und Haustüren oder Garagentore öffnen. Attacken klappen aber nur, wenn die Geräte in Sichtweite sind und eine bestimmte Entfernung nicht überschritten wird.
Licht statt Sprache
In einer Abhandlung zeigen Sicherheitsforscher der University of Michigan und der japanischen University of Electro-Communications, wie das aus 75 Metern Entfernung durch ein Fenster geklappt hat. Unter Laborbedingen ist ihnen das maximal aus einer Entfernung von 110 Metern gelungen. Voraussetzung für eine erfolgreiche Übermittlung ist, dass der Laserstrahl direkt auf das Mikrofon zielt.


In ihren Tests haben die Sicherheitsforscher auf Equipment im Wert von rund 500 Euro gesetzt.(Bild: lightcommands.com/ )

Mikrofone setzen aufgenommene Töne in elektrische Signale um. Die Sicherheitsforscher haben nun herausgefunden, dass man elektrische Signale in einen Lichtstrahl modulieren kann und damit das Mikrofon so anregen kann, als würde es einen Sprachbefehl bekommen. Dafür haben sie eigenen Angaben zufolge Equipment für rund 500 Euro benötigt.
Den Sicherheitsforschern zufolge ist die Prüfung, ob ein Sprachbefehl von einem registrierten Nutzer stammt, bei smarten Lautsprechern standardmäßig nicht aktiv. Beim Großteil von Smartphones und Tablets sei das wiederum der Fall. So könnten potenzielle Angreifer smarte Lautsprecher attackieren, ohne die Stimme des Besitzers imitieren zu müssen. Weiterhin schreiben sie, dass man Wake-up-Befehle wie ""Hey Siri"" und die Spracherkennung für Befehle des Besitzers unter Umständen auch umgehen konnte.
Populäre Geräte anfällig
Auf einer Website zu ihren Ergebnissen listen die Sicherheitsforscher verschiedene Geräte auf, mit denen sie ihre Laser-Attacke erfolgreich getestet haben. Darunter befinden sich beispielsweise die smarten Lautsprecher Amazon Echo, Google Home und die Smartphones iPhone XR und Samsung Galaxy S9. Auf der Seite findet man auch Angaben zur nötigen Leistung eines Laserstrahls und der maximalen Entfernungen für erfolgreiche Attacken.


In einem Versuch übermitteln die Sicherheitsforscher den Befehl zum Öffnen eines Garagentors, indem sie mit einem Laserstrahl von einem Gebäude durch das Fensters eines anderen Gebäudes auf einen smarten Lautsprecher zielen.

Angriffe möglich, aber eher unwahrscheinlich
Die Sicherheitsforscher gehen davon aus, dass alle Geräte betroffen sind, die auf sogenannte MEMS-Mikrofone (microelectro-mechanical systems) setzen. Schützen kann man sich vor solchen Laser-Attacken nur schwer. Bislang gehen die Forscher davon aus, dass es keine Angriffe dieser Art gegeben hat. Es ist eher unwahrscheinlich, dass ein Angreifer den smarten Lautsprecher im eigenen Schlafzimmer ins Visier nimmt.
Um dem Angriffsszenario entgegenzuwirken, empfehlen sie Herstellern, weitere Authentifizierungsschichten zu implementieren. Beispielsweise könnte ein Gerät einem Nutzer eine individuelle Frage stellen. Außerdem könnten Hersteller die MEMS-Mikrofone mit Schutzbarrieren umhüllen, um Lichtstrahlen abzuhalten. Auf der Gegenseite könnten Angreifer aber dann die Leistung eines Laserstrahls erhöhen. (des)


",de,"











Security researchers command Alexa, Siri & Co. via laser beam
Attackers might be able to defend speech assistants in commands encoded in a light beam, opening a smart door lock, for example.
06 Nov 2019, 10:44 by Dennis Schirrmacher
The language assistance systems of, inter alia, Amazon, Apple and Google are susceptible to attacks with a laser beam. For example, attackers could target smart speakers and smartphones and initiate commands with a beam of light.
If that works, they could control devices that might be integrated into a smart home and open doors or garage doors. But attacks only work if the devices are within sight and a certain distance is not exceeded.
Light instead of language
In a memoir, security researchers from the University of Michigan and the Japanese University of Electro-Communications show how it worked through a window 75 meters away. Under laboratory conditions, they have managed this at a maximum distance of 110 meters. Prerequisite for successful transmission is that the laser beam is aimed directly at the microphone.


In their tests, the security researchers have put on equipment worth around 500 euros. (Image: lightcommands.com/)

Microphones convert recorded sounds into electrical signals. The security researchers have now found that one can modulate electrical signals in a beam of light and thus the microphone can stimulate as if it would get a voice command. For this they have reportedly needed equipment for around 500 euros.
According to security researchers, checking whether a voice command comes from a registered user is by default not active on smart speakers. This is again the case for the majority of smartphones and tablets. So potential attackers could attack smart speakers without imitating the owner's voice. They also say that wake-up commands such as ""Hey Siri"" and speech recognition for owner commands may be bypassed.
Popular devices vulnerable
On a web site about their results, the security researchers list various devices with which they have successfully tested their laser attack. These include, for example, the smart speakers Amazon Echo, Google Home and the smartphones iPhone XR and Samsung Galaxy S9. On the page you will also find information about the required power of a laser beam and the maximum distances for successful attacks.


In an attempt, security researchers communicate the command to open a garage door by aiming a laser beam from one building through the window of another building onto a smart speaker.

Attacks possible, but unlikely
The security researchers assume that all devices are affected, which rely on so-called MEMS microphones (microelectro-mechanical systems). It is difficult to protect shooters from such laser attacks. So far, the researchers assume that there have been no attacks of this kind. It is unlikely that an attacker will target the smart speaker in his own bedroom.
To counteract the attack scenario, they recommend that manufacturers implement additional authentication layers. For example, a device could ask a user an individual question. In addition, manufacturers could wrap the MEMS microphones with protective barriers to block light beams. On the other side, attackers could then increase the power of a laser beam. (of)


"
