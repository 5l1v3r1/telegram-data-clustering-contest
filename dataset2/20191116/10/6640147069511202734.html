<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.tomshw.it/hardware/intel-le-nuove-soluzioni-di-intelligenza-artificiale-nervana-e-movidius/"/>
    <meta property="og:site_name" content="Tom's Hardware"/>
    <meta property="article:published_time" content="2019-11-16T10:25:00+00:00"/>
    <meta property="og:title" content="Intel, le nuove soluzioni di intelligenza artificiale Nervana e Movidius | Tom's Hardware"/>
    <meta property="og:description" content="Intel ha annunciato le soluzioni per l'intelligenza artificiale NNP-T100 e NNP-I1000, oltre a una nuova versione della VPU (Vision Processing Unit) Movidius."/>
  </head>
  <body>
    <article>
      <h1>Intel, le nuove soluzioni di intelligenza artificiale Nervana e Movidius | Tom's Hardware</h1>
      <address>
        <time datetime="2019-11-16T10:25:00+00:00">16 Nov 2019, 10:25</time>
      </address>
      <p><b>Intel</b> schiaccia l’acceleratore nel settore dell’<b>intelligenza artificiale</b> con due nuovi chip <b>Nervana NNP</b> (Neural Network Processor), <b>NNP-T100 e NNP-I1000</b>, rispettivamente dedicati all’addestramento di IA e al calcolo inferenziale. L’azienda ha anche annunciato la <b>nuova versione della VPU (Vision Processing Unit) Movidius</b>, per applicazioni di computer vision e inferenza sui dispositivi finali (edge computing).</p>
      <p>Secondo Intel, questi nuovi prodotti rafforzano il suo portfolio di soluzioni di intelligenza artificiale che dovrebbe generare oltre <b>3,5 miliardi di dollari</b> di fatturato nel 2019.</p>
      <figure>
        <img src="https://www.tomshw.it/images/images/2019/11/intel-nervana-63381.768x432.jpg"/>
      </figure>
      <p>“Con questa nuova fase dell’intelligenza artificiale, stiamo raggiungendo un punto di svolta in termini di hardware computazionale e memoria. Per proseguire con gli incredibili progressi nel campo dell’intelligenza artificiale, sono necessari prodotti hardware appositamente sviluppati, come i processori di rete neurale Intel Nervana NNP e le unità di elaborazione visiva Intel Movidius Myriad VPU. L’impiego di forme più avanzate di intelligenza artificiale a livello di sistema ci consentirà di passare dalla trasformazione dei dati in informazioni alla trasformazione di informazioni in conoscenza”, ha dichiarato Naveen Rao, Intel corporate vice president e general manager dell’Intel Artificial Intelligence Products Group.</p>
      <p><b>NNP-T1000</b>, in particolare, fornisce “il giusto equilibrio tra potenza di calcolo, comunicazione e memoria” pur essendo adattabile ad ogni tipo di sistema, dai piccoli <b>cluster</b> fino ai <b>supercomputer</b>. NNP-T offre inoltre il <b>95% di scalabilità</b> su sistemi come <b>Res-Net 50</b> e <b>BERT </b>(su sistemi a 32 schede), con una perdita del bandwidth minima quando si passa da un sistema a otto schede “in chassis” a uno a 32 schede “cross-chassis”.</p>
      <p>Per spingere l’adozione dei nuovi chip, Intel ha sviluppato un progetto di riferimento che consiste di 10 rack e 480 schede NNP-T1000 interconnesse tra loro usando un’interconnessione proprietaria.</p>
      <p><b>NNP-I1000</b>, secondo Intel, è ideale per eseguire inferenze multimodali intensive su scala reale usando fattori di forma flessibili. Comparato con un server rack che sfrutta <b>GPU Nvidia T4</b> per l’inferenza, lo stesso server dotato di Intel NNP-I1000 offre circa <b>quattro volte la densità computazionale</b> rispetto alla soluzione concorrente. “Ciò che abbiamo è il maggior numero di inferenze al secondo che si possano elaborare in una singola unità rack”, ha dichiarato Rao.</p>
      <figure>
        <img src="https://www.tomshw.it/images/images/2019/11/intel-nervana-63380.768x432.jpg"/>
      </figure>
      <p>Entrambe le unità sono state sviluppate da Intel per clienti che hanno l’esigenza di elaborare enormi quantità di dati attraverso l’intelligenza artificiale, come <b>Facebook</b> o <b>Baidu</b>.</p>
      <p>“Siamo entusiasti di lavorare con Intel per implementare calcoli di inferenza più rapidi ed efficienti con il processore di rete neurale Intel Nervana (NNP-I) ed espandere il supporto al nostro compilatore di deep learning <b>Glow</b>” ha affermato Misha Smelyanskiy, direttore dell’AI System Co-Design presso Facebook.</p>
      <p>La nuova VPU <b>Movidius</b>, prevista per la <b>prima metà del 2020</b>, includerà avanzamenti architetturali estremamente efficienti, volti ad offrire <b>prestazioni oltre 10 volte superiori alle prestazioni di inferenza della generazione precedente, con efficienza energetica fino a 6 volte superiore</b> rispetto alle soluzioni della concorrenza.</p>
      <figure>
        <img src="https://www.tomshw.it/images/images/2019/11/intel-movidius-63382.768x432.jpg"/>
      </figure>
      <p>Jonathan Ballon, vicepresidente del gruppo IoT di Intel, ha dichiarato che la nuova VPU Movidius offre prestazioni quattro volte migliori del chip Nvidia TX2 e le prestazioni sono pari al chip Xavier di Nvidia con un quinto dell’energia richiesta. Questo è importante perché i clienti guardano molto ai consumi, alle dimensioni e alla latenza, oltre che alle prestazioni.</p>
      <p>Intel ha anche annunciato il nuovo Intel DevCloud for the Edge che, insieme alla distribuzione Intel del toolkit OpenVINO, risolve un aspetto cruciale per gli sviluppatori, consentendo loro di sperimentare, prototipare e testare soluzioni di intelligenza artificiale su un’ampia gamma di processori Intel prima di acquistare hardware. Secondo Ballon, oltre 2700 clienti stanno già usando Intel DevCloud e ne sono entusiasti.</p>
    </article>
  </body>
</html>