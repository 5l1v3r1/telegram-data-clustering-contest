<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://slashdot.org/story/19/11/15/237230/algorithms-are-like-convex-mirrors-that-refract-human-biases"/>
    <meta property="og:site_name" content="slashdot.org"/>
    <meta property="article:published_time" content="2019-11-16T04:01:00+00:00"/>
    <meta property="og:title" content="'Algorithms Are Like Convex Mirrors That Refract Human Biases'"/>
    <meta property="og:description" content="Emil Protalinski, writing for VentureBeat: At the Movethedial Global Summit in Toronto yesterday, I listened intently to a talk titled &quot;No polite fictions: What AI reveals about humanity.&quot; Kathryn Hume, Borealis AI's director of product, listed a bunch of AI and algorithmic failures -- we've seen pl..."/>
  </head>
  <body>
    <article>
      <h1>'Algorithms Are Like Convex Mirrors That Refract Human Biases'</h1>
      <address><time datetime="2019-11-16T04:01:00+00:00">16 Nov 2019, 04:01</time> by <a rel="author">msmash</a></address>
      <p>Emil Protalinski, <a href="https://venturebeat.com/2019/11/14/hypernet-labs-raises-10-million-to-democratize-data-science/">writing for VentureBeat</a>:<i> At the Movethedial Global Summit in Toronto yesterday, I listened intently to a talk titled "No polite fictions: What AI reveals about humanity." Kathryn Hume, Borealis AI's director of product, listed a bunch of AI and algorithmic failures -- we've seen plenty of that. But it was how Hume described algorithms that really stood out to me. "<a href="https://venturebeat.com/2019/11/14/hypernet-labs-raises-10-million-to-democratize-data-science/">Algorithms are like convex mirrors that refract human biases, but do it in a pretty blunt way</a>," Hume said. "They don't permit polite fictions like those that we often sustain our society with." I really like this analogy. It's probably the best one I've heard so far, because it doesn't end there. Later in her talk, Hume took it further, after discussing an algorithm biased against black people used to predict future criminals in the U.S.<br/><br/>"These systems don't permit polite fictions," Hume said. "They're actually a mirror that can enable us to directly observe what might be wrong in society so that we can fix it. But we need to be careful, because if we don't design these systems well, all that they're going to do is encode what's in the data and potentially amplify the prejudices that exist in society today." If an algorithm is designed poorly or -- as almost anyone in AI will tell you nowadays -- if your data is inherently biased, the result will be too. Chances are you've heard this so often it's been hammered into your brain.</i></p>
      <aside>
        <b>
          <a href="https://venturebeat.com/2019/11/14/hypernet-labs-raises-10-million-to-democratize-data-science">(venturebeat.com)</a>
        </b>
      </aside>
    </article>
  </body>
</html>