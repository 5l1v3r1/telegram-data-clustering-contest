<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://venturebeat.com/2019/11/15/probeat-algorithms-are-like-convex-mirrors-that-refract-human-biases/"/>
    <meta property="og:site_name" content="VentureBeat"/>
    <meta property="article:published_time" content="2019-11-15T18:30:04+00:00"/>
    <meta property="og:title" content="ProBeat: ‘Algorithms are like convex mirrors that refract human biases’"/>
    <meta property="og:description" content="So your algorithm is giving you problematic, biased results. But the algorithm isn't the problem. It is reflecting and refracting the problem."/>
  </head>
  <body>
    <article>
      <h1>ProBeat: ‘Algorithms are like convex mirrors that refract human biases’</h1>
      <address><time datetime="2019-11-15T18:30:04+00:00">15 Nov 2019, 18:30</time> by <a rel="author" href="https://venturebeat.com/author/emil-protalinski/" target="_blank">Emil Protalinski</a></address>
      <p>At the <a href="https://movethedial.com/">Movethedial Global Summit</a> in Toronto yesterday, I listened intently to a talk titled “No polite fictions: What AI reveals about humanity.” Kathryn Hume, Borealis AI’s director of product, listed a bunch of AI and algorithmic failures — we’ve <a href="https://venturebeat.com/2019/01/24/amazon-rekognition-bias-mit/">seen</a> <a href="https://venturebeat.com/2019/04/03/prominent-ai-researchers-call-on-amazon-to-stop-selling-rekognition-facial-analysis-to-law-enforcement/">plenty</a> <a href="https://venturebeat.com/2019/04/24/a-transgender-ai-researchers-nightmare-scenarios-for-facial-recognition-software/">of</a> <a href="https://venturebeat.com/2018/12/06/google-translate-now-returns-both-feminine-and-masculine-translations-for-words-and-phrases/">that</a>. But it was how Hume described algorithms that really stood out to me.</p>
      <p>“Algorithms are like convex mirrors that refract human biases, but do it in a pretty blunt way,” Hume said. “They don’t permit polite fictions like those that we often sustain our society with.”</p>
      <p>I really like this analogy. It’s probably the best one I’ve heard so far, because it doesn’t end there. Later in her talk, Hume took it further, after discussing an algorithm <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">biased against black people</a> used to predict future criminals in the U.S.</p>
      <p>“These systems don’t permit polite fictions,” Hume said. “They’re actually a mirror that can enable us to directly observe what might be wrong in society so that we can fix it. But we need to be careful, because if we don’t design these systems well, all that they’re going to do is encode what’s in the data and potentially amplify the prejudices that exist in society today.”</p>
      <h3>Reflections and refractions</h3>
      <p>If an algorithm is designed poorly or — as almost anyone in AI will tell you nowadays — if your data is inherently biased, the result will be too. Chances are you’ve heard this so often it’s been hammered into your brain.</p>
      <p>The convex mirror analogy is telling you more than just to get better data. The thing about a mirror is you can look at it. You can see a reflection. And a convex mirror is distorted: The reflected image gets larger as the object approaches. The main part that the mirror is reflecting takes up most of the mirror.</p>
      <p>Take this tweet storm that went viral this week:</p>
      <figure>
        <iframe data-src="https://twitter.com/dhh/status/1192540900393705474?ref_src=twsrc%5Etfw" data-embed-type="twitter-tweet" width="100%" height="0" data-service="Twitter" scrolling="no" nowide=""/>
      </figure>
      <p>Yes, the data, algorithm, and app appear flawed. And Apple and Goldman Sachs representatives don’t know why.</p>
      <figure>
        <iframe data-src="https://twitter.com/dhh/status/1192946583832158208?ref_src=twsrc%5Etfw" data-embed-type="twitter-tweet" width="100%" height="0" data-service="Twitter" scrolling="no" nowide=""/>
      </figure>
      <p>Clearly something is going on. Apple and Goldman Sachs are investigating. So is the New York State Department of Financial Services.</p>
      <p>Whatever the bias ends up being, I think we can all agree that a credit limit 20 times larger for one partner over another is ridiculous. Maybe they’ll fix the algorithm. But there are bigger questions we need to ask once the investigations are complete. Would a human have assigned a smaller multiple? Would it have been warranted? Why?</p>
      <p>So you’ve designed an algorithm and there is some sort of problematic bias in your community, in your business, in your data set. You might realize that your algorithm is giving you problematic results. If you zoom out, however, you’ll realize that the algorithm isn’t the problem. It is reflecting and refracting the problem. From there, figure out what you need to fix in not just your data set and your algorithm, but also your business and your community.</p>
      <p>
        <i><a href="https://venturebeat.com/tag/probeat/">ProBeat</a> is a column in which Emil rants about whatever crosses him that week.</i>
      </p>
    </article>
  </body>
</html>