<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://venturebeat.com/2019/11/15/researchers-teach-robots-to-use-inference-to-complete-complex-tasks/"/>
    <meta property="og:site_name" content="VentureBeat"/>
    <meta property="article:published_time" content="2019-11-15T16:02:50+00:00"/>
    <meta property="og:title" content="Researchers teach robots to use inference to complete complex tasks"/>
    <meta property="og:description" content="In a newly published preprint paper, researchers propose a novel technique that teaches robots to infer goals from actions."/>
  </head>
  <body>
    <article>
      <h1>Researchers teach robots to use inference to complete complex tasks</h1>
      <address><time datetime="2019-11-15T16:02:50+00:00">15 Nov 2019, 16:02</time> by <a rel="author" href="https://venturebeat.com/author/kylewiggers/" target="_blank">Kyle Wiggers</a></address>
      <p>There’s much <a href="https://venturebeat.com/category/ai/">robots</a> can achieve by observing human demonstrations, like the actions necessary to move a box of crackers from a counter to storage. But imitation learning is by no means a perfect science — demonstrators often complete subgoals that distract systems from overarching tasks.</p>
      <p>To solve this, researchers at the University of Washington, Stanford University, the University of Illinois Urbana-Champaign, the University of Toronto, and Nvidia propose an “inverse planning” system that taps motions or low-level trajectories to capture the intention of actions. After evaluating their technique by collecting and testing against a corpus of video demonstrations conditioned on a set of kitchen goals, the team reports that their motion reasoning approach improves task success by over 20%.</p>
      <p>The researchers lay out the full extent of the problem in a <a href="https://arxiv.org/pdf/1911.05864.pdf">preprint paper</a> detailing their work. In an environment like, say, a cluttered kitchen, they note that objects are configured in such a way that the goal is obfuscated. Recognizing an action sequence isn’t enough, because a task could have myriad motivations. For example, a demonstrator might move a tablecloth both to remove it from view <i>and</i> reach a knife underneath it.</p>
      <figure>
        <img src="https://venturebeat.com/wp-content/uploads/2019/11/d5be1396-5e79-478d-918e-a56baf21aaa4.png?w=684&amp;resize=684%2C592&amp;strip=all"/>
      </figure>
      <p>The researchers’ AI system, then, outputs the symbolic goal of a task given a real-world video demonstration, which can then be used as input for robotics systems to reproduce said task. To test it, they had it learn a 24-task cooking objective where a human cook poured and prepped ingredients — tomato soup and spam — which were initially blocked by three objects, including a cracker box, a mustard bottle, and a sugar box. They recorded a total of four demonstrations for each task, resulting in a total of 96 demonstrations (excluding videos with substantial missing poses), and then they divided the tasks in two — 12 for system training and 12 for testing.</p>
      <p>The researchers say that their full model explicitly performed motion reasoning about the objects in the demonstration, and thus wouldn’t blindly take all the object movements as intentional. Additionally, they note that it enabled imitation learning across different environments. In one experiment, the system managed to successfully extract the correct goal despite the manipulation of an object (the aforementioned sugar box). Although the sugar box appeared in the kitchen, the robot recognized it didn’t need to move it because it was already out of the way.</p>
      <figure>
        <img src="https://venturebeat.com/wp-content/uploads/2019/11/c2e862a3-6efb-46f5-b0c0-3e4690095579.png?w=800&amp;resize=800%2C142&amp;strip=all"/>
      </figure>
      <p>“Our results show that this allows us to significantly outperform previous approaches that aim to infer the goal based on either just motion planning or task planning,” wrote the coauthors. “In addition, we show that our goal-based formulation enables the robot to reproduce the same goal in a real kitchen by just watching the video demonstration from a mockup kitchen.”</p>
    </article>
  </body>
</html>