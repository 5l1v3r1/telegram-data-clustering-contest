<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://hbr.org/podcast/2019/11/ai-accountability-and-power?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=hbr"/>
    <meta property="og:site_name" content="Harvard Business Review"/>
    <meta property="article:published_time" content="2019-11-20T16:56:38+00:00"/>
    <meta property="og:title" content="AI, Accountability, and Power"/>
    <meta property="og:description" content="Meredith Whittaker, co-director of the AI Now Institute and one of the organizers of the Google Walkout in 2018, joins Azeem Azhar to discuss how discrimination and bias are influencing the development of artificial intelligence, and how tech workers are working to change their industry for the better."/>
  </head>
  <body>
    <article>
      <h1>AI, Accountability, and Power</h1>
      <address>
        <time datetime="2019-11-20T16:56:38+00:00">20 Nov 2019, 16:56</time>
      </address>
      <p>
        <b>
          <i>Listen and subscribe to this podcast via <a href="https://itunes.apple.com/us/podcast/exponential-view-with-azeem-azhar/id1172218725?mt=2">Apple Podcasts</a> | <a href="https://www.google.com/podcasts?feed=aHR0cDovL2ZlZWRzLmhhcnZhcmRidXNpbmVzcy5vcmcvaGFydmFyZGJ1c2luZXNzL2V4cG9uZW50aWFsLXZpZXc%3D">Google Podcasts</a> | <a href="http://feeds.harvardbusiness.org/harvardbusiness/exponential-view">RSS</a></i>
        </b>
      </p>
      <p>In November 2018 mass protests by Google workers drew attention to deep problems with bias and discrimination at some of the largest Silicon Valley companies. <b>Meredith Whittaker</b>, co-director of the AI Now Institute and one of the organizers of the Google Walkout, joins <b>Azeem Azhar</b> to discuss how technology workers can influence progress in culture and technology.</p>
      <p>In this podcast, they also discuss:</p>
      <ul>
        <li>How discriminatory culture influences the values embedded in AI.</li>
        <li>Whether ethics officers can actually influence firms and their products.</li>
        <li>Why responsible AI design must include recognition of the shadow workforce helping to create it.</li>
      </ul>
      <p>
        <a href="https://audio.hbr.org/exponential-view/20191118143543-S4E08_AIAccountabilityandPower.mp3">Download this podcast</a>
      </p>
      <p>Further reading:</p>
      <ul>
        <li>“<a href="https://ainowinstitute.org/discriminatingsystems.pdf">Discriminating Systems: Gender, Race, and Power in AI</a>” (AI Now Institute, April 2019)</li>
        <li>“<a href="https://www.nytimes.com/2019/06/17/business/artificial-intelligence-bias-tech.html">Exposing the Bias Embedded in Tech</a>” (The New York Times, June 17, 2019)</li>
        <li>“<a href="https://techcrunch.com/2019/10/21/people-fix-things-tech-doesnt-fix-things/">’People Fix Things, Tech Doesn’t Fix Things’</a>” (Tech Crunch, October 21, 2019)</li>
      </ul>
      <p>
        <a href="https://twitter.com/mer__edith">@mer__edith</a>
        <br/>
        <a href="https://twitter.com/azeem">@azeem</a>
        <br/>
        <a href="https://twitter.com/exponentialview">@exponentialview</a>
      </p>
      <p>
        <a href="https://www.exponentialview.co/">Exponential View newsletter</a>
      </p>
      <p>
        <i><a href="https://hbr.org/podcasts#presents">HBR Presents</a> is a network of podcasts curated by HBR editors, bringing you the best business ideas from the leading minds in management. The views and opinions expressed are solely those of the authors and do not necessarily reflect the official policy or position of Harvard Business Review or its affiliates.</i>
      </p>
    </article>
  </body>
</html>