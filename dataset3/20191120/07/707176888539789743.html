<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.ft.com/content/f6a0c7c0-0b34-11ea-bb52-34c8d9dc6d84"/>
    <meta property="og:site_name" content="Financial Times"/>
    <meta property="article:published_time" content="2019-11-20T07:46:34+00:00"/>
    <meta property="og:title" content="Self-driving car groups face stricter safety oversight"/>
    <meta property="og:description" content="Fatal Uber accident with pedestrian puts spotlight on ‘self-certifying’ approach"/>
  </head>
  <body>
    <article>
      <h1>Self-driving car groups face stricter safety oversight</h1>
      <h2>Fatal Uber accident with pedestrian puts spotlight on ‘self-certifying’ approach</h2>
      <address><time datetime="2019-11-20T07:46:34+00:00">20 Nov 2019, 07:46</time> by <a rel="author">Patrick McGee</a></address>
      <p>Until now, America has been the Wild, Wild West for autonomous driving. California alone has 62 companies testing driverless vehicles — and some of them are seeking to deploy commercial fleets in the next two years.</p>
      <p>The idea that the National Highway Traffic Safety Administration would step in and make this transition difficult has largely been dismissed. Instead, the NHTSA has been supportive of the efforts, playing a reactive role that lets companies “self-certify” that they meet safety standards.</p>
      <p>But the NHTSA’s approach came in for a scathing attack on Tuesday when the US National Transportation Safety Board, an investigative agency, held a public hearing to determine the cause of the <a href="https://www.ft.com/content/6d0c5544-0afb-11ea-bb52-34c8d9dc6d84">fatal accident</a> in March 2018 when a self-driving Uber killed a pedestrian as she crossed the street with a bicycle.</p>
      <p>“In my opinion they’ve put technology advancement here before saving lives,” Jennifer Homendy, a board member of the NTSB, said of the NHTSA.</p>
      <p>Ms Homendy quoted extensively from the NHTSA’s public guidelines, called “Automated Driving Systems, a Vision for Safety”, saying it was “actually laughable” at times.</p>
      <p>“They should rename it a Vision for Lax Safety,” she said.</p>
      <blockquote>There is a major failing on the Federal government’s part and the state of Arizona because they also didn’t have any standards in place — and still don’t for failing to regulate these operations<cite>Jennifer Homendy, National Transportation Safety Board member</cite></blockquote>
      <p>One person involved in the hearings said afterwards: “I knew there was interdepartmental squabble, but this took it to a new level.”</p>
      <p>The Tuesday hearing was expected to be most consequential for Uber, whose self-driving system had <a href="https://www.ft.com/content/2c3d8efc-5fb2-11e8-9334-2218e7146b04">failed</a> to spot the pedestrian.</p>
      <p>But the NTSB determined that the “immediate cause of the collision was the failure of the <a href="https://www.ft.com/content/2c3d8efc-5fb2-11e8-9334-2218e7146b04">Uber</a> [safety driver] to closely monitor the road” and the brunt of its critique was directed at regulators for weak oversight. Four of the six actions recommended by the NTSB were for the NHTSA and the state of Arizona, where the accident took place.</p>
      <p>Experts at the hearing noted that the NHTSA “encourages” self-driving companies to submit self-assessments of their technology — but only 16 have, and their quality has varied greatly. One expert said “few of them provide” much detail and “others frankly read like marketing brochures”.</p>
      <p>Ms Homendy said: “I actually think that there is a major failing on the Federal government’s part and the state of Arizona because they also didn’t have any standards in place — and still don’t for failing to regulate these operations.”</p>
      <p>Board members of the NTSB unanimously backed the idea that the NHTSA begin to “require” groups that are testing — or that even intend to test — to submit standardised, detailed reports. In addition, the NHTSA was asked to establish a process for ongoing evaluation.</p>
      <p>If implemented, the recommended measures could further delay the deployment of self-driving cars, as some groups have not really encountered strong resistance from regulators.</p>
      <p>Amnon Shashua, chief executive of Mobileye, an Intel-owned computer vision company, said before the hearing that he personally believed regulation was “the biggest hurdle” to getting autonomous robotaxis on the road, but he said few others really understood the challenge.</p>
      <p>“Nobody really talks about regulation,” he said. “Everybody talks about, ‘OK we will drive such and such number of miles’ and show . . . the fact that we don’t have many accidents in those millions of miles, and then everything will be good and we’ll put an autonomous car on the road. But the issue is much more complicated.”</p>
      <p>The consequences of getting regulations wrong could be far-reaching. Transport &amp; Environment, a Brussels-based green lobby group, has warned of “a rush hour that lasts all day” if <a href="https://www.ft.com/stream/a3867483-5d05-4683-b572-b3e3415c8029">driverless cars</a> are allowed on the streets in an unregulated way.</p>
      <p>“Automated vehicles with no driver could become so cheap to run that they would encourage people, or even cars without people in them, to travel more and for longer,” the group warned in September.</p>
      <p>Even without input from regulators, there is a big and unresolved debate as to how much safer driverless cars should be, versus human drivers, before they are deployed.</p>
      <p>Mobileye, which is the global leader in driver-assist technology, believes that to build societal trust in self-driving technology “the fatality rate should be reduced by three orders of magnitude” — meaning they should be 1,000 times safer.</p>
      <p>Others, however, say that setting the bar so high would mean causing unnecessary deaths at the hands of human drivers.</p>
      <p>Dan Ammann, head of GM driverless division Cruise, told the Financial Times: “To the extent we can demonstrate that we’re driving more safely than the average human then, by definition, every time we deploy one of our cars on the road we’re making the world slightly safer. That’s the objective.”</p>
      <h3>Recommended</h3>
      <p><a href="https://www.ft.com/brooke-masters">Brooke Masters <br/></a><a href="https://www.ft.com/content/39c01b56-9be5-11e9-9c06-a4640c9feebb"> It is difficult for self-driving car companies to go it alone <br/></a>Tuesday, 2 July, 2019</p>
      <p>Chris Urmson, chief executive of Aurora, a self-driving start-up backed by Amazon and Sequoia, told the FT’s Future of the Car Summit in Detroit that “it just doesn’t make any sense” to require that self-driving cars be 100 times safer than a human before deploying them.</p>
      <p>“The normal bar that is used in this kind of application — where you’re bringing the technology in the vehicle — is you’re not creating new, unreasonable risk,” he said. “And I think that is the threshold we should be thinking about in the deployment of this technology.”</p>
      <p>After the hearing, Nat Beuse, a former NHTSA official who is now head of Safety at Uber’s autonomous driving unit, said in the past six years “there was no solution per se to go out and regulate”. Instead, regulators were trying to strike a balance between being heavy-handed and letting companies take different approaches.</p>
      <p>Whether or not the recommendations become law, said Noah Zych, his colleague and chief of staff at Uber’s driverless division, “any responsible self-driving developer is going to be looking at [them] very carefully”.</p>
      <p>He added: “The unfortunate reality is there will be another crash with a severe injury or fatality during the development of this technology. [When that occurs,] there will be no refuge for any company that hasn’t established a responsible practice for safety management.”</p>
    </article>
  </body>
</html>