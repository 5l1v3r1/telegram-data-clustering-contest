<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.itweb.co.za/content/o1Jr5qxEal5vKdWL"/>
    <meta property="og:site_name" content="ITWeb"/>
    <meta property="article:published_time" content="2019-11-18T07:19:53+00:00"/>
    <meta property="og:title" content="The datacentre of the future"/>
    <meta property="og:description" content="In a digital world, IT needs to operate with a completely different and increasingly demanding array of user needs and expectations. Organisations are re-evaluating or scaling down their datacentre investments, and some are even asking themselves if they need them at all."/>
  </head>
  <body>
    <article>
      <h1>The datacentre of the future</h1>
      <address><time datetime="2019-11-18T07:19:53+00:00">18 Nov 2019, 07:19</time> by <a rel="author">Kirsten Doyle</a></address>
      <p>CIOs in every industry are designing their IT strategies with a focus on their application portfolio, instead of their physical infrastructure. This is seeing them move away from the IT architecture-driven strategies of old, towards more service-driven ones. At the same time, many businesses are grappling with the issue of cloud adoption, having to deal with IoT projects, edge environments, an increased emphasis on customer experience, and the digital transformation needed to achieve all of the above.</p>
      <p>So says Stephane Duproz, CEO, Africa Data Centres, adding that this digital transformation has completely altered the landscape for industries and technologies, and nowhere is this truer than in the datacentre. These days, traditional datacentres are being relegated to legacy holding areas, dedicated to a few, very specific services that can’t be supported anywhere else, and carrying a handful of systems that are more economically efficient in-house. In fact, many consider on-premise datacentres to be a thing of the past. This is creating a gap where colocation providers are positioning themselves as the place where private, on-premise infrastructure meets public cloud.</p>
      <p>Enterprises across the board are scrutinising their infrastructure investments, getting rid of as much on-premises datacentre space as possible, and replacing it with both cloud services and colocation facilities. “Those businesses in industries such as financial services, healthcare and retail, which built their own datacentres 10 or more years ago, are now understanding the true costs of maintaining and keeping those facilities up to the standards needed today. This can cost hundreds of millions of rands.”</p>
      <p>In addition, Duproz says these businesses realise they need significantly less datacentre space for the same infrastructure, which makes a refresh way too expensive. Many are looking to hosting as an alternative because they can get both the latest infrastructure and access to all the top cloud providers, and they can do this at a fraction of the cost of keeping it all on-premise.</p>
      <p>Jan Hnizdo, MD, Teraco, says more and more businesses are looking to colocation as their current facilities face end-of-life and the need to be in the cloud becomes more pressing. This bodes well for the datacentre market, which has in the past few months seen much growth and potential as the world’s top cloud providers land in South Africa, which remains a leading provider across the continent followed by Nigeria and Kenya.</p>
      <p>“Contrary to predictions that the large datacentre would disappear, we’re actually seeing the emergence of enormous hyperscale datacentres,” says Stefan Jacobs, head of solution design, Wipro Africa. “This is due to cloud technology and the massive data storage and processing requirements that come along with it. At the same time, we’re seeing increasing numbers of small datacentres being established throughout the world, which are driven by the IoT, which requires compute power close to the edge. While this seems to be a dichotomy, they’re intrinsically linked by a data-driven world. The emergence of new technologies like 5G, artificial intelligence, machine learning, robotics, etc. are empowering this.</p>
      <p>In addition, both hyperconverged infrastructure (HCI) and software-defined datacentres (SDDCs) have emerged from the need to store growing amounts of data in smaller physical spaces with more intelligent management capabilities. “These technologies underpin the datacentre of the future as massive compute and store capabilities are heavily reliant on the virtualisation of the datacentre and the ability to obtain the most power and capacity from the smallest physical footprint,” adds Jacobs.</p>
      <h3>The power of software</h3>
      <p>According to Brian Tarr, chief products and solutions officer at Nexio, both HCI and the full SDDC approach use software to control, manage and provision, but there are differences. Both have the same end goal, to add flexibility, scalability, and efficiency, but the methods to achieve those objectives differ. “It comes down to how you want to manage your storage. SDDCs requires deep storage expertise, while HCI doesn’t. While there are some differences in capital costs, there’s much more in operational costs. More so, each solves different problems and fits best for different use cases.</p>
      <p>“HCI is almost always the best choice for virtualised workloads. It’s designed specifically to work with virtualisation, and it presents resources natively to the hypervisor. Whether you’re refreshing infrastructure for an existing virtual environment or starting a new project, it makes more sense to invest in HCI. This also holds true for containerised workloads, most of which run within a virtual machine. Because HCI already abstracts the storage layer completely, it’s a natural fit for any container workload running on a virtual infrastructure,” says Tarr.</p>
      <p>“Companies that opt to deploy SDDC on HCI will gain the benefits of consolidation and modernisation of datacentre technology, easy creation of a multi-tenant platform, pay-as-you-grow scalability, and centralised management of distributed datacentres,” he adds.</p>
      <p>Most organisations try to predict the future, and then based on those predictions, build a rigid IT ecosystem, says Avsharn Bachoo, CTO, PPS. “The secret sauce lies in architecture. You have to use architecture to drive your technical strategy. This results in manageable complexity as opposed to accidental complexity, which creates a mess. The real datacentre architectural question that needs to be answered is whether your hardware and software should be coupled or decoupled. If this isn’t given enough thought, the business could end up in a mess. A decoupled architecture is implemented to achieve higher computing performance by isolating and executing individual components independently and in parallel, even in the cloud.</p>
      <p>“However, HCI couples hardware and software within a single appliance. That means it combines servers, storage and networking all in one box. So although this simplifies infrastructure complexity, the flip side is a loss of flexibility and very high costs for scaling. These days, with just about every organisation having a cloud-first strategy, the future of HCI does look bleak. That being said, almost every organisation moving to the cloud still has a requirement to keep a backup of core data on premise for backups, business continuity, disaster recovery or regulatory reasons. These organisations don’t necessarily want to build entire datacentres just for backups and business continuity, and this is where the value of HCI comes in. However, proceed with caution, because a cloud-native application designed and built for the cloud wouldn’t run on HCI. There could be benefits for HCI within an edge computing architecture, as this can be set up as a micro-datacentre closer to the customer, branch or transaction to improve performance.”</p>
      <h3>Centralised vs distributed</h3>
      <p>Bachoo says that datacentres traditionally have a centralised architectural design, but the introduction of edge computing changes this to a distributed architecture. “Edge computing will allow data processing to happen closer to where data is being generated, enabling data analysis in near real-time. Edge computing will also help reduce connectivity costs by removing the need for super-fast connectivity between devices and traditional datacentres. Edge datacentres represent a host of new variables; they could be anything from a fleet of racks tucked under an office stairwell to a single box embedded in the bottom of a cell tower.”</p>
      <blockquote>Traditional centres are being relegated legacy holding areas, dedicated to a few, very specific services than cannot be supported anywhere else.<cite>Stephane Duproz, Africa Data Centres</cite></blockquote>
      <p>Says Hnizdo: “In much the same way as the cloud enables a whole new range of applications and use cases, edge will deliver more real-time, highly interactive use cases. Gartner predicts that, by 2022, more than 50% of enterprise-generated data will be created and processed outside the datacentre or cloud. Manufacturing has been implementing edge environments and many edge success stories will come from industrial or operational technology use cases. Building skills and expertise in edge use cases (both IoT and people-oriented) will be critical. Early edge deployments will be highly customised, requiring consulting expertise, but those are also opportunities to have skills and expertise transfer for the next edge project.”</p>
      <p>The huge datacentre model won’t become obsolete by any means and will still be used for a wide range of functions, but the rise of edge computing will see datacentres slowly become more distributed, says Tarr. Growing numbers of datacentres, though, means that storage and security imperatives are more widely distributed, too. Businesses will demand that the same level of performance and security they enjoy from centralised datacentres is replicated in the more distributed future. For this to happen, security and storage solutions must be considered at the start of planning processes and not tacked on at the end once new models are ready to roll out.</p>
      <h3>Rise of automation</h3>
      <p>Automation is also changing the face of the datacentre as we know it. Hnizdo says, for a while, experts have been predicting that automation technologies applied in factories worldwide would soon be applied to datacentres. “In fact, 90% of outages are due to people, which automation will easily address. It will inevitably become commonplace while also bringing value to datacentre customers. A significant benefit of automation is the self-healing datacentre, where traditional processes will be taken over by machines, meaning that humans are no longer needed to perform patches to servers at three in the morning. Issues can be identified and flagged by machines before they occur, eliminating downtime.”</p>
      <p>“The biggest fear regarding automation is that jobs will be lost,” says Bachoo. “This stems from watching robots take over assembly lines. However, when it comes to datacentre automation, we can think of it assisting and not replacing IT professionals. Automation is an efficiency obtained by adding some degree of optimisation to mundane and routine processes.”</p>
      <blockquote>Most organisations try to predict the future, and then based on those predictions, build a ridid IT ecosystem.<cite>Avsharn Bachoo, PPS</cite></blockquote>
      <p>Says Hnizdo: “Datacentre power management is also becoming a key differentiator in this market. Voltage fluctuations are becoming more frequent and can have disastrous effects on legacy datacentre builds, specifically on the refrigerant cooling systems in terms of predicted life spans. With an influx of international organisations deploying in SA, greener designs to reduce mechanical energy are a prerequisite in today’s datacentre designs. With the proposed NERSA pricing increases over the next three years, these green designs will escalate the ROI of the capital equipment and provide the datacentre operators with better efficiencies, reducing the burden on clients with increased electricity prices. Green options include elements like energy-efficient lighting systems, ultrasonic humidifiers, and direct or indirect free cooling systems. Many datacentres are providing warmer environmental conditions to the server inlet, providing substantial energy efficiencies.”</p>
      <p>Jacobs says there are many interesting initiatives underway in an attempt to have a greener datacentre. “The huge footprint of datacentres makes them ideal candidates for solar power, and many providers are exploring this option. Wind power is also being explored in areas where this is feasible. Providers are looking at innovative locations for establishing datacentres, such as under the sea to reduce cooling requirements. They’re also looking at leveraging ambient temperatures for cooling by constructing datacentres in cold regions such as Scandinavia so that air-conditioning isn’t required. To make the most of the energy produced, they are also exploring the option to exhaust the hot air out of the datacentre and use it to heat nearby buildings. We’re also seeing the re-emergence of liquid cooling technologies, as well as a move away from spinning disk towards flash storage, which generates less heat and therefore has reduced cooling and power requirements.”</p>
      <h3>Hardening the borders</h3>
      <p>Another challenge for datacentre operators is security. Although cloud migration pressure is mounting, and IT departments are being forced to consider what computing infrastructure they want in the cloud, security and compliance concerns are inhibiting migration. “Data security is crucial because it addresses network security as well as physical security within a datacentre. As digital transformation continues to drive businesses forward, dominating strategy and moving everything closer to the edge, security and sovereignty concerns become key, and datacentres need to address the risk to enable successful migration. The threat surface is constantly increasing, and as more compute, storage and networking capacity is added on-premise in the cloud, the threats multiply, says Hnizdo. The key is to build digital architecture from a platform that offers resilient datacentre services. This approach needs to offer access to both local and global cloud offerings, physical or virtual interconnections, peering and 24x7 support services. It’s about finding an open marketplace of connectivity and services from which to securely innovate and grow a business.</p>
      <p>Jacobs says on a physical level, some providers are looking at building datacentres in mountains and underground bunkers to make physical access more secure and to prevent natural disasters from destroying the data. “Access controls like biometrics are becoming more commonly used and drone security is being investigated. In terms of data security, the biggest focus is on improving encryption beyond 256-bit, and quantum computing is being explored to encrypt data for maximum security.</p>
      <p>“With edge computing that gets closer to users, the structure, and the capabilities, of databases will evolve as well. The security of this data is a major concern, and blockchain is one potential methodology to maintain the integrity of stored data,” Bachoo speculates.</p>
      <p>“Rather than a centralised design with all data maintained within one datacentre, blockchain distributes its database across multiple datacentres,” he says.</p>
      <p>Jacobs says 5G is the technology that will drive edge computing and enable the true power of the IoT to be realised. “It will also cause massive growth in data, which will require the capacity and power of the datacentre to be increased. To support this, there will be massive growth of distributed datacentres to bring computing closer to the edge.”</p>
      <p>However, 5G will also place additional pressure on datacentres as users demand more content to be more quickly and easily available. “Latency will remain an issue and 5G networks could also enhance data efficiency and datacentres will need to manage resource-intensive data without compromising on the energy consumption and cost factor, says Hnizdo</p>
      <p>“We’re also seeing businesses looking to their outward-facing applications to drive better customer experience, which is causing them to rethink the placement of their applications based on network latency, geopolitical limitations, and customer population clusters. Let's not forget they also have considerations such as data sovereignty and residency to consider, particularly in light of the increasingly stringent regulatory environment they are operating in, with the introduction of POPIA and GDPR, for example,” adds Duproz.</p>
      <p>For datacentre providers, being able to manage a range of infrastructure has become non-negotiable. Hybrid cloud is on the rise, and hyperscale is taking off at an unprecedented rate. We have to find new ways to help customers get their application infrastructure physically closer to users, not only to boost performance, but to keep bandwidth costs down. “Ultimately, in the future, computing workloads will be located according to business requirements, as opposed to physical location, and businesses that want to stay competitive will need to develop relationships with service partners who can help them be more flexible and agile,” ends Duproz.</p>
    </article>
  </body>
</html>