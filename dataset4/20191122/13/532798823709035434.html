<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.scientificamerican.com/podcast/episode/bots-outperform-humans-if-they-impersonate-us/"/>
    <meta property="og:site_name" content="Scientific American"/>
    <meta property="article:published_time" content="2019-11-22T13:31:00+00:00"/>
    <meta property="og:title" content="Bots Outperform Humans If They Impersonate Us"/>
    <meta property="og:description" content="Bots masquerading as humans in a game outperformed their human opponents&amp;mdash;but the their superiority vanished when their machine identity was revealed. Christopher Intagliata reports."/>
  </head>
  <body>
    <article>
      <h1>Bots Outperform Humans If They Impersonate Us</h1>
      <address><time datetime="2019-11-22T13:31:00+00:00">22 Nov 2019, 13:31</time> by <a rel="author">Christopher Intagliata</a></address>
      <figure>
        <audio src="https://flex.acast.com/www.scientificamerican.com/podcast/podcast.mp3?fileId=0FDDACB8-7561-4394-A956E41A305C5213"/>
      </figure>
      <p>Bots masquerading as humans in a game outperformed their human opponents—but the their superiority vanished when their machine identity was revealed. Christopher Intagliata reports. </p>
      <h3>Full Transcript</h3>
      <p>Last year, <a href="https://www.scientificamerican.com/article/so-umm-google-duplexs-chatter-is-not-quite-human/">Google unveiled Duplex</a>, its <a href="https://www.scientificamerican.com/article/googles-duplex-ai-scares-some-people-but-i-cant-wait-for-it-to-become-a-thing/">artificial-intelligence-powered assistant</a>.</p>
      <p>&lt;&lt;"'How can I help you?' 'Hi, I'm calling to book a women's haircut for a client. Um, I'm looking for something on May 3rd.'"&gt;&gt; That's a robot. &lt;&lt;"'Sure, give me one second.' 'Mmm-hmm.' 'For what time are you looking for around?'"&gt;&gt; </p>
      <p>The machine assistant never identified itself as a bot in the demo. And Google got a lot of flak for that. They later clarified that they would only launch the tech with "<a href="https://www.cnet.com/news/google-duplex-assistant-bot-deception-scary-ethics-question/">disclosure built in</a>." </p>
      <p>But therein lies a dilemma. Because a new study in the journal <i>Nature Machine Intelligence</i> suggests that a bot is most effective when it <i>hides</i> its machine identity. </p>
      <p>"That is, if it is allowed to pose as human."</p>
      <p>Talal Rahwan is a computational social scientist at New York University’s campus in Abu Dhabi. His team recruited nearly 700 online volunteers to play the prisoner's dilemma—a classic game of negotiation, trust and deception—against either humans or bots. Half the time, the human players were told the truth about who they were matched up against. The other half, they were told they were playing a bot, when they were actually playing a human; or, that they were battling a human, when in fact, it was only a bot. </p>
      <p>And the scientists found that bots actually did remarkably well in this game of negotiation—<i>if</i> they impersonated humans. </p>
      <p>"When the machine is reported to be human, it outperforms humans themselves. It's more persuasive, it's able to induce cooperation and persuade the other opponent to cooperate, more than humans themselves." </p>
      <p>But whenever the bots' true nature was disclosed, their superiority vanished. And Rahwan says that points to a fundamental conundrum. We can now build really efficient bots—that perform tasks even better than we can—but their efficiency may be linked to their ability to hide their identity. Which, you know, feels ethically problematic. </p>
      <p>"Those very humans who will be deceived by the machine, they are the ones who ultimately have to make that choice. Otherwise it would violate fundamental values of autonomy, respect and dignity for humans."</p>
      <p>It's not realistic to ask people for consent before every bot-human interaction. That would, of course, reveal the bots' true identity. So we as a society will have to figure out if making our lives a bit easier is worth interacting with bots that pretend to be human.</p>
      <p>—Christopher Intagliata </p>
      <p>[Fatimah Ishowo-Oloko et al, <a href="https://www.nature.com/articles/s42256-019-0113-5">Behavioural evidence for a transparency–efficiency tradeoff in human–machine cooperation</a>]</p>
      <p>
        <i>[The above text is a transcript of this podcast.]</i>
      </p>
    </article>
  </body>
</html>