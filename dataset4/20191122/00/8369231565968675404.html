<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.sciencealert.com/turns-out-that-insults-hurt-even-when-they-come-from-a-robot"/>
    <meta property="og:site_name" content="ScienceAlert"/>
    <meta property="article:published_time" content="2019-11-22T00:00:00+00:00"/>
    <meta property="og:title" content="Even a Robot Can Hurt Your Feelings if It Hurls The Right Insult, New Study Shows"/>
    <meta property="og:description" content="Having insults hurled our way is never going to do wonders for our self-esteem, but it turns out that disparaging remarks can cut us even when they're delivered by a robot."/>
  </head>
  <body>
    <article>
      <h1>Even a Robot Can Hurt Your Feelings if It Hurls The Right Insult, New Study Shows</h1>
      <address><time datetime="2019-11-22T00:00:00+00:00">22 Nov 2019</time> by <a rel="author">DAVID NIELD</a></address>
      <p>Having insults hurled our way is never going to do wonders for our self-esteem, but it turns out that disparaging remarks can cut us even when they're delivered by a robot.</p>
      <p>You might think that critical comments from robots – droids only saying what they've been programmed to say, with no consciousness or feelings of their own – are the sort of barbs we can easily brush off.</p>
      <p>Not so, based on a test involving 40 human participants who had to put up with insults being flung from humanoid robot <a href="https://en.wikipedia.org/wiki/Pepper_(robot)">Pepper</a> during a series of games. Human performance in the game was worse with discouragements from Pepper, and better when the machine was more  encouraging.</p>
      <p>The work could be useful in teaching us how we might use robots as companions or as learning tools in the future. When the robot uprising comes, it might well start with a few snarky comments.</p>
      <p>"This is one of the first studies of human-robot interaction in an environment where they are not cooperating," <a href="https://www.cmu.edu/news/stories/archives/2019/november/robot-trash-talk.html">says computer scientist Fei Fang</a>, from Carnegie Mellon University (CMU).</p>
      <p>"We can expect home assistants to be cooperative, but in situations such as online shopping, they may not have the same goals as we do."</p>
      <p>The study involved 40 participants, who played a game called <i>Guards and Treasures</i> 35 times over with Pepper. The game is an example of a <a href="https://en.wikipedia.org/wiki/Stackelberg_competition">Stackelberg game</a>, with a defender and an attacker, and intended to teach rationality.</p>
      <p>While the participants all improved in terms of their rationality over the course of the test, those who were getting insulted by their robot opponent didn't hit the same level of scores as those who were getting praised. Players met with a critical robot had a more negative view of it too, as you might expect.</p>
      <p>Verbal abuse dished out by Pepper included comments like "I have to say you are a terrible player" and "over the course of the game your playing has become confused".</p>
      <p>The findings match up with <a href="https://today.uconn.edu/2019/07/trash-talk-really-can-put-players-off-game-uconn-researcher-finds/">previous research</a> showing that 'trash talk' really can have a negative effect on gameplay – but this time the talk is coming from an automated machine.</p>
      <p>Although this was only a small-scale study, as our interactions with robots get more frequent – whether that's via a smart speaker in a home or as a bot designed to improve mental health in a hospital – we need to understand how humans react to these seemingly personable machines.</p>
      <p>In situations where robots might think they know better than us, such as getting directions from A to B or buying something in a store, programmers need to know how best to handle those arguments when coding a droid.</p>
      <p>Next, the team behind the study wants to look at non-verbal cues given out by robots.</p>
      <p>The researchers report that some of the study participants were "technically sophisticated" and fully understood that it was the robot that was putting them off – but were still affected by the programmed responses.</p>
      <p>"One participant said, 'I don't like what the robot is saying, but that's the way it was programmed so I can't blame it,'" <a href="https://www.cmu.edu/news/stories/archives/2019/november/robot-trash-talk.html">says computer scientist Aaron Roth</a>, from CMU.</p>
      <p>The research has yet to be published in a peer-reviewed journal, but has been presented at the <a href="https://ro-man2019.org/agenda/">IEEE International Conference on Robot &amp; Human Interactive Communication</a> in India.</p>
    </article>
  </body>
</html>