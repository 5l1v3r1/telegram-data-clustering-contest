<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.technologynetworks.com/informatics/news/bots-behaving-badly-can-we-trust-in-an-algorithm-controlled-society-327530"/>
    <meta property="og:site_name" content="Technology Networks"/>
    <meta property="article:published_time" content="2019-11-22T00:00:00+00:00"/>
    <meta property="og:title" content="Bots Behaving Badly: Can We Trust in an Algorithm-controlled Society?"/>
    <meta property="og:description" content="Stanford and UMass Amherst develop a new way to help machine-learning designers build AI with safeguards against specific, undesirable outcomes such as racial and gender bias&lt;br /&gt;&lt;br /&gt;"/>
  </head>
  <body>
    <article>
      <h1>Bots Behaving Badly: Can We Trust in an Algorithm-controlled Society?</h1>
      <address>
        <time datetime="2019-11-22T00:00:00+00:00">22 Nov 2019</time>
      </address>
      <figure>
        <img src="https://cdn.technologynetworks.com/tn/images/thumbs/jpeg/640_360/bots-behaving-badly-can-we-trust-in-an-algorithm-controlled-society-327530.jpg?v=10462201"/>
      </figure>
      <p>
        <i>Credit: Photo by Lukas on Unsplash https://unsplash.com/@hauntedeyes</i>
      </p>
      <p>Read Time:</p>
      <p>Artificial intelligence has moved into the commercial mainstream thanks to the growing prowess of machine learning algorithms that enable computers to train themselves to do things like drive cars, control robots or automate decision-making.<br/><br/>But as AI starts handling sensitive tasks, such as helping pick which prisoners get bail, policy makers are insisting that computer scientists offer assurances that automated systems have been designed to minimize, if not completely avoid, unwanted outcomes such as excessive risk or racial and gender bias.<br/><br/>A team led by researchers at Stanford and the University of Massachusetts Amherst published a paper Nov. 22 in Science suggesting how to provide such assurances. The paper outlines a new technique that translates a fuzzy goal, such as avoiding gender bias, into the precise mathematical criteria that would allow a machine-learning algorithm to train an AI application to avoid that behavior.<br/><br/>"We want to advance AI that respects the values of its human users and justifies the trust we place in autonomous systems," said Emma Brunskill, an assistant professor of computer science at Stanford and senior author of the paper.</p>
      <h3>Avoiding misbehavior</h3>
      <p>The work is premised on the notion that if "unsafe" or "unfair" outcomes or behaviors can be defined mathematically, then it should be possible to create algorithms that can learn from data on how to avoid these unwanted results with high confidence. The researchers also wanted to develop a set of techniques that would make it easy for users to specify what sorts of unwanted behavior they want to constrain and enable machine learning designers to predict with confidence that a system trained using past data can be relied upon when it is applied in real-world circumstances.<br/><br/>"We show how the designers of machine learning algorithms can make it easier for people who want to build AI into their products and services to describe unwanted outcomes or behaviors that the AI system will avoid with high-probability," said Philip Thomas, an assistant professor of computer science at the University of Massachusetts Amherst and first author of the paper.</p>
      <h3>Fairness and safety</h3>
      <p>The researchers tested their approach by trying to improve the fairness of algorithms that predict GPAs of college students based on exam results, a common practice that can result in gender bias. Using an experimental dataset, they gave their algorithm mathematical instructions to avoid developing a predictive method that systematically overestimated or underestimated GPAs for one gender. With these instructions, the algorithm identified a better way to predict student GPAs with much less systematic gender bias than existing methods. Prior methods struggled in this regard either because they had no fairness filter built-in or because algorithms developed to achieve fairness were too limited in scope.<br/><br/>The group developed another algorithm and used it to balance safety and performance in an automated insulin pump. Such pumps must decide how big or small a dose of insulin to give a patient at mealtimes. Ideally, the pump delivers just enough insulin to keep blood sugar levels steady. Too little insulin allows blood sugar levels to rise, leading to short term discomforts such as nausea, and elevated risk of long-term complications including cardiovascular disease. Too much and blood sugar crashes - a potentially deadly outcome.<br/><br/>Machine learning can help by identifying subtle patterns in an individual's blood sugar responses to doses, but existing methods don't make it easy for doctors to specify outcomes that automated dosing algorithms should avoid, like low blood sugar crashes. Using a blood glucose simulator, Brunskill and Thomas showed how pumps could be trained to identify dosing tailored for that person - avoiding complications from over- or under-dosing. Though the group isn't ready to test this algorithm on real people, it points to an AI approach that might eventually improve quality of life for diabetics.<br/><br/>In their Science paper, Brunskill and Thomas use the term "Seldonian algorithm" to define their approach, a reference to Hari Seldon, a character invented by science fiction author Isaac Asimov, who once proclaimed three laws of robotics beginning with the injunction that "A robot may not injure a human being or, through inaction, allow a human being to come to harm."<br/><br/>While acknowledging that the field is still far from guaranteeing the three laws, Thomas said this Seldonian framework will make it easier for machine learning designers to build behavior-avoidance instructions into all sorts of algorithms, in a way that can enable them to assess the probability that trained systems will function properly in the real world.<br/><br/>Brunskill said this proposed framework builds on the efforts that many computer scientists are making to strike a balance between creating powerful algorithms and developing methods to ensure that their trustworthiness.<br/><br/>"Thinking about how we can create algorithms that best respect values like safety and fairness is essential as society increasingly relies on AI," Brunskill said.<br/><br/><b>Reference</b>: Thomas, P. S., Silva, B. C. da, Barto, A. G., Giguere, S., Brun, Y., &amp; Brunskill, E. (2019). Preventing undesirable behavior of intelligent machines. Science, 366(6468), 999â€“1004. https://doi.org/10.1126/science.aag3311<br/><br/>This article has been republished from the following <a href="https://news.stanford.edu/2019/11/21/stanford-helps-train-ai-not-misbehave/">materials</a>. Note: material may have been edited for length and content. For further information, please contact the cited source.</p>
    </article>
  </body>
</html>