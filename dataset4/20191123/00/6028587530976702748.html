<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.forbes.com/sites/tomcoughlin/2019/11/23/content-delivery-cache-and-neural-network-memory/?utm_source=dlvr.it&amp;utm_medium=twitter"/>
    <meta property="og:site_name" content="Forbes"/>
    <meta property="article:published_time" content="2019-11-23T00:00:00+00:00"/>
    <meta property="og:title" content="Content Delivery Cache And Neural Network Memory"/>
    <meta property="og:description" content="Local memory and storage can play an important role. Local cached content allows content delivery without burdening Internet networks. On-chip memory also speeds up AI applications and reduces power consumption. Using non-volatile fast memory will provide even lower power consumption."/>
  </head>
  <body>
    <article>
      <h1>Content Delivery Cache And Neural Network Memory</h1>
      <address><time datetime="2019-11-23T00:00:00+00:00">23 Nov 2019</time> by <a rel="author">Tom Coughlin</a></address>
      <p>We have written in the past about the uses of memory and storage in data movement and in AI applications.  This piece will talk about digital distribution technology and the role of content caching from Mo-DV as well as some neural network-based AI systems from Supermicro and an upcoming edge solution neural network chip from startup, Brainchip.  Although many people enjoy streaming content, there are many folks who download and view content off-line, e.g. when they are travelling.  This is what Mo-DV’s Mo2Go Speedspots and SLAN is about.</p>
      <p>Mo2Go SpeedSpots are meant to provide fast mobile video delivery without the need for an Internet connection. Mo2Go SpeedSpots provide an alternative to traditional internet</p>
      <p>video streaming and downloading to mobile devices, combining secure Digital Rights Management with fast Wi-Fi delivery of high-definition video content from content stored in the SpeedSpot, even in places where there is inadequate or no Internet access.  The storage capacity in these SpeedSpots is typically 1-2 TBs.</p>
      <p>The Mo2Go SpeedSpots are low-cost, physical content servers placed in high-traffic areas, such as airports, shopping centers, and train stations. These SpeedSpots connect automatically with any mobile device within range that’s running the Mo2Go App, and in seconds transfers secure video content for streaming and downloading to that device. </p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd99020e0af7b0006b223b5/960x0.jpg?fit=scale"/>
        <figcaption>Mo2Go Content Delivery Solution<cite>From Presentation Deck from Mo-DV</cite></figcaption>
      </figure>
      <p>SLAN is an entirely new technology from MO-DV, in early development, that connects existing, local, business and home Wi-Fi access points into a mesh network with local content storage cache and proxy responses for content. It acts like a local content delivery network, using resources, such as storage, in a local mesh network.   It is believed that these neighborhood networks can reduce bandwidth demand on the ISPs, reduce latency, and allow pre-loading of content. The result is a better viewing experience for users and lower costs for ISPs and content providers.</p>
      <p>The company says that SLAN is inexpensive to deploy, as it only requires replacing or modifying existing Wi-Fi access points (usually paid for by the users). Applications for SLAN are many, including bandwidth reduction for ISPs; lower latency for video conferencing, video telephony, and gaming; pre-loading content during off-peak hours; and performing as a Mo2Go SpeedSpot.</p>
      <p>Separately, Supermicro announced several large scale distributed training AI systems built using Intel Nervana Neural Network (NNP-T) ASICS.  </p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd990d82c886a0007ece7a4/960x0.jpg?fit=scale"/>
        <figcaption>Supermicro AI Systems with Intel NNP-T<cite>Images from Supermicro announcement</cite></figcaption>
      </figure>
      <p>The company said that the Intel Nervana NNP-T helped solves memory constraints and is designed to scale out through systems with racks easier than today's solutions. As part of the validation process, Supermicro integrated 8 NNP-T processors, dual 2<sup>nd</sup> Generation Intel Xeon Scalable processors, up to 6TB DDR4 memory per node supporting both PCIe card and OAM form factors. Supermicro NNP-T systems are expected to be available mid-year 2020. </p>
      <p>Brainchip is going to introduce its Akida Neuron Fabric using 80 neural processing units (NPUs) with 8 MB of SRAM initially manufactured by foundry TSMC for AI edge applications.  The layout of the Akida chip is shown below.</p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd9911dea103f0006532f02/960x0.jpg?fit=scale"/>
        <figcaption>Brainchip Akida AI Edge Solution Neural SoC Chip<cite>Image from Brainchip Presentation</cite></figcaption>
      </figure>
      <p>The company said that it is looking at the possibility of using non-volatile fast emerging memories in future versions of the chip in order to reduce power consumption for use in energy constrained applications.  Layer computations are done on allocated NPUs and all NPUs run in parallel in the fabric.  All intermediate results stored on chip memory, eliminating the additional overhead of off-chip memory access.  The device includes interfaces for external memory.</p>
      <p>Local memory and storage can play an important role.  Local cached content allows content delivery without burdening Internet networks.  On-chip memory also speeds up AI applications and reduces power consumption.  Using non-volatile fast memory will provide even lower power consumption.</p>
    </article>
  </body>
</html>