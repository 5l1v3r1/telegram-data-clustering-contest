<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.lepoint.fr/invites-du-point/aurelie-jean-quand-les-algorithmes-discriminent-les-femmes-23-11-2019-2349124_420.php"/>
    <meta property="og:site_name" content="Le Point"/>
    <meta property="article:published_time" content="2019-11-23T11:38:00+00:00"/>
    <meta property="og:title" content="Aurélie Jean – Quand les algorithmes discriminent les femmes"/>
    <meta property="og:description" content="CHRONIQUE. L'algorithme de l'application Apple Card proposerait des crédits plus importants aux hommes. Explications sur un bug très instructif."/>
  </head>
  <body>
    <article>
      <h1>Aurélie Jean – Quand les algorithmes discriminent les femmes</h1>
      <h2>CHRONIQUE. L'algorithme de l'application Apple Card proposerait des crédits plus importants aux hommes. Explications sur un bug très instructif.</h2>
      <address><time datetime="2019-11-23T11:38:00+00:00">23 Nov 2019, 11:38</time> by <a rel="author">Aurélie Jean</a></address>
      <p>À notre désagréable étonnement, ce n'est pas une manœuvre marketing maladroite de la part d'<a href="https://www.lepoint.fr/tags/apple">Apple</a> pour faire connaître sa carte de crédit, mais bien un problème plus inquiétant. L'application Apple Card proposerait des lignes de crédit bien plus élevées aux hommes qu'aux femmes. David Heinemeier Hansson, célèbre développeur informatique, en témoigne publiquement en confirmant une ligne de crédit 20 fois plus élevée pour lui que pour sa femme alors qu'ils ont les mêmes conditions fiscales et le même historique de crédit. Mais alors, pourquoi et comment l'algorithme d'Apple Card prêterait-il plus d'argent aux hommes ?</p>
      <p>Alors qu'on pensait loin le temps où les femmes devaient se battre pour l'obtention d'un compte bancaire à leur nom, on assiste à un rétropédalage dans l'égalité des droits économiques entre les genres. La grosse différence aujourd'hui est que ce ne sont pas des parlementaires et la société qui en sont l'origine, mais bien un algorithme dépourvu de corps et d'esprit. C'est vite dit, car derrière chaque algorithme se cachent des individus qui le pensent, qui le construisent, qui le programment, qui le testent, et qui le vendent. Chaque personne sur la chaîne de déploiement d'un algorithme et la compagnie en question ont un rôle à jouer et des responsabilités à honorer.</p>
      <h3>Bug discriminatoire</h3>
      <p>Dans ce cas précis, Apple n'a certainement pas écrit des conditions algorithmiques explicites qui sous-évaluent systématiquement la ligne de crédit des femmes. Mais l'algorithme a sûrement développé des critères implicites en étant entraîné sur des données passées biaisées d'obtention de crédit, et ce, sur plusieurs décennies. L'émancipation économique remarquable des femmes, toujours en évolution, biaise alors tout algorithme qui apprendrait sur les situations bancaires des individus plusieurs décennies en arrière, en discriminant malheureusement le genre féminin. Ce ne sont que des hypothèses. Et comme le souligne justement la scientifique Cathy O'Neil, pour confirmer la présence de biais, Apple devrait réaliser des tests sur ses algorithmes et sur les données d'apprentissage.</p>
      <p>Une fois l'erreur identifiée et admise, la compagnie doit trouver les origines de ce bug discriminatoire et le fixer. On peut par exemple ajouter des critères explicites de rééquilibrage, utiliser des données synthétiques à partir de données issues de population masculine à qui on affecterait le genre féminin, ou tout simplement en éliminant le genre des métadata des clients.</p>
      <h3>Frein à l'évolution de la société</h3>
      <p>Cette mauvaise expérience est également une leçon bien plus large sur les techniques d'apprentissage en intelligence artificielle : il ne faut en aucun cas transformer une tendance statistique en une condition systématique. Même si les données du passé présentent une représentativité statistique d'un plus faible crédit accordé aux femmes, en les utilisant comme vecteur de décision, on empêche notre société d'évoluer, pire on revient en arrière. Sans déresponsabiliser Apple, on admettra qu'ils ne sont pas un cas isolé. Il y a un an, l'algorithme test d'Amazon avait développé des biais de genre sur des sélections automatisées de CV pour le poste de développeur informatique, car il avait été entraîné sur les embauches des dix dernières années, majoritairement masculines.</p>
      <p>Vous l'aurez compris, les erreurs existent et doivent être partagées afin qu'elles ne se reproduisent plus, autant que les solutions et les bonnes pratiques !</p>
      <related>
        <h4>La rédaction vous conseille</h4>
        <a href="https://www.lepoint.fr/sciences-nature/aurelie-jean-l-etoile-scientifique-qui-monte-10-11-2019-2346286_1924.php"/>
        <a href="https://www.lepoint.fr/phebe/phebe-ce-qu-il-y-a-d-humain-dans-les-algorithmes-03-11-2019-2344971_3590.php"/>
      </related>
      <ul>
        <li>
          <a href="https://www.lepoint.fr/high-tech-internet/">High Tech et Internet</a>
        </li>
      </ul>
    </article>
  </body>
</html>