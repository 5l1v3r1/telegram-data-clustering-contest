<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.calcalistech.com/ctech/articles/0,7340,L-3774484,00.html"/>
    <meta property="og:site_name" content="CTECH"/>
    <meta property="article:published_time" content="2019-11-25T00:00:00+00:00"/>
    <meta property="og:title" content="Algorithms That Affect People’s Lives Should Be Transparent, Says Cyber Law Expert"/>
    <meta property="og:description" content="Nimrod Kozlovski, partner at Israel-based law firm Herzog, Fox, Neeman, and lecturer at Tel Aviv University spoke at Calcalist’s Mind the Tech conference in Tel Aviv"/>
  </head>
  <body>
    <article>
      <h6 class="kicker">Mind The Tech</h6>
      <h1>Algorithms That Affect People’s Lives Should Be Transparent, Says Cyber Law Expert</h1>
      <h2>Nimrod Kozlovski, partner at Israel-based law firm Herzog, Fox, Neeman, and lecturer at Tel Aviv University spoke at Calcalist’s Mind the Tech conference in Tel Aviv</h2>
      <address><time datetime="2019-11-25T00:00:00+00:00">25 Nov 2019</time> by <a rel="author">Lior Gutman</a></address>
      <p>Recently, U.S. legislators admitted for the first time that they were wrong to let commercial companies rate people to determine who gets credit or who stays in prison, Nimrod Kozlovski, partner at Israel-based law firm Herzog, Fox, Neeman, and lecturer at Tel Aviv University, said Monday.</p>
      <p>Kozlovski spoke at Calcalist and Israel’s Bank Leumi’s Mind the Tech conference in Tel Aviv. From now on, every artificial intelligence project in the U.S. must have its database evaluated to determine whether it creates consistent bias, Kozlovski said.</p>
      <figure>
        <img src="https://images1.calcalist.co.il/PicServer3/2019/11/25/950102/1L.jpg"/>
        <figcaption>Nimrod Kozlovski. Photo: Yariv Katz</figcaption>
      </figure>
      <p>Kozlovski presented three examples where databases allegedly created bias in decision-making processes, directly affecting people’s lives.</p>
      <p>In the U.S., the number of prisoners per capita is among the highest in the world, Kozlovski said. This led authorities to examine house arrests as an alternative to incarceration, he explained. The next logical step was to build an artificial intelligence-based system that examines every potential prisoner to create a ranking of their likelihood to repeat their crime, he added. “The ranking effectively determined who was released to house arrest and who remained in prison, but when the results were examined, the databases were found to be outdated or otherwise inaccurate.”</p>
      <p>The second example Kozlovski gave was U.S. social services, where algorithms attempt to determine whether a child is considered at-risk. According to Kozlovski, examining this system also revealed bias as it was based on decades-old assumptions.</p>
      <p>The last example was credit ratings, where women tended to score lower than men based on outdated assumptions as to their careers.</p>
      <p>Companies and public agencies should be required to reveal the data their algorithms are basing decisions on, giving people a chance to appeal or sue when a faulty algorithm affects their lives, Kozlovski said.</p>
      <related>
        <h4>Related stories:</h4>
        <a href="https://www.calcalistech.com/ctech/articles/0,7340,L-3774481,00.html"/>
        <a href="https://www.calcalistech.com/ctech/articles/0,7340,L-3774482,00.html"/>
        <a href="https://www.calcalistech.com/ctech/articles/0,7340,L-3774465,00.html"/>
      </related>
    </article>
  </body>
</html>