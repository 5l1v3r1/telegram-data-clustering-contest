<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://arstechnica.com/tech-policy/2019/11/nyc-wants-a-chief-algorithm-officer-to-counter-bias-build-transparency/"/>
    <meta property="og:site_name" content="Ars Technica"/>
    <meta property="article:published_time" content="2019-11-25T19:49:04+00:00"/>
    <meta property="og:title" content="NYC wants a chief algorithm officer to counter bias, build transparency"/>
    <meta property="og:description" content="The big, black decision-making boxes could get more transparent to New Yorkers."/>
  </head>
  <body>
    <article>
      <h1>NYC wants a chief algorithm officer to counter bias, build transparency</h1>
      <h2>The big, black decision-making boxes could get more transparent to New Yorkers.</h2>
      <address><time datetime="2019-11-25T19:49:04+00:00">25 Nov 2019, 19:49</time> by <a rel="author">Kate Cox</a></address>
      <p>It takes a lot of automation to make the nation's largest city run, but it's easy for that kind of automation to perpetuate existing problems and fall unevenly on the residents it's supposed to serve. So to mitigate the harms and ideally increase the benefits, New York City has created a high-level city government position essentially to manage algorithms.</p>
      <p>Mayor Bill de Blasio created the job, formally titled the Algorithms Management and Policy Officer, with an executive order he signed last week. The person who ultimately holds the role will be responsible for, basically, ethics management, which entails developing guidelines and best practices to make the city's automated decision-makers make more equitable, fair, and transparent decisions.</p>
      <h3>Why now?</h3>
      <p>The mayor's office announced the creation of the position following the publication of a report (<a href="https://www1.nyc.gov/assets/adstaskforce/downloads/pdf/ADS-Report-11192019.pdf">PDF</a>) by a city panel called the Automated Decision Systems Task Force, which spent about 18 months assessing the city's use of what it calls automated decision systems, or ADS.</p>
      <p>"The use of ADS is increasing both within New York City government and in municipalities across the United States," the report notes, "as cities continue to see value in delivering services more quickly and effectively to residents who depend on them, streamlining decision-making processes, expanding their abilities to help their residents, and attempting to identify and remove any human bias from their work."</p>
      <p>The panel's top recommendation was the creation of a single entity in the city government to standardize baseline principles to be applied by all other agencies within the city government that rely on automated decision-making. The report also suggests principles the manager should incorporate into that baseline framework when it is developed.</p>
      <p>A strong public education and transparency program should be among those considerations, the report says. The city should not only explain in plain language what algorithmic systems are in place but also create processes in which individuals can request more information about the decisions made by those systems, why those decisions are made, and potentially challenge those decisions.</p>
      <h3>Bigger than the Big Apple</h3>
      <p>In some ways, the city may prove a test bed for the rest of the nation. With a population of about 8.6 million, New York City has more residents squeezed into its square miles than the majority of US states. The city's five boroughs, standing alone, would be the 12th largest state by population, squeezing in after New Jersey and just ahead of Virginia.</p>
      <p>Decisions in every department of every state are increasingly being handed over to software systems that often prove both opaque and biased. Entire <a href="https://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/">books</a> have been written about the many ways in which software coded by human hands, with all our unconscious human biases, can perpetuate inequality.</p>
      <p>Algorithmic scoring in criminal justice, for example, tends to be <a href="https://arstechnica.com/tech-policy/2019/09/algorithms-should-have-made-courts-more-fair-what-went-wrong/">disproportionately punitive</a> to black suspects as compared to white ones. Algorithmic scoring in hospital settings has led to black patients <a href="https://arstechnica.com/science/2019/10/a-health-care-algorithm-offered-less-care-to-black-patients/">receiving far less medical care</a> than white patients.</p>
      <p>Even luxury goods aren't immune: New York financial regulators are <a href="https://arstechnica.com/tech-policy/2019/11/ny-regulators-investigating-apple-card-after-viral-complaint-of-sexism/">probing Apple and Goldman Sachs</a> after a report indicating some men were receiving far superior credit terms to their wives, who had higher incomes or better credit scores than their husbands, went viral.</p>
      <related>
        <h4>Further Reading</h4>
        <a href="https://arstechnica.com/tech-policy/2019/01/yes-algorithms-can-be-biased-heres-why/"/>
        <a href="https://arstechnica.com/science/2019/10/a-health-care-algorithm-offered-less-care-to-black-patients/"/>
      </related>
    </article>
  </body>
</html>