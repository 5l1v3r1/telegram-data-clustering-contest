<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://mspoweruser.com/microsoft-research-pen-first-ui-demo/"/>
    <meta property="og:site_name" content="MSPoweruser"/>
    <meta property="article:published_time" content="2019-11-03T10:53:00+00:00"/>
    <meta property="og:title" content="Microsoft Research shows off an innovative pen-first UI for tablets"/>
    <meta property="og:description" content="Microsoft Research has shown off a pen-first UI for tablets which may show up in Windows 10 X and the Surface Neo. See the video here."/>
  </head>
  <body>
    <article>
      <h1>Microsoft Research shows off an innovative pen-first UI for tablets</h1>
      <address><time datetime="2019-11-03T10:53:00+00:00">03 Nov 2019, 10:53</time> by <a rel="author">Surur</a></address>
      <p>Satya Nadella recently claimed tha<a href="https://mspoweruser.com/satya-nadella-is-dogfooding-the-surface-duo-and-neo-praises-pen-first-experience/">t Windows 10 X as embodied on the Surface Neo had a pen-first UI which he really enjoyed using</a>.  From the little we have seen and what has been leaked regarding Windows 10 X however there appears to be little innovation when it came to the pen-based features of the operating system, with the main application being taking notes in OneNote on one screen while viewing and interacting with content on another.</p>
      <p>This is a pity as Microsoft Research has of course been working for many years on very interesting new Natural User Interfaces combining Pen and Touch in very powerful ways.  One such recent demonstration is a way to add powerful sensing capabilities to tablets which let the operating system adapt to how users and holding the device, to adapt naturally to their intention.</p>
      <p>In the demo, <a href="https://www.microsoft.com/en-us/research/video/sensing-posture-aware-pentouch-interaction-on-tablets-2/">Microsoft Research proposes</a> sensing techniques that transition between various nuances of mobile and stationary use via postural awareness. These postural nuances include shifting hand grips, varying screen angle and orientation, planting the palm while writing or sketching, and detecting what direction the hands approach from.</p>
      <p>To achieve this, their system combines three sensing modalities:</p>
      <ol>
        <li>raw capacitance touchscreen images,</li>
        <li>inertial motion, and</li>
        <li>electric field sensors around the screen bezel for grasp and hand proximity detection.</li>
      </ol>
      <p>It includes interesting pen-first UI elements such as using the stylus as physical widget for “storing” and retrieving pictures and other clipboard items and a floating menu which is always present at the tip of your pen, no matter where you are on the screen.</p>
      <p>See the demo below:</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/b8zE0BcGiZ0" width="1140" height="641" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>The research was published in the ACM CHI 2019 Conference on Human Factors in Computing Systems—where the work received an Honorable Mention award.</p>
      <p>Do our readers agree that Windows 10 X and the Surface Neo is an opportunity for Microsoft to actually put some of their bold NUI ideas into practice? Let us know below.</p>
    </article>
  </body>
</html>