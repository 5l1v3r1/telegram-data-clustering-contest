<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.profesionalreview.com/2019/11/24/nnp-dlboost-keem-bay-intel/"/>
    <meta property="og:site_name" content="Profesional Review"/>
    <meta property="article:published_time" content="2019-11-24T03:14:07+00:00"/>
    <meta property="og:title" content="NNP, DLBoost y Keem Bay, Nuevos chips Intel para IA y redes neuronales"/>
    <meta property="og:description" content="Intel anunció nuevo hardware dedicado en su evento AI Summit el 12 de noviembre alejado del mercado masivo, NNP, DLBoost y Keem Bay."/>
  </head>
  <body>
    <article>
      <h1>NNP, DLBoost y Keem Bay, Nuevos chips Intel para IA y redes neuronales</h1>
      <address><time datetime="2019-11-24T03:14:07+00:00">24 Nov 2019, 03:14</time> by <a rel="author">Gustavo Gamarra</a></address>
      <p><b>Intel anunció nuevo hardware dedicado en su evento AI Summit el 12 de noviembre </b>alejado del mercado masivo, <b>NNP, DLBoost y Keem Bay.</b> Estos productos son sin duda la culminación de más de tres años de trabajo desde las adquisiciones de Movidius y Nervana en la segunda mitad de 2016 y la creación de su Grupo AI Products, liderado por Naveen Rao, cofundador de Nervana.</p>
      <h3>NNP, DLBoost y Keem Bay, Nuevos chips Intel para IA y redes neuronales</h3>
      <figure>
        <img src="https://www.profesionalreview.com/wp-content/uploads/2019/11/NNP-DLBoost-y-Keem-Bay-Nuevos-chips-Intel-para-IA-y-redes-neuronales_2.jpg"/>
      </figure>
      <p><b>Rao señaló que Intel ya es un gran actor en el sector de la IA y que sus ingresos por IA en 2019 superarán los 3.500 millones de dólares</b>, frente a los más de 1.000 millones de dólares de 2017. Intel ya tiene distintos hardware para todos los frentes OpenVINO para IOt, FPGAs Agilex, Ice Lake en el PC, DLBoost de Cascade Lake, y aún más lejos, sus futuros gráficos discretos.</p>
      <h4>Procesadores: DLBoost</h4>
      <figure>
        <img src="https://www.profesionalreview.com/wp-content/uploads/2019/11/NNP-DLBoost-y-Keem-Bay-Nuevos-chips-Intel-para-IA-y-redes-neuronales_3.jpg"/>
      </figure>
      <p><b>Intel demostró la compatibilidad con bfloat16 en Cooper Lake de 56 núcleos que saldrá el año que viene como parte de su gama DLBoost de funciones de IA en sus procesadores</b>. Bfloat16 es un formato numérico que alcanza una precisión similar a la del punto flotante de precisión única (FP32) en el entrenamiento de IA.</p>
      <p>Intel no proporcionó una estimación de la mejora del rendimiento, pero sí afirmó que, a efectos de inferencia, Cooper Lake es 30 veces más rápido que Skylake-SP. En el lado PC, Ice Lake incorpora las mismas instrucciones de DLBoost AVX-512_VNNI que también están en Cascade Lake.</p>
      <h4>Movidius: VPU de Keem Bay</h4>
      <figure>
        <img src="https://www.profesionalreview.com/wp-content/uploads/2019/11/NNP-DLBoost-y-Keem-Bay-Nuevos-chips-Intel-para-IA-y-redes-neuronales_4.jpg"/>
      </figure>
      <p><b>Como parte de su estrategia hacia la inteligencia artificial, como cámaras inteligentes, robots, drones y VR/AR, Intel adquirió Movidius en 2016</b>. Movidius llama a sus chips de baja potencia “vision processing units’’ (VPU). Presentan capacidades de procesamiento de señales de imagen (ISP), aceleradores de hardware, procesadores MIPS y procesadores vectoriales programables (VLIW) de 128 bits a los que llama núcleos SHAVE.</p>
      <p>Visita nuestra <a href="https://www.profesionalreview.com/hardware/mejores-procesadores/">guía sobre los mejores procesadores del mercado</a></p>
      <p>Intel ha detallado ahora lo que llama la ‘Gen 3’ Intel Movidius VPU con el nombre en código <b>Keem Bay</b>. Según Intel, tiene un rendimiento de inferencia más de 10 veces superior al de la Myriad X y consume la misma cantidad de energía.</p>
      <h4>Nervana Neural Network Processors (NNP)</h4>
      <figure>
        <img src="https://www.profesionalreview.com/wp-content/uploads/2019/11/NNP-DLBoost-y-Keem-Bay-Nuevos-chips-Intel-para-IA-y-redes-neuronales.jpg"/>
      </figure>
      <p><b>Intel tiene NNPs tanto para el entrenamiento como para la inferencia de redes neuronales profundas.</b> El NNP-I de Intel para inferencia se basa en dos núcleos de Ice Lake Sunny Cove y doce núcleos de acelerador ICE. Intel afirma que ofrecerá un estupendo rendimiento por vatio y densidad de cálculo. En su factor de forma M.2, es capaz de 50 TOPS a 12W, lo que equivale a 4.8TOPS/W, como ya había anunciado. Intel reveló que el factor de forma de la tarjeta PCIe consume 75W y produce hasta 170 TOPS (con precisión INT8).</p>
      <p>Intel reiteró su alta eficiencia de escalado casi lineal, del 95% para 32 tarjetas, en comparación con el 73% de Nvidia.</p>
      <p>Intel tiene preparado un amplio surtido de chips para todos los frentes, IA, 5G, redes neuronales, conducción autónoma, etc, en un mercado que generara este año Ingresos estimados en 10.000 millones de dólares. Os mantendremos informados.</p>
      <footer>Fuente<br/><a href="https://www.tomshardware.com/news/intel-announces-movidius-keem-bay-vpu">tomshardware</a></footer>
    </article>
  </body>
</html>