<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.forbes.com/sites/martineparis/2019/11/16/whats-my-score-great-hack-star-says-you-should-know/"/>
    <meta property="og:site_name" content="Forbes"/>
    <meta property="article:published_time" content="2019-11-16T00:00:00+00:00"/>
    <meta property="og:title" content="What’s My Score? Great Hack Star Says You Should Know"/>
    <meta property="og:description" content="A revolution is brewing, the resistance is growing, could the end be nigh for surveillance capitalism? The term is used to describe the productization of the human experience by corporate interests for the sole purpose of selling advertising at whatever cost to individual freedom and society."/>
  </head>
  <body>
    <article>
      <h1>What’s My Score? Great Hack Star Says You Should Know</h1>
      <address><time datetime="2019-11-16T00:00:00+00:00">16 Nov 2019</time> by <a rel="author">Martine Paris</a></address>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dcf729dea103f0006528e6e/960x0.jpg?fit=scale"/>
        <figcaption>The Great Hack's Professor David Carroll<cite>Netflix</cite></figcaption>
      </figure>
      <p>A revolution is brewing, the resistance is growing, could the end be nigh for <a href="https://news.harvard.edu/gazette/story/2019/03/harvard-professor-says-surveillance-capitalism-is-undermining-democracy/">surveillance capitalism</a>? The term, coined by Harvard’s Shoshana Zuboff, is used to describe the productization of the human experience by corporate interests for the sole purpose of selling advertising at whatever cost to individual freedom and society. The data monopolies of the past decade, which include Facebook, Apple, Amazon, Netflix and Google, have spurred massive economic growth and the longest bull run in history, but if <a href="https://www.crunchbase.com/organization/cambridge-analytica#section-overview">Cambridge Analytica</a> has taught us anything, it’s that selling our soul (literally) for infinite prosperity has its price. </p>
      <p>At the time the scandal broke in March 2018, the now infamous British political consulting firm founded by Steve Bannon and Robert Mercer had claimed to have nearly 5,000 data points on every American voter, some 230 million people. Weaponizing this data, they microtargeted specific individuals in key territories and bombarded them with deceptive ads that many believe influenced the outcome of US and UK elections.</p>
      <p>One of the most popular documentaries on this subject is Netflix’s <a href="https://youtu.be/iX8GxLP1FHo">The Great Hack</a> which dissects what happened through the eyes of former employees and privacy advocate David Carroll, the Parsons School of Design Professor who sued Cambridge Analytica under the <a href="http://www.legislation.gov.uk/ukpga/1998/29/contents">UK Data Protection of 1998</a> to obtain his data, profile and score.</p>
      <p>I had a chance to talk with Carroll following his data rights panel at the Fast Company Innovation Festival last week. The following is an edited transcript of our discussion:</p>
      <p>
        <b>There’s been a surge in interest in getting comprehensive federal data privacy laws passed in the U.S. thanks in part to the Cambridge Analytica scandal. By way of background, can you explain what laws they broke?</b>
      </p>
      <p>Cambridge Analytica breached the UK Data Protection Act of 1998 by unfairly processing people's profiles, politically, in connection with the U.S. election. In the UK, it’s illegal to create political profiles without permission and they did that for all registered voters, not just the 87 million people they had access to on Facebook, but for every American voter. You didn't even need to be on Facebook to have your data abused. </p>
      <p>The UK had jurisdiction because Cambridge Analytica was a UK company who exploited our data on UK soil, and in the UK rights are afforded where data is processed. Had they captured our data in the U.S., there would have been no recourse because the U.S. does not have federal data privacy laws. </p>
      <p>Cambridge Analytica got their data from Facebook developers, Joseph Chancellor and Aleksander Kogan, who created a survey app (“This Is Your Digital Life”) that harvested data not just on the (270,000) Facebook users of the app, but on the 87 million U.S. Facebook users who were friends of friends. They were able to do this through default permissions that few users knew about. At the time, users needed to opt-out by going deep into privacy settings and clicking a button that said, “Don't let developers share my friends data.”</p>
      <p><a href="https://www.priv.gc.ca/en/opc-actions-and-decisions/investigations/investigations-into-businesses/2019/pipeda-2019-002/">Canadian privacy commissioners</a> warned Facebook that this was a privacy disaster waiting to happen, but it was during the days of Farmville and Facebook pushed back saying they needed it for growth for their developers.</p>
      <p>Additionally, Chancellor and Kogan said the data harvesting was for academic purposes, then sold it for political purposes to Cambridge Analytica. This too was illegal in the UK where you can’t collect data for one purpose and then sell it for another purpose. It also violated Facebook's own policies.</p>
      <p>
        <b>When you look across the globe there seems to be a wide spectrum of how data rights are handled.</b>
      </p>
      <related>
        <a href="https://video.vice.com/en_us/video/china-tests-social-credit-system-to-score-citizens-behaviors/5c104d8ebe4077635d69bc57"/>
      </related>
      <related>
        <a href="https://time.com/5290043/nazi-history-eu-data-privacy-gdpr/"/>
      </related>
      <p>
        <b>Somewhere in the middle lies America, land of free enterprise where companies move fast and break things and corporations have a fiduciary duty to maximize shareholder profits. America is different than the EU and China when it comes to data rights because although no one wants an Orwellian nightmare, no one wants to stifle innovation either.</b>
      </p>
      <p>This is true. In America, corporations even have personhood with corporate speech protected by the First Amendment. This is the basis of the <a href="https://www.npr.org/2014/07/28/335288388/when-did-companies-become-people-excavating-the-legal-evolution">2010 Supreme Court decision</a> which says you can’t restrict corporate political spending. In the EU and China, there is no freedom of speech, however, the <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:12012P/TXT">EU Charter</a> has the right to free expression and the right to data protection, striking that balance.</p>
      <p>Americans need to think about what's happening in China and the EU and where we want to be decades from now. There are people that say it's not worth passing data privacy regulation because the laws are always catching up with technology, but in the case of the <a href="https://ec.europa.eu/home-affairs/what-we-do/networks/european_migration_network/glossary_search/charter-fundamental-rights-european_en">EU Charter</a>, they were 30 years ahead of the game. They foresaw the future and imagined the need to have data protection as a fundamental human right long before we got here, and addressed things like genetic and biometric privacy and so on. </p>
      <p>The EU’s GDPR [<a href="https://gdpr-info.eu/">General Data Protection Regulation</a>] <a href="https://gdpr-info.eu/issues/right-to-be-forgotten/">Right To Be Forgotten</a> is a representation of just how advanced their implementation of these rights have gotten while the U.S. hasn’t even bothered to think about these things. </p>
      <p><b>Many U.S. states have passed </b><a href="http://www.ncsl.org/research/telecommunications-and-information-technology/state-laws-related-to-internet-privacy.aspx"><b>data brokerage registration laws</b></a><b> and the CCPA [</b><a href="https://oag.ca.gov/system/files/attachments/press_releases/CCPA%20Fact%20Sheet%20%2800000002%29.pdf"><b>California Consumer Protection Act</b></a><b>]</b> <b>goes into effect on January 1. In an economy that is so data-driven, will these new privacy laws be good or bad for business?</b></p>
      <p>I think they will be good for business as privacy startups emerge to help people take advantage of these new rights, collect scores, and deal with the red tape that's required to do so, which can be optimized and made more efficient.</p>
      <p>With the new California law, consumers will have the right to know what data is being collected, have the right to delete that data, and have the right to opt-out of sale of that data, and companies will be required to display buttons that say, “Do not sell my data.” So it’s a good step forward. </p>
      <p>I would like to see the laws go further and require companies to proactively send a data statement, similar to the type of bank statement that banks are required to send that disclose to us our financial information. Why should we have to do the work? Companies should have to send you your data profile, score, and how they generated it, and if you dispute it, click here, and if you want to delete it, click here. </p>
      <p>
        <b>Do you think Congress understands the technology enough to regulate it?</b>
      </p>
      <p>I don't think they understand custom audiences and retargeting well enough or the way data is recombined by data brokers and how shadow profiles are created. They need to better understand the difference between data that we supply to the platforms and the data that the platforms infer about us. </p>
      <p>
        <b>Can you explain the issue with microtargeting?</b>
      </p>
      <p>When you go into the Facebook Ads Manager and build a campaign, you start with segmentation and select parameters, and as you make these choices that are based on demographics and interest, you can narrow your targeted audience down. But then there's another feature called <a href="https://www.facebook.com/business/help/341425252616329?id=2469097953376494">custom audiences</a> in which you can upload a data set. This could be a list of names a list of email addresses, phone numbers, cookie data. When you do that it matches to the Facebook profile. From there you can do what's called lookalikes. If you have a customer list you can upload that and an algorithm will match all the Facebook users that are similar. This enables you to target people by name and is a very effective way to grow an audience and business.</p>
      <p><a href="https://www.propublica.org/article/facebook-blocks-ad-transparency-tools">Pro Publica’s Jeremy Merrill</a> posted on <a href="https://twitter.com/jeremybmerrill/status/1192222165338337281">Twitter</a> an image of the Facebook ads transparency tool where Clear, the biometric and security clearance company, allowed Delta Airlines to use its data to microtarget people with ads. When people sign up for Clear and agree to let them use their biometrics to speed through security, many blindly click through privacy policies not realizing that they’ve just given permission for Clear to share their most personal information with the airlines for marketing purposes. Companies are pushing boundaries more and more and will try get away with whatever they can. </p>
      <p>
        <b>How did we get here?</b>
      </p>
      <p>Companies know there’s power in data. In their memoirs, former Cambridge Associate employees, Christopher Wylie and Brittany Kaiser,<b> </b>describe how they were seduced by the data, the power that you feel by collecting it, reducing it to measurements, using those measurements to change behavior, and then watching the behavior change. </p>
      <related>
        <a href="https://www.vice.com/en_us/article/43k7z3/nationwide-fake-host-scam-on-airbnb"/>
      </related>
      <p>I think it’s troubling. We’re all racing full steam ahead incorporating these algorithms into our society and there's no caution being taken. It’s all being done in the name of efficiency and scale. </p>
      <p>Some of the most disturbing examples are related to law enforcement, employment, and healthcare. With law enforcement, our algorithms are increasingly deciding criminal penalties. Data biases are perpetuating systemic racism, encoding it into the system that’s determining people's freedom. </p>
      <p>With employment, big companies are increasingly adopting systems that analyze social media and use face and emotion recognition technology during interviews to pre-screen applicants. You could be denied a job that you are entirely qualified for because some arbitrary machine learning algorithm has sifted you out and you have no way to know how that even happened. </p>
      <p>With healthcare, a big argument for data mining is to cure cancer. That potentially could advance medicine in ways that might appear to be good but also could have unintended consequences. As we move towards genetic analysis, the overlap between commercial genetic testing industry and the healthcare industry is blurring faster and faster, and once your genetics are breached there's no going back. There have already been reports of biometric identity companies being breached. When your passwords are breached you can change your password but once your facial geometry, your fingerprints, your iris scans are breached, you can't change that - you're just permanently breached. </p>
      <p>As we race forward, we need to be cognizant of how this might play out. This is why we need fundamental data rights so that individuals can exert their autonomy and personal freedoms to counterbalance these new infrastructures. This is why we are arguing for rights to our scores.</p>
      <p>New York Times reporter Kashmir Hill <a href="https://www.nytimes.com/2019/11/04/business/secret-consumer-score-access.html">wrote about</a> her experience trying to get her scores from data brokers under the new laws, so we're starting to see the glimmer of this becoming possible. </p>
      <p>Once we know our scores, we can better understand how they might be affecting our life possibilities. We can demand companies explain how they are creating these scores and challenge the opacity of their black box algorithms. I don't think we want a delivery service determining our autonomy as a human being. It's not worth it. Our obsession with getting trust scores when we don't even know how the score was generated seems like it's not really a trustworthy measurement.</p>
      <p>
        <b>Where are we heading with all of this?</b>
      </p>
      <p>The movie “Her” by Spike Jonze takes an interesting look at the <a href="https://medium.com/@profcarroll/spike-jonezs-intention-economy-a0f5f4ea595d">future of AI and advertising</a>. In its uncluttered universe there is little need for advertising because AI agents work on behalf of humans. An agent might say to a user, “Your mom's birthday is coming up, she might really enjoy a handwritten card. I know a service that can do that for you. Shall I order one?” The agent then goes to the automated market to negotiate the deal on behalf of the user, and perhaps exchange data.</p>
      <p>Another advertising dystopia is “Minority Report” where<b> </b>iris scanners follow Tom Cruise around in a mall where he is being microtargeted for ads in real-time. A Lexus ad refers to him by name, an American Express ad shows his name on the card, and he’s greeted by name at <a href="https://youtu.be/hlX_F1VKIdo">The Gap.</a> This fusion of personalized advertising and government surveillance fully-realized creates a terrifying film. In some ways the advertising world holds Minority Report as their holy grail. </p>
      <p>
        <b>Why has this become your mission?</b>
      </p>
      <p>When we’re talking scores, we’re making life into a game where there can only be a few winners. If we don’t address this now, a lot of people are going to lose. </p>
      <p>
        <i>This conversation has been condensed and edited for clarity.</i>
      </p>
    </article>
  </body>
</html>