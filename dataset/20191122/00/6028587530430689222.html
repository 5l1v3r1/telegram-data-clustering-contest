<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.forbes.com/sites/ibm/2019/11/22/ibm-engineer-bob-dennard-and-the-chip-that-changed-the-world/"/>
    <meta property="og:site_name" content="Forbes"/>
    <meta property="article:published_time" content="2019-11-22T00:00:00+00:00"/>
    <meta property="og:title" content="IBM Engineer Bob Dennard And The Chip That Changed The World"/>
    <meta property="og:description" content="Robert Heath Dennard’s life and career illustrate the power of invention and ingenuity in transforming America from an agrarian society to today’s post-industrial information economy."/>
  </head>
  <body>
    <article>
      <h1>IBM Engineer Bob Dennard And The Chip That Changed The World</h1>
      <address><time datetime="2019-11-22T00:00:00+00:00">22 Nov 2019</time> by <a rel="author">IBM Contributor</a></address>
      <p><b>By John Markoff</b>The inspiration, as is often the case, came late one evening. Bob Dennard, then a 34-year-old IBM electrical engineer, was pondering a colleague’s research presentation he’d seen earlier that day.</p>
      <p>It was autumn 1966, and Dennard had been helping design a new form of computer memory using silicon-based transistors. This approach, known as microelectronics, would stand in contrast to the computer industry’s tradition of storing data magnetically – a technology referred to as core memory. The presentation that Dennard had listened to with a trace of envy was by a competing team of IBM engineers at work on new, smaller forms of magnetic storage.</p>
      <p>Back then, the largest magnetic storage systems, requiring room-sized equipment, could store only one megabyte of information. That would be equivalent to a 500-page book – or about a minute of a Top 40 pop song. Besides being bulky, these storage systems were slow and they consumed huge amounts of electrical power. But now Dennard’s colleagues were describing a plan to shrink the next generation of magnetic memory to a compact 25-centimeter square.</p>
      <p>Inspired, Dennard that evening wondered whether he could develop something equally simple and elegant – and probably much cheaper. His approach would involve semiconductor technology that used patterns of tiny wires, transistors and other components etched in silicon.</p>
      <p>At the time, IBM’s silicon chip designers were wrestling with complex circuits that would use six transistors to store a single bit of data. They worked. But they were hardly efficient – and it didn’t seem a promising way to build systems capable of storing large volumes of data. But suddenly, that evening in 1966, Dennard had a flash of insight: <i>You could store a bit with a single transistor!</i></p>
      <p>To do so, he realized, would require using tiny amounts of energy to repeatedly refresh the capacitor that held the charge to store the bit. Otherwise, the data would soon be lost, as the charge dissipated. But if you were willing do that, you could achieve the same storage capacity in a far simpler, smaller and more elegant circuit.</p>
      <p>From that insight, Bob Dennard invented dynamic random-access memory, or DRAM.</p>
      <p>Dennard’s insight would soon lead to DRAM chips with a kilobit (a thousand bits) of memory. Over the next five-and-a-half decades  DRAM would, generation by generation, evolve into an 8-gigabit (8 billion bits) storage medium. Today, 8-gigabit DRAM chips are found in everything from the smartphones in our pockets to the supercomputers that power the global economy. And 16-gigabit DRAM is on the way.</p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd569472c886a0007ec99eb/0x800.jpg?cropX1=0&amp;cropX2=1200&amp;cropY1=135&amp;cropY2=1679"/>
        <figcaption>Robert H. Dennard changed the future of computing with his invention, conceived in 1966, that reduced computer memory to a single-transistor cell known as dynamic random-access memory, or DRAM.<cite>IBM</cite></figcaption>
      </figure>
      <p>Not only did DRAM memory sweep away the earlier magnetic technologies. It became the foundational technology for an industry that has reshaped human society – from the way we work, to the way we entertain ourselves and even to the way we fight wars. Today, two-thirds of the human population carries Dennard’s invention in their pocket, contained within cellphones or smartphones that are capable of storing movies, thousands of photos and entire music libraries.</p>
      <p>Dennard’s career was celebrated recently when the Semiconductor Industry Association bestowed upon him the Robert N. Noyce Award, the industry’s highest honor.</p>
      <p>“With Bob's invention, we were able to really unleash the capability of programmable computing,” said Dr. John E. Kelly III, IBM executive vice president.</p>
      <p>Dennard, in a 2013 speech accepting the prestigious Kyoto Prize in Basic Sciences, recalled the good fortune of IBM’s assigning him the task of translating computer memory to the new silicon-based technology of the 1960s. “I was ready,” he said. “I was working on the right project at the right time, and I knew my mission.”</p>
      <p>
        <b>The Theory that Inspired the Future</b>
      </p>
      <p>Dennard began his career in 1958 as a newly minted electrical engineer with a Ph.D from Carnegie Institute of Technology, now Carnegie Mellon University. He went to work at IBM’s then partially completed Thomas J. Watson Research Center in Yorktown Heights, New York, designed by the neo-futurist architect Eero Saarinen.</p>
      <p>Yorktown Heights was, and remains, the workplace of some of the world’s foremost scientists, electrical engineers and physicists – an intellectual hive where many of IBM’s running total of more than 139,000 total U.S. patents have been hatched.</p>
      <p>Dennard’s 1966 invention of the DRAM is not his only lasting mark on the computing world. In 1972, he conceived an idea that had an equally lasting legacy, showing a path forward for the entire semiconductor industry. He sketched out a detailed model, fleshed out in a paper two years later, for his “scaling theory,” now known as Dennard Scaling.</p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd568bae0af7b0006b1d17f/0x800.jpg?cropX1=0&amp;cropX2=1647&amp;cropY1=0&amp;cropY2=1733"/>
        <figcaption>Besides inventing DRAM, Dennard made a lasting mark with a prediction known as Dennard Scaling – a physics roadmap that the semiconductor industry could follow for decades. Simply put: As more and more transistor could be packed into ever smaller spaces, computers themselves would become faster and cheaper.<cite>IBM</cite></figcaption>
      </figure>
      <p>Dennard Scaling predicted that as the dimensions of transistors became smaller, they would become more numerous in a given space, more powerful and cheaper, even as the energy each chip consumed would remain almost constant.</p>
      <p>With each successive generation, there would be a doubling of the number of transistors that could fit into a given space, as the operating frequency – known as clock speed – would increase by 40 percent.</p>
      <p>In short: As more and more transistors could be packed into ever smaller spaces, computers themselves would get faster and cheaper.</p>
      <p>Implicit in this simple set of principles was not only a remarkable economics-of-scale but an entire business ecosystem. Since his 1974 paper, the semiconductor industry has marched forward guided by a steadfast belief in Dennard Scaling, willing to invest hundreds of billions of dollars in the breakthroughs that have led to astounding technical progress.</p>
      <p>Companies and entire industries could map their futures on the assumption that every few years, newer and ever more powerful memory and logic chips would be attainable.</p>
      <p>Executives in the semiconductor industry readily acknowledge Dennard’s scientific contribution. They also note his willingness to share his wisdom.</p>
      <p>“Bob was already a legend the first time I met him when I joined IBM Watson in 1995,’’ said Lisa Su, president and CEO of the chipmaker AMD, who spent more than a dozen years at IBM early in her career. “Besides being a brilliant technologist, he was always willing to mentor and guide young device folks like me.’’</p>
      <p>
        <b>A Golden Age of Technical Progress</b>
      </p>
      <p>The world had been first alerted to the power of scaling by Douglas Engelbart in 1959 in a paper presented at a Philadelphia technical conference. Engelbart, then working at the Stanford Research Institute, realized that shrinking transistors in silicon-based systems would lead to increasingly powerful computing systems. He went on to design technologies that would lead directly to personal computing and the Internet.</p>
      <p>Sitting in the audience for Engelbart’s presentation was Gordon Moore, then a 30-year-old chemist who two years earlier had helped found Fairchild Semiconductor Corporation. In 1965 Moore, in an article in <i>Electronics Magazine</i>,<i> </i>noted that the semiconductor industry was “cramming more components onto integrated circuits” at a remarkable rate, at the time doubling the number of transistors that could be etched on a silicon chip at a regular interval of roughly two years.</p>
      <p>Soon thereafter, the phenomenon was dubbed “Moore’s Law” by California Institute of Technology physicist Carver Mead, who would proceed to develop new design methodologies that would make it possible to create computing chips with hundreds of thousands of components. The chips of this era are known as Very Large Scale Integrated, or VLSI, circuits.</p>
      <p>It would be Dennard, however, who spelled out the math and physics underpinning Moore’s Law and who provided the industry agenda for what IBM Fellow</p>
      <p>Bijan Davari describes as the “golden age” of scaling that lasted into the early 21st century.</p>
      <p>“During that period the frequency of the microprocessor improved 1,000-fold,” Davari said. That in turn created a virtuous circle as more memory and more computing speed made new software applications possible, which in turn created new demands for computing power.</p>
      <figure>
        <img src="https://specials-images.forbesimg.com/imageserve/5dd56967ea103f000652e09b/960x0.jpg?fit=scale"/>
        <figcaption>Robert Dennard, 1999<cite>IBM</cite></figcaption>
      </figure>
      <p>
        <b>A French-Horn Turning Point</b>
      </p>
      <p>Robert Heath Dennard’s life and career illustrate the power of invention and ingenuity in transforming America from an agrarian society to today’s post-industrial information economy.</p>
      <p>Born in 1932, in Terrell, Texas, Dennard spent some of his early years on a farm without electricity and taking his lessons in a one-room schoolhouse. When Dennard was in high school, still in Texas but now living near Dallas, a guidance counselor recommended that he pursue electrical engineering because it was a growing field. He had been planning to join his friends at a local junior college. But then, a recruiter for Southern Methodist University, aware of his talent with the French horn, offered young Dennard a band scholarship. Music and science would ever after be his twin passions.</p>
      <p>After SMU came Carnegie Institute and then IBM. It was a time of technological ferment. The transistor had been demonstrated a decade earlier at Bell Laboratories. In 1958, shortly after Dennard’s arrival at Yorktown Heights, Jack Kilby, a Texas Instruments electrical engineer, demonstrated the first solid state electronics device composed of a transistor, a resistor and a capacitor to create a simple oscillator circuit. It was indeed ‘’the right time’’ for Dennard, whose career would parallel the explosive growth of the scientific advances and societal impacts of microelectronics.</p>
      <p>
        <b>“A Willingness to Go Beyond the Rules”</b>
      </p>
      <p>Beyond his inventions and his theories, Dennard would make his mark at IBM by instilling a culture of innovation in the company’s research division. Thomas J. Watson Jr., who led the company from the early 1950s to the early 1970s, referred to employees like Dennard as “wild ducks.” People who refused to “fly in formation.”</p>
      <p>Russ Lange, an IBM Fellow and former IBM Vice President of Technology, who worked with Dennard for many years, said that his wild-duck instincts meant that no challenge was insurmountable.</p>
      <p>“You needed a willingness to go beyond the rules, to not be told <i>no</i> by executives,’’ Lange said. ‘’That was a critical aspect to really making progress in this industry. That was the recognition of the wild duck.”</p>
      <p>Those qualities are more essential than ever today at IBM – and wherever else researchers continue to push the boundaries of computing. Because, despite the decades-long durability of Dennard’s pivotal theory, the golden age of scaling is now over.</p>
      <p>First, around 2006, computer clock speed stopped increasing; nanoscale transistors were nearing such microscopic dimensions that the leakage of electrical current was becoming a vexing challenge. The work-arounds have included a variety of technical breakthroughs, including new types of insulating materials and 3-D chip architectures that minimize current leakage.</p>
      <p>More recently, though, the cost of transistors has stopped falling. That is mainly because of the huge investments required – more than $20 billion – to build silicon foundries capable of making chips with features 10 nanometers and smaller.</p>
      <p>In other words, computing’s virtuous cycle of ever faster, ever cheaper chips has slowed to a virtual crawl.</p>
      <p>These developments have put a renewed premium on the values that Bob Dennard’s career embodied: human ingenuity and invention. The physical limits of semiconductors, for example, are one reason that IBM is intent on pursuing quantum computing to help solve some of the most specialized kinds of information-processing challenges in coming years.</p>
      <p>“Bob, and I would always have lively discussions about whether there was there going to be an end to the scaling,” Lange recalled. “And he would say, ‘Yes, there's an end to scaling. But there's no end to creativity.’”</p>
      <p>______          </p>
      <p>
        <i>John Markoff, a former New York Times technology reporter, is writer-in-residence at the Stanford Institute for Human-Centered Artificial Intelligence.</i>
      </p>
    </article>
  </body>
</html>