<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.techzine.be/nieuws/trends/49244/google-laat-ai-uitleggen-waarom-het-bepaalde-beslissingen-maakt/"/>
    <meta property="og:site_name" content="Techzine"/>
    <meta property="article:published_time" content="2019-11-21T10:05:34+00:00"/>
    <meta property="og:title" content="Google laat AI uitleggen waarom het bepaalde beslissingen maakt"/>
    <meta property="og:description" content="Met Google Cloud AI Explanations kondigt Google op de Cloud Next-conferentie in Londen een systeem aan dat transparantie moet geven inzake de beslissingen die een AI-systeem neemt. Google is zich bewust van de negatieve gevolgen van AI als een zwarte doos die ongecontroleerde beslissingen neemt. Wanneer professionele organisaties willen vertrouwen op AI, moet er een […]"/>
  </head>
  <body>
    <article>
      <h1>Google laat AI uitleggen waarom het bepaalde beslissingen maakt</h1>
      <address><time datetime="2019-11-21T10:05:34+00:00">21 Nov 2019, 10:05</time> by <a rel="author">Michaël Aussems</a></address>
      <p>
        <b>Met Google Cloud AI Explanations kondigt Google op de Cloud Next-conferentie in Londen een systeem aan dat transparantie moet geven inzake de beslissingen die een AI-systeem neemt.</b>
      </p>
      <p>Google is zich bewust van de negatieve gevolgen van AI als een zwarte doos die ongecontroleerde beslissingen neemt. Wanneer professionele organisaties willen vertrouwen op AI, moet er een manier zijn om de algoritmes ter verantwoording te roepen. Die introduceert de internetreus op Google Cloud Next in Londen onder de noemer ‘Google Cloud AI Explanations’.</p>
      <h3>Verklaring voor beslissingen</h3>
      <p>Explanations ambieert om op een overzichtelijke manier aan te geven welke factoren een algoritme ertoe hebben aangezet om een bepaalde beslissing te nemen. Concreet geeft het systeem een waarde aan ingevoerde data al naargelang hoe hard die gewogen heeft op de eindbeslissing. Dat moet gebruikers helpen om te begrijpen welke datapunten van belang zijn. Zo kunnen ze het model eventueel aanpassen om beter tegemoet te komen aan de verwachtingen.</p>
      <figure>
        <img src="https://cdn7.tz.nl/wp-content/uploads/2019/11/google-oog.jpg"/>
      </figure>
      <p>De voorstelling van de datapunten kan op een visuele manier gebeuren. Google geeft als voorbeeld een AI die een dokter helpt bij de diagnose van een oogaandoening als voorbeeld. Het gaat om diabetic retinopathy, al hoef je geen doktersdiploma te hebben om te begrijpen wat Explenations precies doet. De AI kijkt naar foto’s van een oog en helpt bij de diagnose. Voor een dokter volstaat een ‘ja’ of een ‘nee’ natuurlijk niet, die wil weten waarop de AI de diagnose baseert. Explanations zorgt voor een geannoteerd beeld waarin de delen van de scan die verantwoordelijk zijn voor de diagnose worden aangeduid. “Ook voor kredietverstrekkers is het belangrijk om te kunnen zeggen waarom iemand precies geweigerd wordt”, voegt Google Cloud-CEO Thomas Kurian nog toe.</p>
      <h3>Niet perfect</h3>
      <p>Google geeft toe dat Explanations nog beperkt is. Zo geeft de tool wel aan welke datapunten belangrijk zijn in een beslissing van een algoritme, maar vertelt het niets over de relaties en verbanden tussen datapunten en de manier waarop die het beslissingsproces beïnvloeden. Google maakt zich sterk dat het aan zoveel mogelijk transparantie werkt maar geeft terwijl mee dat gebruikers die beperking in het achterhoofd moeten houden wanneer ze de data van Explanations interpreteren.</p>
      <p>Google Cloud AI Explanations is per direct beschikbaar voor modellen die draaien op AutoML Tbles en Google AI Platform Predictions. De tool speelt samen met de What-If-tool van Google, en kan zo helpen om een gedetailleerd beeld uit te bouwen van het gedrag van een getraind model.</p>
      <h3>Racisme</h3>
      <p>In het kader van transparantie pakt Google in Londen verder uit met wat het Model Cards noemt. Dat zijn een soort certificaten die bij getrainde AI-modellen horen en praktische informatie geven over een specifiek model. Zo zien gebruikers snel hoe een model werd getest maar ook waar eventuele beperkingen liggen. Google zet zijn principiële aanpak rond de ontwikkeling van AI erg in de verf op de conferentie. Het bedrijf moet wel: het ontdekte in 2015 al uit eerste hand wat er gebeurt wanneer je te los omspringt met de training van modellen. Het beeldherkenningsalgoritme van Google uitte zich toen als racistisch door zwarte mensen als ‘Gorilla’ te categoriseren.</p>
    </article>
  </body>
</html>