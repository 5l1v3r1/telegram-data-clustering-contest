<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.theverge.com/2019/11/11/20959260/twitter-deepfake-manipulated-media-survey-policy-facebook"/>
    <meta property="og:site_name" content="The Verge"/>
    <meta property="article:published_time" content="2019-11-11T15:54:00+00:00"/>
    <meta property="og:title" content="Twitter proposes draft deepfake policies"/>
    <meta property="og:description" content="And it wants your help in choosing how to moderate them"/>
  </head>
  <body>
    <article>
      <h1>Twitter proposes draft deepfake policies</h1>
      <h2>And it wants your help in choosing how to moderate them</h2>
      <address><time datetime="2019-11-11T15:54:00+00:00">11 Nov 2019, 15:54</time> by <a rel="author" href="https://www.theverge.com/authors/makena-kelly" target="_blank">Makena Kelly</a></address>
      <p>On Monday, Twitter announced that it would be developing a deepfake policy and asked users to help make its final decision on the new rule.</p>
      <p>Late last month, the <a href="https://twitter.com/TwitterSafety/status/1186403736995807232">Twitter Safety team announced</a> it’d be seeking feedback on what a deepfake and synthetic media policy would look like on the platform. In a blog post on Monday referencing that announcement, Twitter vice president of trust and safety Del Harvey wrote that if manipulated media was flagged on the platform, Twitter could end up placing a notice next to it alerting users that it’s been distorted, warning them that it’s false before they share it, or adding context in the form of a link or news article breaking down why others believe that it’s untrue. Twitter could also remove the content, Harvey wrote.</p>
      <aside>It’s similar to how government agencies issue new rules</aside>
      <p>At the end of the blog, Twitter <a href="https://survey.twitterfeedback.com/survey/selfserve/53b/191016?list=3&amp;co=BLOG#?">directs users to take a survey</a> to help weigh the platform’s options. It asks multiple-choice questions prompting them to help decide whether manipulated video should simply be removed or flagged. It’s similar to how government agencies like the Federal Communications Commission issue new rules — first publishing their drafts and asking for public comment before officials vote.</p>
      <p>The survey questions show a company embroiled over whether it should have the power to decide what’s true or false. It’s an issue other platforms, like Facebook, have struggled with over the past couple months. Much of the content moderation discussion we’re seeing now came to a head last May when a video of <a href="https://www.theverge.com/2019/5/24/18637771/nancy-pelosi-congress-deepfake-video-facebook-twitter-youtube">House Speaker Nancy Pelosi</a> distorted to make her appear drunk circulated across social media platforms like Facebook, Twitter, and YouTube. At the time, YouTube removed it. Facebook sent it to a third-party fact-checker and laid news coverage debunking the video next to it while warning users that it was false before they shared it. Twitter let it stand.</p>
      <p>The debate over whose responsibility it is to decide what’s true and false has only heated up in recent weeks after Facebook decided not to take down misleading or false ads placed by politicians. After lawmakers called out CEO Mark Zuckerberg and his company for this decision, <a href="https://www.theverge.com/2019/10/30/20940587/twitter-political-ad-ban-election-2020-jack-dorsey-facebook">Twitter announced that it would ban all political ads</a> on November 22nd.</p>
      <p>Twitter didn’t have a deepfake policy when the Pelosi video went viral, and now it looks like the company is moving to change that. On November 27th, Twitter will close its commenting period and announce an official policy 30 days before it rolls out.</p>
    </article>
  </body>
</html>