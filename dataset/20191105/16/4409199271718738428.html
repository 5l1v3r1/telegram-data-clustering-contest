<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.spiegel.de/netzwelt/gadgets/light-commands-forscher-taeuschen-sprachassistenten-per-laser-a-1294910.html"/>
    <meta property="og:site_name" content="SPIEGEL ONLINE"/>
    <meta property="article:published_time" content="2019-11-05T16:37:00+00:00"/>
    <meta property="og:title" content="Forscher hacken Sprachassistenten mit Laserattacke"/>
    <meta property="og:description" content="Dieser Angriff funktioniert sogar durch die Fensterscheibe: Wissenschaftler haben einen Weg gefunden, Sprachassistenten heimlich Befehle zu geben - per Laser. Die Forscher haben eine Schwachstelle entdeckt."/>
  </head>
  <body>
    <article>
      <h1>Forscher hacken Sprachassistenten mit Laserattacke</h1>
      <h2>Dieser Angriff funktioniert sogar durch die Fensterscheibe: Wissenschaftler haben einen Weg gefunden, Sprachassistenten heimlich Befehle zu geben - per Laser. Die Forscher haben eine Schwachstelle entdeckt.</h2>
      <address>
        <time datetime="2019-11-05T16:37:00+00:00">05 Nov 2019, 16:37</time>
      </address>
      <p>Stellen Sie sich vor, Sie nutzen einen Sprachassistenten wie Siri oder den Google Assistant und plötzlich führt dieser Befehle aus, die Sie ihm nie gegeben haben. Befehle, die niemand im Raum ausgesprochen hat, denn Sie sind allein. Doch auf dem Gerät, das den Sprachassistenten beherbergt, fällt Ihnen ein kleiner Lichtpunkt auf.</p>
      <p>Das Szenario klingt unheimlich - und jenseits von Forschungsexperimenten sind bislang glücklicherweise keine solchen Attacken bekannt. Unrealistisch ist es aber nicht. Das zeigen Erkenntnisse eines Wissenschaftler-Teams der University of Michigan und der Tokioter University of Electro-Communications, die jetzt <a href="https://lightcommands.com/">ins Netz gestellt wurden</a>. Die fünf Wissenschaftler zeigen unter dem Projektnamen "Light Commands", wie sich Sprachassistenten mit Lichtsignalen hacken lassen, per Laser.</p>
      <p>Mit einem solchen Angriff, so skizzieren es die Forscher, könnten Sprachassistenten zum Beispiel Befehle zum Öffnen von Haus- oder Garagentüren oder für Online-Einkäufe gegeben werden - je nachdem, wie die Geräte am Einsatzort vernetzt sind. Der Nutzer könnte die eingeschleusten Befehle selbst nicht hören, sondern - sofern er denn überhaupt anwesend ist - nur die akustische Reaktion des Geräts darauf, die teils von Leuchtanzeigen ergänzt wird. Erkennen lässt sich die Attacke sonst noch an der Reflektion des eingesetzten Laserstrahls auf dem Zielgerät.</p>
      <p>
        <b>Prinzipiell sind viele Geräte gefährdet</b>
      </p>
      <p>Für die Attacke anfällig sind den Forschern zufolge zahlreiche weit verbreitete Geräte, auf denen der Google Assistant, Apples Siri bis oder Amazons Alexa läuft. Das liegt an der verbauten Technik. Der vorgestellte Angriff fokussiert sich auf sogenannte MEMS-Mikrofone, die in Smart Speakern, aber auch in Smartphones, verbaut sind und mit denen die Assistenten Sprachbefehle ihrer Nutzer erkennen können. Dabei wird der Sound in elektrische Signale umgewandelt.</p>
      <p>Diese elektrischen Signale lassen sich den Mikrofonen jedoch auch entlocken, indem man sie Laserstrahlen unterschiedlicher Intensität aussetzt, fanden die Forscher heraus. Bei ihrem Angriff, der bislang nur in Testumgebungen erprobt wurde, zielten sie mit einem Laser auf ein Mikrofon, teils auch durch ein Fenster. Die Kommandos werden so - anders als die üblichen Sprachbefehle - unhörbar übermittelt. Es sei "möglich, Mikrofone so auf Licht reagieren zu lassen, wie sie auf Geräusche reagieren", sagte Forscher Takeshi Sugawara <a href="https://www.wired.com/story/lasers-hack-amazon-echo-google-home/">dem Tech-Magazin "Wired"</a>.</p>
      <p>Praktisch hat solch ein Angriff aber Grenzen, wie auch die Wissenschaftler selbst klarstellen. Zunächst einmal muss mit dem Laser auf das Mikrofon gezielt werden, was nicht immer leicht ist. Und selbst wenn es gelingt, würde in vielen Szenarien schon ein simples Wegstellen des Geräts vom Fenster die Attacke unmöglich machen.</p>
      <p>
        <b>Man braucht das passende Equipment</b>
      </p>
      <p>Zudem braucht ein Angreifer passendes, einige Hundert Euro teures Equipment, erst recht wenn er sich nicht in unmittelbarer Nähe zum Mikrofon befindet. Die Forscher berichten, dass ihnen auf einem Flur mitunter immerhin aus 110 Meter Entfernung eine Signalübermittlung gelungen sei - eine größere Entfernung sei nicht mehr getestet worden.</p>
      <p>Auf ihrer Website stellen die Forscher Untersuchungsergebnisse zu verschiedenen Geräten vor. So wird angegeben, welche Stärke ein Laser haben muss, damit der Angriff aus 30 Zentimeter Entfernung funktioniert. Beim Google Home etwa sind dies den Angaben zufolge 0,5 Milliwatt, bei einem Amazon Echo Plus der ersten Generation 2,4 Milliwatt, bei einem Echo Spot 29 Milliwatt. Bei einem Samsung Galaxy S9 (mit integriertem Google Assistant) werden mindestens 60 Milliwatt benötigt.</p>
      <p>Mit einem solchen Laser sei beim S9 ein Angriff aus maximal fünf Meter Entfernung möglich, heißt es in einer Tabelle. Bei den meisten anderen Testgeräten habe ein Angriff aber auch mit mehr als fünfzig Meter Abstand zum Mikrofon funktioniert.</p>
      <p>
        <b>Eine Sprechererkennung schützt nicht unbedingt</b>
      </p>
      <p>Bei der Abwehr der Laserangriffe hilft es den Forschern zufolge übrigens nicht, an einem Gadget eine Sprechererkennung zu aktivieren - das Gadget also so einzustellen, dass es nur auf die Befehle einer bestimmten Stimme reagiert. Die Laserkommandos ließen sich auch bei solchen Geräten anwenden, heißt es. In der Regel beziehe sich die Sprechererkennung auch nur auf die Aufweckwörter für ein Gerät, wird weiter ausgeführt: Im Zweifel ließe sich also auch ein Befehl des echten Nutzers abwarten, bevor man eigenen Kommandos per Laser einschleust.</p>
      <p>Ein sinnvoller Schutz wäre es den Forschern zufolge beispielsweise, wenn das Gerät vor dem Ausführen eines Befehls eine zufällige Frage an den Nutzer stellt und zunächst abwartet, ob diese korrekt beantwortet wird. Das würde zumindest Angreifer, die sich nicht in Hörweite des Geräts befinden, ausbremsen.</p>
      <p>Auch seitens der Hardware könnten die Hersteller ihre Produkte besser gegen Laserattacken schützen, heißt es. Ein Ansatz, den die Forscher vorschlagen, ist der Einbau weiterer Mikrofone. Empfängt dann, wie bei der vorgestellten Attacke, nur ein einziges Mikrofon ein Signal, könnte das Gerät dies als Anomalie werten: Das Ausführen des Befehls könnte es dann verweigern.</p>
      <p>Von Google und Amazon hieß es in einer Stellungnahme für "Wired", man werde die Forschungsergebnisse prüfen. Apple wollte dem Magazin zufolge keinen Kommentar zum Thema abgeben.</p>
      <p>
        <i>mbö</i>
      </p>
    </article>
  </body>
</html>