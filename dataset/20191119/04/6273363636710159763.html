<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://venturebeat.com/2019/11/18/facebooks-deepfovea-ai-promises-power-efficient-vr-foveated-rendering/"/>
    <meta property="og:site_name" content="VentureBeat"/>
    <meta property="article:published_time" content="2019-11-19T04:00:55+00:00"/>
    <meta property="og:title" content="Facebook’s DeepFovea AI promises power-efficient VR foveated rendering"/>
    <meta property="og:description" content="A new approach to foveated rendering uses AI to imagine the pixels in your peripheral vision, cutting down on actual pixels sent over a video stream."/>
  </head>
  <body>
    <article>
      <h1>Facebook’s DeepFovea AI promises power-efficient VR foveated rendering</h1>
      <address><time datetime="2019-11-19T04:00:55+00:00">19 Nov 2019, 04:00</time> by <a rel="author" href="https://venturebeat.com/author/jeremy/" target="_blank">Jeremy Horwitz</a></address>
      <p><a href="https://venturebeat.com/2019/01/10/htc-vive-pro-eye-hands-on-gaze-into-vrs-future-with-foveated-rendering/">Foveated rendering</a> addresses a growing challenge for VR headsets, rendering sharp details for your eye’s visual sweet spot — the fovea — and a simpler, blurrier version for your peripheral vision. Now engineers at Facebook Reality Labs have come up with <a href="https://ai.facebook.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr">DeepFovea</a>, an AI-assisted alternative that creates “plausible peripheral video” rather than actually rendering accurate peripheral imagery. The new process is known as “foveated reconstruction,” and Facebook says it achieves more than 14 times compression on RGB video with no significant degradation in user-perceived quality.</p>
      <p>When capturing a video stream, DeepFovea samples only 10% of the pixels in each video frame, focusing largely but not exclusively on the area where the user’s eye is focused, represented by the lizard head above. By comparison, the peripheral area is sampled only by scattered dots that become less dense further from the eye’s focus area. The system then uses trained generative adversarial neural networks to reconstruct each frame from the tiny samples, while relying on the stream’s temporal and spatial content to fill in details in a stable rather than jittery manner.</p>
      <p>As the images above show, the heavily but not fully sampled lizard head is essentially indistinguishable from frame to frame, while the adjacent tree bark in the “reconstructed” image isn’t as sharp or detailed as the “reference” pixels. But it’s not supposed to be. A traditional foveated rendering system would depict those pixels as low-resolution flat-shaded blocks, while DeepFovea preserves — or more accurately, approximates — more of the original shapes and colors.</p>
      <p>The key reason DeepFovea matters is that it offers a superior combination of power efficiency and image quality compared with standard foveated rendering. Facebook’s claim is that the 14x reduction in rendering means that it will be able to deliver real-time, low-latency video to displays that depend upon gaze detection — a necessary step in building lightweight VR and AR headsets that will display high-resolution graphics originally rendered in the cloud. <a href="https://venturebeat.com/2019/08/22/facebook-expect-oculus-insight-tracking-in-future-wearable-ar-glasses/">All-day wearable Oculus AR headsets</a> are said to be impractical until mobile chip power consumption drops as dramatically for real-time 3D mapping as we’re seeing for streamed video.</p>
      <p>Facebook’s Michael Abrash first hinted at the concepts underlying DeepFovea last year at <a href="https://venturebeat.com/2018/09/26/oculus-chief-scientist-mike-abrash-still-sees-the-rosy-future-through-ar-vr-glasses/">Oculus Connect 5</a>, suggesting that in the future — at some point in the next five years — deep learning-based foveation and good eye tracking would come together to enable higher-resolution VR headsets such as its prototype <a href="https://venturebeat.com/2018/05/02/oculus-demos-half-dome-rift-prototype-with-140-degree-fov-and-moving-varifocal-screens/">Half Dome</a>. At Oculus Connect 6 this year, Abrash said that the company will be testing <a href="https://venturebeat.com/2019/09/25/facebook-teases-oculus-half-dome-2-and-3-prototypes/">next-generation Half Dome hardware</a> in its own offices before deploying it to the public.</p>
      <p>Rather than keeping DeepFovea solely to itself while it works on next-generation headsets, Facebook is releasing a sample version of the network architecture for researchers, VR engineers, and graphics engineers. The company is presenting the <a href="https://fb.sharepoint.com/:f:/s/Press/Er7ZUV2QFH5Cp1pw87sLKUQBHMgXjTswQzdkbC7vSfcQ4Q?e=n2Zdb9">underlying research paper</a> at Siggraph Asia tonight, and will make the samples available thereafter.</p>
    </article>
  </body>
</html>