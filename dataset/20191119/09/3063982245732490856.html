<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.golem.de/news/via-technologies-centaur-zeigt-x86-chip-mit-ai-block-1911-145071.html"/>
    <meta property="og:site_name" content="Golem.de"/>
    <meta property="article:published_time" content="2019-11-19T09:00:02+00:00"/>
    <meta property="og:title" content="Centaur zeigt x86-Chip mit AI-Block"/>
    <meta property="og:description" content="Acht CPU-Kerne und ein integrierter Coprozessor für künstliche Intelligenz: Centaur meldet sich nach Jahren mit einem x86-System-on-a-Chip zurück. Das Server-Design"/>
  </head>
  <body>
    <article>
      <h1>Centaur zeigt x86-Chip mit AI-Block</h1>
      <h2>Acht CPU-Kerne und ein integrierter <a href="https://www.golem.de/specials/cpu/">Coprozessor</a> für künstliche Intelligenz: Centaur meldet sich nach Jahren mit einem x86-System-on-a-Chip zurück. Das <a href="https://www.golem.de/specials/server/">Server</a>-Design unterstützt AVX-512 und ist sehr schnell bei Inferencing.</h2>
      <address><time datetime="2019-11-19T09:00:02+00:00">19 Nov 2019, 09:00</time> by <a rel="author">Marc Sauter</a></address>
      <p>Centaur hat erstmals seit dem <a href="https://www.golem.de/0801/57247.html">Isaiah</a> alias Nano einen neuen x86-Prozessor vorgestellt (<a href="https://centtech.com/wp-content/uploads/PRSlides_1118_Release.pdf">PDF</a>): Das CHA-Design hat acht Kerne, die mit Blick auf den Servereinsatz entwickelt wurden, und enthält einen Coprozessor für Inferencing, also das Ausführen bereits trainierter neuronaler Netze für künstliche Intelligenz.</p>
      <p>Der Chip misst laut Centaur weniger als 195 mm², er wird im 16FFC-Verfahren (FinFet Compact) beim weltgrößten Auftragsfertiger TSMC produziert. Er hat 44 PCIe-Lanes unbekannter Geschwindigkeit und bezieht Daten über vier DDR4-3200-Kanäle. Im SoC stecken acht x86-Kerne namens CNS, sie sollen eine hohe Leistung pro Takt (Instructions per Cycle, IPC) aufweisen und derzeit mit 2,5 GHz laufen. Die Cores beherrschen die AVX-512- und die Bfloat16-Befehle, teilen sich 16 MByte L3-Cache und sind per Ringbus verknüpft.</p>
      <p>Ebenfalls am Ringbus als eigener Block hängt der AI-Coprozessor, er heißt NCORE. Mit 16FCC kommt der Beschleuniger auf 34,4 mm² der unter 195 mm². Der Block soll immerhin 20 Teraops und 20 TByte/s erreichen: Hier teilt Centaur den NCORE in zwei 8er-Segmente auf, zusammen haben sie satte 16 MByte lokalen Speicher und sind extreme 32.768 Bit breit. Damit lassen sich 4.096 Berechnungen in einem Takt erledigen, was in einer sehr kurzen Latenz für Inferencing resultiert, wichtig etwa für Sprache.</p>
      <slideshow>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093008_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093009_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093010_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093011_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093012_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093013_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093015_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093016_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
        <figure>
          <img src="https://scr3.golem.de/screenshots/1911/Centaur-CHA-NCORE-CNS/thumb620/2019-11-19_093017_cr.png"/>
          <figcaption>Präsentation zum CHA-Design (Bild: Centaur)</figcaption>
        </figure>
      </slideshow>
      <p>Mit einem frühen Software-Stack, der kaum einen Monat als ist, hat Centaur mit dem CHA-Design den <a href="https://mlperf.org/inference-results/">MLPerf</a>-Benchmark durchgeführt: Ein Bild wird in weniger als 330 ms klassifiziert (MobileNet-v1 Stream), was einen Rekord darstellt - der Durchsatz entspricht grob 23 aktuellen <a href="https://www.golem.de/news/cascade-lake-ap-sp-das-koennen-intels-xeon-cpus-mit-56-kernen-1904-140382.html">Intel-CLX-SP</a>-Kernen (MobileNet-v1 Offline). Die meisten anderen Systeme benötigen dafür eine Host-CPU, an die per PCIe externe Beschleuniger wie Intels <a href="https://www.golem.de/news/nnp-t-und-nnp-i-intel-hat-den-t-1000-der-kuenstlichen-intelligenz-1911-144987.html">NNP-T1000</a> oder Googles <a href="https://www.golem.de/news/cloud-tpu-googles-neue-deep-learning-hardware-braucht-fluessigkuehlung-1805-134293.html">TPU v3</a> angeschlossen sind. Wann der CHA erscheinen soll, sagte Centaur nicht - ein System gibt es auf der ISC East 2019 in New York City zu sehen.</p>
    </article>
  </body>
</html>