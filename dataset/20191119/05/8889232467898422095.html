<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.eurekalert.org/pub_releases/2019-11/cmu-tth111819.php"/>
    <meta property="og:site_name" content="EurekAlert!"/>
    <meta property="article:published_time" content="2019-11-19T05:00:00+00:00"/>
    <meta property="og:title" content="Trash talk hurts, even when it comes from a robot"/>
    <meta property="og:description" content="Trash talking has a long and colorful history of flustering game opponents, and now researchers at Carnegie Mellon University have demonstrated that discouraging words can be perturbing even when uttered by a robot."/>
  </head>
  <body>
    <article>
      <h1>Trash talk hurts, even when it comes from a robot</h1>
      <h2>Discouraging words from machines impair human game play</h2>
      <address><time datetime="2019-11-19T05:00:00+00:00">19 Nov 2019, 05:00</time> by <a rel="author">Carnegie Mellon University</a></address>
      <p>PITTSBURGH--Trash talking has a long and colorful history of flustering game opponents, and now researchers at Carnegie Mellon University have demonstrated that discouraging words can be perturbing even when uttered by a robot.</p>
      <p>The trash talk in the study was decidedly mild, with utterances such as "I have to say you are a terrible player," and "Over the course of the game your playing has become confused." Even so, people who played a game with the robot -- a commercially available humanoid robot known as Pepper -- performed worse when the robot discouraged them and better when the robot encouraged them.</p>
      <p>Lead author Aaron M. Roth said some of the 40 study participants were technically sophisticated and fully understood that a machine was the source of their discomfort.</p>
      <p>"One participant said, 'I don't like what the robot is saying, but that's the way it was programmed so I can't blame it,'" said Roth, who conducted the study while he was a master's student in the CMU Robotics Institute.</p>
      <p>But the researchers found that, overall, human performance ebbed regardless of technical sophistication.</p>
      <p>The study, presented last month at the IEEE International Conference on Robot &amp; Human Interactive Communication (RO-MAN) in New Delhi, India, is a departure from typical human-robot interaction studies, which tend to focus on how humans and robots can best work together.</p>
      <p>"This is one of the first studies of human-robot interaction in an environment where they are not cooperating," said co-author Fei Fang, an assistant professor in the Institute for Software Research. It has enormous implications for a world where the number of robots and internet of things (IoT) devices with artificial intelligence capabilities is expected to grow exponentially. "We can expect home assistants to be cooperative," she said, "but in situations such as online shopping, they may not have the same goals as we do."</p>
      <p>The study was an outgrowth of a student project in AI Methods for Social Good, a course that Fang teaches. The students wanted to explore the uses of game theory and bounded rationality in the context of robots, so they designed a study in which humans would compete against a robot in a game called "Guards and Treasures." A so-called Stackelberg game, researchers use it to study rationality. This is a typical game used to study defender-attacker interaction in research on security games, an area in which Fang has done extensive work.</p>
      <p>Each participant played the game 35 times with the robot, while either soaking in encouraging words from the robot or getting their ears singed with dismissive remarks. Although the human players' rationality improved as the number of games played increased, those who were criticized by the robot didn't score as well as those who were praised.</p>
      <p>It's well established that an individual's performance is affected by what other people say, but the study shows that humans also respond to what machines say, said Afsaneh Doryab, a systems scientist at CMU's Human-Computer Interaction Institute (HCII) during the study and now an assistant professor in Engineering Systems and Environment at the University of Virginia. This machine's ability to prompt responses could have implications for automated learning, mental health treatment and even the use of robots as companions, she said.</p>
      <p>Future work might focus on nonverbal expression between robot and humans, said Roth, now a Ph.D. student at the University of Maryland. Fang suggests that more needs to be learned about how different types of machines -- say, a humanoid robot as compared to a computer box -- might invoke different responses in humans.</p>
      <hr/>
      <p>In addition to Roth, Fang and Doryab, the research team included Manuela Veloso, professor of computer science; Samantha Reig, a Ph.D. student in the HCII; Umang Bhatt, who recently completed a joint bachelor's-master's degree program in electrical and computer engineering; Jonathan Shulgach, a master's student in biomedical engineering; and Tamara Amin, who recently finished her master's degree in civil and environmental engineering.</p>
      <p>The National Science Foundation provided some support for this work.</p>
    </article>
  </body>
</html>