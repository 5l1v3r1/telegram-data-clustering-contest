<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://overclockers.ru/itnews/show/100340/nvidia-predstavlyaet-po-magnum-io-dlya-optimizacii-uzkih-mest-pri-rabote-s-dannymi-v-prilozheniyah-ii-i-hpc"/>
    <meta property="og:site_name" content="Overclockers.ru"/>
    <meta property="article:published_time" content="2019-11-19T06:10:00+00:00"/>
    <meta property="og:title" content="NVIDIA представляет ПО Magnum IO для оптимизации узких мест при работе с данными в приложениях ИИ и HPC"/>
    <meta property="og:description" content="Новое решение перемещает данные до 20 раз быстрее"/>
  </head>
  <body>
    <article>
      <h1>NVIDIA представляет ПО Magnum IO для оптимизации узких мест при работе с данными в приложениях ИИ и HPC</h1>
      <address><time datetime="2019-11-19T06:10:00+00:00">19 Nov 2019, 06:10</time> by <a rel="author" href="https://overclockers.ru/author/show/admin" target="_blank">admin</a></address>
      <p>ДЕНВЕР—SC19 — 18 ноября 2019—NVIDIA сегодня представила набор программного обеспечения <a href="https://www.nvidia.com/en-us/data-center/magnum-io/">NVIDIA Magnum IO</a>, позволяющее исследователям в области ИИ и HPC обрабатывать большие объемы данных за считанные минуты вместо нескольких часов. <br/><br/>ПО Magnum IO устраняет узкие места при хранении и передаче данных, ускоряя до 20 раз обработку массивов данных в многосерверных мульти-GPU вычислительных узлах и позволяя быстро выполнять финансовый анализ, моделирование климата и другие HPC-задачи.</p>
      <figure>
        <img src="https://st.overclockers.ru/images/soft/2019/10/30/Magnum-IO.jpg"/>
      </figure>
      <p>NVIDIA разработала Magnum IO в сотрудничестве с лидерами индустрии в сегменте передачи и хранения данных, включая DataDirect Networks, Excelero, IBM, Mellanox и WekaIO. <br/><br/>“В основе всего того, что связано с ИИ, находится обработка больших объемов собранных или смоделированных данных, - говорит Дженсен Хуанг (Jensen Huang), учредитель и генеральный директор NVIDIA. - По мере экспоненциального увеличения объемов и скорости поступления данных их обработка становится одной из самых важных, но и крайне затратных задач для ЦОД.” <br/><br/>“Для экстремальных вычислений нужны экстремально быстрые интерфейсы. Именно это и обеспечивает ПО Magnum IO, применяя GPU-ускорение, кардинально изменившее вычисления, к передаче и хранению данных. Исследователям больше не придется долго ожидать окончания обработки данных. Теперь они смогут сконцентрироваться на сути своей работы”, - добавил Дженсен. <br/><br/>В основе ПО Magnum IO лежит технология GPUDirect, позволяющая данным обходить CPU и перемещаться по магистралям, созданным графическими процессорами, накопителями и сетевыми устройствами. GPUDirect совместима с широким спектром интерфейсов и API, включая NVIDIA NVLink™ и NCCL, а также OpenMPI и UCX, и состоит из одноранговых (peer-to-peer) и RDMA элементов. <br/><br/>Новейшим элементом является GPUDirect Storage, позволяющий исследователям в обход CPU получать доступ к хранимым файлам для моделирования, анализа и визуализации. <br/><br/>ПО NVIDIA Magnum IO уже доступно, за исключением GPUDirect Storage, к которому пока только открыт ранний доступ. Широкая доступность GPUDirect Storage запланирована на первое полугодие 2020 года.</p>
    </article>
  </body>
</html>