<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.nature.com/articles/d41586-019-03454-y"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="article:published_time" content="2019-11-13T00:00:00+00:00"/>
    <meta property="og:title" content="Star Wars-style 3D images created from single speck of foam"/>
    <meta property="og:description" content="Ultrasonic speakers steer tiny bead to generate displays that you can touch and hear."/>
  </head>
  <body>
    <article>
      <h1>Star Wars-style 3D images created from single speck of foam</h1>
      <h2>Ultrasonic speakers steer tiny bead to generate displays that you can touch and hear.</h2>
      <address><time datetime="2019-11-13T00:00:00+00:00">13 Nov 2019</time> by <a rel="author">Elizabeth Gibney</a></address>
      <figure>
        <iframe src="https://www.youtube.com/embed/jOnxr9Ez_Kc" width="640" height="360" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>With the tap of a keyboard, Ryuji Hirayama brings a listless foam bead to life. The white speck jumps up and hovers with perfect stillness in space. Another tap, and the dot transforms into a luminous butterfly-like shape, which flaps its wings as it circles inside a black box. Diego Martinez Plasencia, Hirayama’s colleague at the University of Sussex in Brighton, UK, reaches into the box to show that there are no strings. The effect seems to be pure magic. “I showed it first to my daughters. They were like — ‘wow’,” Martinez Plasencia says, his eyes widening in child-like delight.</p>
      <p>Behind the mid-air metamorphosis is a relatively simple set-up. Two slender arrays of 256 tiny speakers above and below the bead move it by generating ultrasound waves. The object darts so fast that all the eye sees is a continuously evolving 3D image a few centimetres across, drawn in the air as if by a high-speed Etch a Sketch. The same ultrasound speakers that create the image can also generate audio and tactile sensations. Reach towards the butterfly, and your finger might feel a flutter. In another case, a smiley face appears, accompanied by the strains of Queen’s ‘We Will Rock You’. Remarkably, most of the components used to create these effects are off-the-shelf.</p>
      <figure>
        <img src="https://media.nature.com/w800/magazine-assets/d41586-019-03454-y/d41586-019-03454-y_17355906.jpg"/>
        <figcaption>A team at the University of Sussex, UK, has created a virtual butterfly that can hover in space.<br/>Credit: Eimontas Jankauskis/Univ. Sussex</figcaption>
      </figure>
      <p>“It’s an elegant and exciting platform,” says Daniel Smalley, a physicist at Brigham Young University in Provo, Utah, who last year <a href="https://www.nature.com/articles/d41586-018-01125-y">unveiled a similar technique</a>, using lasers to steer around a fleck of cellulose to produce images [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR1">1</a></sup>]. Until now, few physicists thought it would be possible to use sound to move a bead fast enough to create such a display, he says. In August, Tatsuki Fushimi, a physicist at the University of Bristol, UK, and his collaborators became the first to show that it was feasible. But their bead took longer to trace out shapes, meaning that only images smaller than 1 centimetre across could appear as a single, continuous object [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR2">2</a></sup>]. The Sussex team’s work is “a piece of engineering that makes us believe things we didn’t think were possible,” says Smalley.</p>
      <p>The acoustic device, described in <i>Nature</i> [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR3">3</a></sup>] on 13 November, is the latest example of a 3D-image-generation technology known as volumetric display, which differs in fundamental ways from technologies such as holograms, virtual reality and stereoscopes. Those more-familiar approaches use tricks of the light to create the illusion of depth, and can be life-sized and photorealistic. But holograms can be seen only from certain angles, virtual reality and stereoscopes require headgear, and all these techniques can cause eye strain. Free-space volumetric displays, by contrast, use lasers, electric fields, fog projections and other approaches to create truly 3D images that viewers can see from any vantage point. In that way, they’re the closest any display technology has come to Princess Leia’s SOS message in the 1977 film <i>Star Wars</i>.</p>
      <p>Research into volumetric displays is even older than that film. And the approach has a crucial advantage over holograms, because it requires much less computational power. But despite decades of effort, free-space displays are still limited to small, crude drawings, and they have struggled to get off the ground commercially, says Smalley. Still, he is hopeful that work bringing together different and more-practical technologies, including acoustic levitation, will help volumetric display to find its killer app. It might be used in detailed interactive mock ups for medical trainees, perhaps, or to give people the ability to chat with distant relatives rendered in 3D. And the Sussex team’s acoustic method wouldn’t necessarily require a long phase of development to make its way out of the laboratory, says Smalley. “I would make a bet on this technology becoming commercial before many of the other technologies we work on.”</p>
      <h3>Lights and sound</h3>
      <p>Volumetric displays that are already on the market usually work by making 2D images 3D. The Voxon VX1, for example, projects photons onto a screen that vibrates rapidly up and down. Timed right, this creates a 3D image without the need for special eyewear. But the display’s complex mechanical parts mean it is locked behind glass and has so far found only niche uses, such as in museum displays.</p>
      <figure>
        <img src="https://media.nature.com/w800/magazine-assets/d41586-019-03454-y/d41586-019-03454-y_17355914.jpg"/>
        <figcaption>The Voxon VX1 system.<br/>Credit: Voxon Photonics</figcaption>
      </figure>
      <p>In 2006, Hidei Kimura made one of the first attempts to draw images directly onto 3D space [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR4">4</a></sup>]. Kimura, chief executive of Burton, a firm based in Kawasaki City, Japan, and his academic collaborators developed a technique that used a laser to knock electrons off air molecules, causing them to glow. By moving the laser’s focal point at high speed, they could create luminescent dots of plasma that build to make a coarse image. “With nothing, we can create 3D images directly in the air,” says Kimura, who envisages using the technique to broadcast emergency information into the sky, or to project 3D replays above the pitch at a sporting event.</p>
      <p>The plasma technique creates relatively stable images, but it faces some major limitations: it is low resolution (one laser blast equals one point in the image) and the laser is so intense that it could cause burns, says Yoichi Ochiai, a computer scientist and artist at the University of Tsukuba in Japan.</p>
      <p>In 2016, Ochiai’s team adapted the plasma technique, using a lower-energy, shorter-pulse laser to make images that are safe to touch [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR5">5</a></sup>]. At a few millimetres across, the images are much smaller than the Kimura team’s. But by using lasers that pulse at a higher frequency and modulators to shape the beam into multiple focuses, the team can ramp up the resolution to 10–200 times higher than that used in Kimura’s work. This allows them to create more complex images, such as pinhead-sized fairies.</p>
      <p>At Sussex, the acoustic 3D display started with another stalwart of science fiction: the tractor beam made famous in the 1960s television series <i>Star Trek</i>. Since 2012, Sriram Subramanian, who leads the team, has pioneered ways of crafting sound waves to create points of high pressure that can trap and move small objects [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR6">6</a></sup>]. But it was only when Hirayama came to the laboratory in 2018 that the team found a way to use sound to create images.</p>
      <figure>
        <video src="https://media.nature.com/w800/magazine-assets/d41586-019-03454-y/d41586-019-03454-y_17355910.gif" autoplay="" loop=""/>
        <figcaption>A globe in a volumetric display.This was shot using exposure times of 0.025–20 seconds. (Only images drawn within 0.1 seconds appear as continuous images to the human eye.)<br/>Credit: Eimontas Jankauskis/Univ. Sussex</figcaption>
      </figure>
      <p>To appear as a solid picture, a bead needs to create each image frame in less than one-tenth of a second. Until now, acoustic levitation has tended to focus on holding objects as steady as possible; motion happens relatively slowly in a stop–start fashion, from one stable point to another. Hirayama’s innovation was to kick the bead before it comes to rest, calculating each new target point inside hardware designed especially for the computation. This meant that the team could change the field’s focus 40,000 times per second. The bead reaches speeds as high as 8.75 metres per second, which “looks like teleportation” when a 2-millimetre-wide bead crosses a few centimetres of space, says Hirayama. As the bead moves, a rapidly changing LED bathes the display in light to create colour.</p>
      <figure>
        <img src="https://media.nature.com/w800/magazine-assets/d41586-019-03454-y/d41586-019-03454-y_17355908.jpg"/>
        <figcaption>Volumetric display of a butterfly created in Daniel Smalley’s lab.<br/>Credit: Nate Edwards/BYU Photo</figcaption>
      </figure>
      <p>The team was inspired by Smalley’s work using lasers to move and illuminate a speck of cellulose plant fibre [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR1">1</a></sup>]. Using the same number of particles and data, Smalley says his images are only one-tenth the size of the Sussex team’s, but have ten times the resolution.</p>
      <p>The Sussex technique does have a drawback: it needs speakers on two sides of the display, which restricts a viewer’s ability to interact with the display and limits its size. But with hardware upgrades, Subramanian says it could be possible to use a different kind of acoustic wave to create images with speakers on just one side. The researchers are also working to improve their understanding of how the bead responds to the forces acting on it, which would allow them to move it faster, to draw more-complex images by levitating multiple beads at once and to integrate sight and touch more closely. In the current set-up, the tactile sensation and image don’t occur in exactly the same place, because the fields needed to create them can interfere with each other. Ochiai’s group has already found a way to bring together touch and sight by using fields that do not interfere: an acoustic field for tactile feedback and a laser to draw tiny images in plasma. The group has used the approach to draw braille dots in the air [<sup><a href="https://www.nature.com/articles/d41586-019-03454-y#ref-CR7">7</a></sup>].</p>
      <h3>Interactive advantage</h3>
      <p>Inevitably, any 3D display gets compared to <i>Star Wars </i>holograms. Sussex’s technique makes bigger images than previous similar methods and incorporates sound, so it brings us closer to recreating that, says Qiong-Hua Wang at Beihang University in Beijing, who works on 3D display devices. But the images are still tiny and far from photorealistic. Creating the kind of 3D effect in <i>Star Wars</i> by any means could take ten years, or even longer, she says.</p>
      <p>But Barry Blundell, a physicist specializing in 3D technologies at the University of Derby, UK, cautions against trying to use volumetric technology to make rich, photorealistic displays. “No one would look at a sculpture and compare it to a painting,” he says. He adds that efforts to compete with holograms have often led to commercial dead-ends, and that the displays are best suited to applications that would be impossible in other media but don’t require high detail, such as interactive displays capable of showing complex 3D movements.</p>
      <p>Interactivity could be powerful, Smalley says. Surgeons in training might use such displays to practise threading a catheter through the vessels of the heart, for instance. With one million moving particles, he adds, “you can have a disembodied face — do face-to-face telepresence”. Creating avatars of people in a space could give a stronger sense of presence than a photorealistic image seen through virtual reality, he says.</p>
      <p>At the Sussex lab, a million-particle display seems a long way off. Only time will tell if the group’s approach will pave the way to such numbers. After showing off his sphere’s limited repertoire of tricks, Hirayama shuts off power to the speakers. The flapping butterfly vanishes, and the bead that created it drops and bounces gently on the display’s base. Hirayama picks it up and sets it in a box with hundreds of others, ready at any time to create magic in thin air.</p>
      <p>Nature <b>575</b>, 272-273 (2019)</p>
      <p>doi: 10.1038/d41586-019-03454-y</p>
      <h3>References</h3>
      <p>1. <anchor name="ref-CR1"/>Smalley, P. E. <i>et al.</i> <i>Nature</i> <b>553</b>, 486–490 (2018).</p>
      <p>2. <anchor name="ref-CR2"/>Fushimi, T., Marzo, A., Drinkwater, B. W. &amp; Hill, T. L. <i>Appl. Phys. Lett.</i> <b>115</b>, 064601 (2019).</p>
      <p>3. <anchor name="ref-CR3"/>Hirayama, R., Martinez Plasencia, D., Masuda, N. &amp; Subramanian, S. <i>Nature</i> <b>575</b>, 320–323 (2019).</p>
      <p>4. <anchor name="ref-CR4"/>Kimura, H., Uchiyama, T. &amp; Yoshikawa, H. Proc. SIGGRAPH ‘06 <a href="https://doi.org/10.1145/1179133.1179154">https://doi.org/10.1145/1179133.1179154</a> (2006).</p>
      <p>5. <anchor name="ref-CR5"/>Ochiai, Y. <i>et al.</i> <i>ACM Trans. Graphics</i> <b>35</b>, 17 (2016).</p>
      <p>6. <anchor name="ref-CR6"/>Marzo, A. <i>et al.</i> <i>Nature Commun.</i> <b>6</b>, 8661 (2015).</p>
      <p>7. <anchor name="ref-CR7"/>Ochiai, Y., Hoshi, T., Hasegawa, S. &amp; Hayasaki, Y. <i>Proc. 2016 CHI Conf. Human Factors Comput. Syst.</i> 3238–3427 (2016).</p>
      <related>
        <a href="http://www.nature.com/articles/s41586-019-1739-5"/>
        <a href="http://www.nature.com/articles/s41586-019-1713-2"/>
        <a href="http://www.nature.com/articles/d41586-019-03143-w"/>
        <a href="http://www.nature.com/articles/d41586-019-03408-4"/>
        <a href="http://www.nature.com/articles/s41586-019-1736-8"/>
        <a href="http://www.nature.com/articles/d41586-019-03455-x"/>
        <a href="http://www.nature.com/articles/d41586-019-03550-z"/>
        <a href="http://www.nature.com/articles/d41586-019-03366-x"/>
        <a href="http://www.nature.com/articles/d41586-019-03049-7"/>
      </related>
    </article>
  </body>
</html>