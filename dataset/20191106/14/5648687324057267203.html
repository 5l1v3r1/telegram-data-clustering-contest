<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://www.fr.de/wissen/alexa-siri-google-assistant-sicherheitsluecke-smart-speaker-zr-13198042.html"/>
    <meta property="og:site_name" content="Frankfurter Rundschau"/>
    <meta property="article:published_time" content="2019-11-06T14:49:00+00:00"/>
    <meta property="og:title" content="Schwachstelle in Smart Speakern: Forschende steuern Alexa, Siri und Google Assistant mit Licht"/>
    <meta property="og:description" content="Smart Speaker sind beliebt - doch nun haben Forschende in Alexa, Siri und Google Assistant eine Sicherheitslücke entdeckt, die Besitzer von Smart Speakern aufhorchen lassen sollte."/>
  </head>
  <body>
    <article>
      <h6 class="kicker">Smart Speaker</h6>
      <h1>Schwachstelle in Smart Speakern: Forschende steuern Alexa, Siri und Google Assistant mit Licht</h1>
      <address><time datetime="2019-11-06T14:49:00+00:00">06 Nov 2019, 14:49</time> by <a rel="author">Tanja Banner</a></address>
      <p>
        <b>Smart Speaker sind beliebt - doch nun haben Forschende in Alexa, Siri und Google Assistant eine Sicherheitslücke entdeckt, die Besitzer von Smart Speakern aufhorchen lassen sollte.</b>
      </p>
      <p><b>Sprach-Assistenten wie Siri, Alexa oder Google Assistant sind beliebt</b>. Der Nutzer spricht ein Kommando - und das Gerät führt es aus. Das reicht von einfachen Aufgaben wie dem Abspielen eines Lieds oder dem Programmieren der Erinnerungsfunktion bis hin zum Steuern externer Geräte wie Lampen, Türschlösser oder Garagentore.</p>
      <p>Vor diesem Hintergrund ist eine Schwachstelle, die Forschende in Mikrofonen entdeckt haben, besonders heikel: Ihnen ist es gelungen, verschiedene Sprachassistenten zu hacken - und zwar nicht per Sprachkommandos, sondern <b>mit Lichtsignalen</b>. „Light Commands“ haben Forschende der University of Michigan und der University of Electro Communications in Tokio die Schwachstelle genannt, die sie gefunden haben.</p>
      <h3>Alexa, Siri und Google Assistant reagieren auf Lichtsignale</h3>
      <p>Im Versuch ist es den Forschenden gelungen, Siri, <a href="https://www.fr.de/kultur/datensammler-schritt-tritt-10964749.html">Google Assistant</a>* und Alexa mit Hilfe eines Laserstrahls auszutricksen. Das gelang in unterschiedlichen Szenarien: aus nächster Nähe, über eine Distanz von 110 Metern und sogar über eine lange Distanz und durch ein geschlossenes Fenster.</p>
      <p>„Es ist möglich, Mikrofone dazu zu bringen, auf Licht so zu reagieren, als ob es Klang wäre“, erklärt der Forscher Takeshi Sugawara <a href="https://www.wired.com/story/lasers-hack-amazon-echo-google-home/">gegenüber dem Magazin „Wired“</a>. „Das bedeutet, dass jedes Gerät, das auf Klang reagiert, auch auf Lichtkommandos reagiert.“</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/EtzP-mCwNAs" width="560" height="315" data-service="Youtube" scrolling="no"/>
      </figure>
      <figure>
        <iframe src="https://www.youtube.com/embed/EtzP-mCwNAs" width="560" height="315" data-service="Youtube" scrolling="no"/>
      </figure>
      <h3>Wie funktioniert der Hacker-Angriff auf Alexa, Siri und Google Assistant?</h3>
      <p>Die MEMS-Mikrofone, die in Smart Speakern und Smartphones verbaut sind, verwandeln Klang in elektrische Signale. Allerdings lässt sich dieser Mechanismus durch Licht austricksen: Der Laserstrahl bringt ein Bauteil des Mikrofons zum Schwingen, wodurch die elektrischen Signale entstehen. So schicken die Forschenden unhörbare Signale an die Smart Speaker, die darauf reagieren.</p>
      <h3>Welche Gefahr besteht für Besitzer von Alexa, Siri und Google Assistant?</h3>
      <p>Mit Hilfe von „Light Commands“ könnten beispielsweise Garagentore geöffnet werden, die mit dem Smart Speaker verbunden sind, warnen die Forschenden. Genau dieses Szenario hatten sie in ihren Tests ausprobiert. Aber auch nicht autorisierte Online-Einkäufe, das Öffnen von Haustüren oder das Entriegeln und Starten von Fahrzeugen seien so möglich.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/ORji7Tz5GiI" width="560" height="315" data-service="Youtube" scrolling="no"/>
      </figure>
      <h3>Wie kann man den Angriff auf Alexa, Siri und Google Assistant verhindern?</h3>
      <p>Um über längere Strecken die richtige Stelle von Alexa, Siri oder Google Assistant mit dem Laser zu treffen, benötigten die Forschenden unter anderem ein Teleobjektiv oder ein Teleskop - der Angriff ist also nicht ganz günstig. Außerdem kann man ihn verhältnismäßig einfach verhindern: Der Smart Speaker darf einfach nicht von außen einsehbar sein - sollte also beispielsweise nicht am Fenster stehen.</p>
      <p>Die Forschenden schlagen zudem Mechanismen vor, die den Zugriff aus der Ferne verhindern sollen: So könnte der Smart Speaker vor dem Ausführen eines kritischen Befehls eine Frage stellen und den Befehl erst nach erhaltener Antwort ausführen. Auch möglich sei es, verschiedene Mikrofone zu nutzen: Der Smart Speaker reagiert dann nur, wenn er das Kommando über mehrere Mikrofone empfangen hat.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/iK2PtdQs77c" width="560" height="315" data-service="Youtube" scrolling="no"/>
      </figure>
      <h3>Smart Speaker: Beliebt und umstritten</h3>
      <p>Smart Speaker sind beliebt und weit verbreitet: Es gibt Statistiken, wonach jeder vierte US-Amerikaner einen smarten Lautsprecher besitzt. Amazon hat nach eigenen Angaben weltweit fast 120 Millionen Geräte mit Alexa-Unterstützung verkauft - Stand Anfang 2019. Mittlerweile dürften es deutlich mehr sein.</p>
      <p>Aber Siri, Alexa und Co. sind auch umstritten: <a href="https://www.fr.de/kultur/smart-home-lautsprecher-soll-mord-aufklaeren-11073529.html">Sie hören jederzeit Gespräche mit</a>*, um bei Bedarf reagieren zu können, kritisieren Datenschützer. Für Empörung sorgten vor einiger Zeit die Enthüllungen, dass Amazon teilweise Mitarbeiter die aufgezeichneten Befehle an Alexa anhören und abtippen lässt - um die Spracherkennung zu verbessern. Auch bei Apples Siri wurde ein entsprechendes Vorgehen bekannt. Mittlerweile haben beide Unternehmen Fehler zugegeben.</p>
      <p>Von Tanja Banner</p>
      <p>
        <i>*<a href="https://www.fr.de/">fr.de</a> ist Teil der bundesweiten Ippen-Digital-Zentralredaktion.</i>
      </p>
    </article>
  </body>
</html>