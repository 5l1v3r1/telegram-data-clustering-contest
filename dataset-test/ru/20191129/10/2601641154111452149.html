<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://vc.ru/future/94909-poteryavshiy-sluh-sovetskiy-matematik-ustroilsya-v-google-chtoby-pomoch-drugim-lyudyam-s-narusheniyami-sluha-i-rechi"/>
    <meta property="og:site_name" content="vc.ru"/>
    <meta property="article:published_time" content="2019-11-29T10:25:57+00:00"/>
    <meta property="og:title" content="Потерявший слух советский математик устроился в Google, чтобы помочь другим людям с нарушениями слуха и речи"/>
    <meta property="og:description" content="Дмитрий Каневский разрабатывает продукты, которые помогают людям общаться с близкими, коллегами, мобильными устройствами и целым миром."/>
  </head>
  <body>
    <article>
      <h1>Потерявший слух советский математик устроился в Google, чтобы помочь другим людям с нарушениями слуха и речи</h1>
      <address><time datetime="2019-11-29T10:25:57+00:00">29 Nov 2019, 10:25</time> by <a rel="author">Дарья Дейнека</a></address>
      <p>Дмитрий Каневский разрабатывает продукты, которые помогают людям общаться с близкими, коллегами, мобильными устройствами и целым миром.</p>
      <figure>
        <img src="https://leonardo.osnova.io/029198b4-8e7c-c97d-d972-3a78a21fad00/"/>
        <figcaption>​Дмитрий Каневский на мероприятии AI in Action</figcaption>
      </figure>
      <p>Тем не менее он научился читать по губам, закончил МГУ, стал кандидатом наук, переехал в США и сейчас работает исследователем в Google.</p>
      <p>Последние 40 лет он разрабатывает устройства и технологии, которые помогают людям с нарушениями слуха. Например, прибор, помогающий «слышать» с помощью кожи, и приложение, которое переводит в текст речь людей с сильным акцентом, заиканием и другими особенностями речи.</p>
      <p>Изобретатель рассказал, как создал свой аппарат для чтения с губ, устроился в Google и помог разработать алгоритм для автоматического создания титров на YouTube.</p>
      <h3>Конструктор</h3>
      <p>В детстве я потерял слух. Но меня научили читать по губам, и я пошёл в обычную школу.</p>
      <p>У меня было много друзей. Тогда я не испытывал больших сложностей в общении. Трудно стало, когда в восьмом классе я перешёл во вторую математическую школу в Москве. Там были другие ребята и сложные технологические предметы — учиться приходилось в основном по учебникам.</p>
      <p>Тем не менее после школы я поступил в МГУ — в 1969 году, а потом ещё восемь лет учился математике и стал кандидатом наук, написав диссертацию по алгебраической геометрии.</p>
      <blockquote>Думаю, математика делала меня более независимым. В ней ты один на один с проблемой. Ты можешь сфокусироваться на ней, бороться с ней. Это соответствует моему характеру.</blockquote>
      <p>Заканчивая диссертацию, я встретил будущую жену. Она переезжала со своими родителями в Израиль, и я решил отправиться вслед за ней.</p>
      <p>Я знал, что в новой стране не буду так же хорошо читать по губам, как в СССР, и не смогу свободно общаться с людьми. Тогда я разработал аппарат, который помогал читать с губ.</p>
      <p>Прибор крепился на теле и позволял «слышать» кожей — улавливал звуки и переводил их в вибрации. Проблема была в том, что некоторые звуки, например «с», «ш», «и», «а», находятся на высоких частотах, поэтому их сложно почувствовать кожей. Тогда я придумал переводить высокие частоты в низкие.</p>
      <p>Мне удалось сделать настолько маленький аппарат, что его под одеждой не замечали другие люди.</p>
      <p>Я получил разрешение вывезти устройство в Израиль, и оно помогало мне говорить на иврите, в котором большое количество слов с «высокочастотными» звуками вроде «шабат», «шалом» и так далее.</p>
      <p>В Израиле я показал аппарат одному доктору. Он сказал, что это великолепная вещь и нужно открыть компанию по продаже устройства.</p>
      <p>Мы назвали её SensorAid, и параллельно с этим стартапом я ещё работал математиком в институте Вейцмана. В месяц я зарабатывал 2000 шекелей, это было в 1981 году.</p>
      <p>Аппарат потом применяли во многих странах — он был универсален для всего мира. В одной больнице его сравнили с разработкой компании Cohler, которая вживляла передатчик в ухо человека, чтобы он мог считывать звуки.</p>
      <p>Мой аппарат показал такой же результат, как у Cohler, но их разработка стоила $25 тысяч и требовала серьёзной операции, а мой вариант был в несколько раз дешевле и не требовал вмешательства хирургов.</p>
      <p>В 1984 году американская компания Spectro выкупила авторские права на аппарат (<i>сумму сделки Каневский назвать отказался — vc.ru</i>). Сперва я отправился работать в академические институты в Германии и США, а после перешёл в IBM.</p>
      <h3>Работа в IBM</h3>
      <p>В начале работы я разработал алгоритм для распознавания речи.</p>
      <p>Чтобы перевести речь в текст, системе требовалось считать акустический сигнал и сопоставить его со словом, которое он представляет.</p>
      <p>Для этого звук представляется как последовательность чисел, которая сравнивается с каждым словом в словаре, используя некоторый критерий. Произнесённым считается слово, которое лучше всего согласовано с этой последовательностью чисел. Критерии — многочлены, которые состояли из 50 млн переменных, или параметров.</p>
      <p>В 1990-е годы вычислять многочлены с 50 млн параметров за линейное время (<i>временная сложность алгоритма, которая зависит от числа операций — vc.ru</i>) позволяли методы динамического программирования.</p>
      <p>Более совершенные критерии были основаны не на многочленах, а на рациональных функциях — отношениях многочленов. Для них долгое время не могли найти способ вычисления значений для 50 млн параметров в линейное время. А я нашёл этот метод. И когда его стали применять, точность распознавания речи значительно улучшилась.</p>
      <figure>
        <img src="https://leonardo.osnova.io/23af7abb-d4a4-56e0-06e4-bd8d45dcaf3c/"/>
        <figcaption>Дмитрий Каневский на мероприятии AI in Action</figcaption>
      </figure>
      <p>Вместе с этим я постоянно работал над технологиями, которые помогали бы людям с нарушениями слуха. В то время появился интернет и с его помощью я создал первые в мире сервисы, которые помогали понимать речь.</p>
      <p>Например, сервис, который позволял переводить устную речь в письменную. Для этого клиент звонил людям, умеющим быстро печатать, включал громкую связь, и они набирали текст, который слышали во время звонка.</p>
      <p>Текст в реальном времени высвечивался на экране компьютера клиента, и тот понимал, о чём говорят рядом с ним. Такая услуга стоила до $120–150 в час.</p>
      <p>Также я занимался изобретательством, не связанным с распознаванием речи.</p>
      <p>Одна из таких технологий — Artificial Passenger (<i>искусственный пассажир — vc.ru</i>). Она помогала водителям не уснуть за рулём. Система наблюдала за человеком, разговаривала с ним, поэтому водитель, отвечая на вопросы, не засыпал.</p>
      <p>Другая разработка касалась безопасности в банках. Чтобы подтвердить личность клиента, консультанты обычно просили назвать его имя матери или жены.</p>
      <p>Я разработал систему, которая позволяла банку собирать больше информации о клиенте, чтобы сотрудники каждый раз могли задать новый вопрос. Например: «Как зовут вашу собаку?» или «Когда вы вернулись из отпуска?»</p>
      <p>В это же время технология идентифицировала голос звонящего и проверяла, действительно ли он принадлежит клиенту банка. Если всё было в порядке и человек давал правильный ответ на вопрос, сотрудник банка понимал, что звонит не мошенник.</p>
      <h3>Распознавание речи для YouTube</h3>
      <p>В 2014 году я перешёл в Google, где продолжил работать над распознаванием речи.</p>
      <p>Я занялся системой Closed Caption для YouTube, которая автоматически распознаёт речь на видео и переводит её в субтитры. В то время технология работала плохо, и мы с командой должны были улучшить её алгоритм.</p>
      <p>Для создания акустических моделей слов (<i>математических функций, основанных на фонемах — элементарных единицах речи — vc.ru</i>) нам были нужны данные: тексты и их озвученные версии, чтобы обучить машину. Причём нужно, чтобы слова произносились разными голосами.</p>
      <p>Ранее для этого нанимались люди, которые слушали и расшифровывали аудио в текст. Так набирались несколько тысяч часов примеров речи, что мало для хорошей системы распознавания.</p>
      <p>YouTube интересен тем, что там огромное количество видео, где звук и текст уже имеются. Многие пользователи загружают на сайт видеоролики, в которые уже сами вшили субтитры с расшифровкой. Отчасти это делалось потому, что ролики с субтитрами поиск выдавал выше.</p>
      <p>У меня появилась идея использовать для обучения алгоритмов сотни тысяч часов готовых данных от пользователей. Проблема была лишь в том, что люди часто делают не только ошибки в тексте, но и просто ставят в субтитры случайный набор букв, чтобы получить высокий ранг при поиске. Нам пришлось поставить фильтры, которые отличали качественные данные от плохих.</p>
      <p>В итоге мы закончили разработку в 2016 году, и Closed Caption стала намного лучше распознавать речь. То, что видят пользователи сейчас, нажимая на автоматическое создание субтитров, — результат этой работы.</p>
      <h3>Проекты для людей с ограниченными возможностями</h3>
      <p>В 2017 году я перебрался из офиса Нью-Йорка в калифорнийское отделение Google.</p>
      <p>Уже тут за полгода вместе с командой я создал приложение Live Transcribe, которое использует ту же технологию перевода речи в текст, что и YouTube, но в виде отдельного приложения. С помощью него люди с проблемами слуха могут узнать, что им говорят.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/jLCwjIaPXwA" width="640" height="360" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>Система распознаёт и дополнительные звуки, о которых также пишет пользователю: лай собаки, плач ребёнка, звук гитары, стук в дверь, смех и так далее. Эта часть аудиоинформации обрабатывается на самом телефоне, а расшифровка прямой речи работает через интернет.</p>
      <p>Один из главных создателей этого приложения — Чет Гнеги. Часто сотрудники Google разрабатывают проекты для решения проблем их коллег. Гнеги видел, как я использую сервисы, где люди печатают для меня речь, которую слышат, и решил помочь.</p>
      <p>Он создал первый прототип приложения. Оно помогало нам работать вместе и в итоге выросло в отдельный проект Google под названием Live Transcribe.</p>
      <p>Ещё один проект, в котором я участвую, — Euphonia. Это приложение для людей с нестандартной речью: тех, у кого есть БАС (<i>заболевание, при котором поражается центральная нервная система — vc.ru</i>), глухих, заикающихся, людей переживших инсульт.</p>
      <p>Для этого проекта нам вновь нужно множество примеров нестандартной речи. Только в этот раз их не найти даже на YouTube. Такая речь очень индивидуальна, и здесь нужен другой подход для сбора данных.</p>
      <p>Я сам надиктовал первые 25 часов записи. Я заранее писал доклады, с которыми планировал выступать, а потом записывал их в аудио. Так я тренировал систему. Я мог выступать, а зрители видели текстовую расшифровку моих докладов.</p>
      <p>С каждым новым выступлением система всё лучше меня понимала и распознавала даже новые фразы. Сейчас мне уже не нужно писать доклады заранее — алгоритм переводит в текст абсолютно всё, что я говорю.</p>
      <p>Так стало понятно, что этот подход работает, и мы начали приглашать людей с особенной речью тоже читать и записывать текст.</p>
      <p>В случае с людьми с БАС мы начали работу с того, что дали им типичные фразы, которые они говорят, чтобы взаимодействовать, например, с Google Home. Им нужно повторить 100 фраз, чтобы натренировать систему под себя. Таким людям трудно разговаривать, и они быстро устают, поэтому мы не можем ждать от них большого количества записей.</p>
      <p>Тем не менее постепенно мы начали объединять примеры речи разных людей с этим заболеванием, чтобы в будущем создать универсальную систему. Это медленный процесс — данных слишком мало, и Euphonia — всё ещё проект-исследование, а не готовый продукт.</p>
      <figure>
        <iframe src="https://www.youtube.com/embed/OAdegPmkK-o" width="640" height="360" data-service="Youtube" scrolling="no"/>
      </figure>
      <p>Euphonia не требует соединения с интернетом, как в случае с Live Transcribe. У смартфонов небольшие вычислительные мощности, на которых сложно заниматься расшифровкой аудио. Однако команде удалось справиться с этим.</p>
      <p>Многие люди боятся, что их данные обрабатываются через интернет. Если пользователь приходит к врачу, то и он, и врач переживают, что их диалог попадёт на удалённые серверы. Тут этого нет, потому что для Euphonia не нужно подключение к сети.</p>
      <p>Сейчас мы даём ссылку, где люди с особенностями речи могут зарегистрироваться и оставить примеры своей речи. В некоторых случаях Google старается делать для них бесплатно индивидуальный распознаватель речи.</p>
      <p>Также я работаю над проектом по распознаванию языка жестов. Здесь мы работаем с визуальной информацией. Эта задача ещё труднее, чем распознавание речи. Сейчас разработка находится на начальном этапе.</p>
      <p>В языке жестов один жест может означать не отдельную букву, а целую фразу. И нам вновь нужно найти огромное количество примеров. По этому проекту мы сотрудничаем с Галлодетским университетом. В США это единственное высшее учебное заведение для слабослышащих и глухих.</p>
      <p>Кроме этого я вернулся к идее своего прибора, который переводил высокие частоты в низкие. Мои коллеги работают над его новой версией, более современной, с помощью которой получится передавать больше информации.</p>
      <figure>
        <video src="https://leonardo.osnova.io/118c98b7-7b7a-945d-43bf-0af55cfa59f0/-/format/mp4/"/>
        <figcaption>Прототип прибора перевода высоких частот в низкие</figcaption>
      </figure>
      <blockquote>Каждый год в Google проводится конкурс, где можно предложить идею, как помочь людям с ограниченными возможностями. Примерно один месяц в году сотрудники могут работать над этим проектом. <br/><br/>Потом они показывают идею, и Google отбирает лучшие. Например, четыре года назад выиграл проект, который помогал людям с трясущимися руками держать ложку, чтобы есть.<cite>Дмитрий Каневский</cite></blockquote>
    </article>
  </body>
</html>