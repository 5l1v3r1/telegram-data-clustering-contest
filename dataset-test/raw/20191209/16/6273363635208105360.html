<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://venturebeat.com/2019/12/09/google-proposes-hybrid-approach-to-ai-transfer-learning/"/>
    <meta property="og:site_name" content="VentureBeat"/>
    <meta property="article:published_time" content="2019-12-09T16:55:51+00:00"/>
    <meta property="og:title" content="Google proposes hybrid approach to AI transfer learning for medical imaging"/>
    <meta property="og:description" content="Researchers at Google study the role transfer learning plays in the development of highly accurate medical imaging machine learning models."/>
  </head>
  <body>
    <article>
      <h1>Google proposes hybrid approach to AI transfer learning for medical imaging</h1>
      <address><time datetime="2019-12-09T16:55:51+00:00">09 Dec 2019, 16:55</time> by <a rel="author" href="https://venturebeat.com/author/kylewiggers/" target="_blank">Kyle Wiggers</a></address>
      <p>Medical imaging is among the most popular application of AI and machine learning, and with good reason. Computer vision algorithms are naturally adept at spotting anomalies experts sometimes miss, in the process reducing wait times and lightening clinical workloads. Perhaps that’s why although the percentage of health care organizations that have adopted AI remains relatively low (22%) <a href="https://www.beckershospitalreview.com/quality/77-of-radiology-leaders-agree-ai-is-important-for-medical-imaging-but-adoption-is-low.html">globally</a>, the majority of practitioners (77%) believe the technology is important to the medical imaging field as a whole.</p>
      <p>Unsurprisingly, data scientists have devoted outsize time and attention to developing AI imaging models for use in health care systems, a few of which Google scientists detail in a paper accepted to this week’s NeurIPS conference in Vancouver.  In “<a href="https://arxiv.org/abs/1902.07208">Transfusion: Understanding Transfer Learning for Medical Imaging</a>,” coauthors hailing from Google Research (the R&amp;D-focused arm of Google’s business) investigate the role transfer learning plays in developing image classification algorithms.</p>
      <p>In transfer learning, a machine learning algorithm is trained in two stages. First, there’s retraining, where the algorithm is generally trained on a benchmark data set representing a diversity of categories. Next comes fine-tuning, where it is further trained on the specific target task of interest. The pretraining step helps the model to learn general features that can be reused on the target task, boosting its accuracy.</p>
      <p>According to the team, transfer learning isn’t quite the end-all, be-all of AI training techniques. In a performance evaluation that compared a range of model architectures trained to diagnose diabetic retinopathy and five different diseases from chest x-rays, a portion of which were pretrained on an open source image data set (ImageNet), they report that transfer learning didn’t “significantly” affect performance on medical imaging tasks. Moreover, a family of simple, lightweight models performed at a level comparable to the standard architectures.</p>
      <p>In a second test, the team studied the degree to which transfer learning affected the kinds of features and representations learned by the AI models. They analyzed and compared the hidden representations (i.e., representations of data learned in the model’s latent portions) in the different models trained to solve medical imaging tasks, computing similarity scores for some of the representations between models trained from scratch and those pretrained on ImageNet. The team concludes that for large models, representations learned from scratch tended to be much more similar to each other than those learned from transfer learning, while there was greater overlap between representation similarity scores in the case of smaller models.</p>
      <p>To rectify these and other issues, the team proposes a hybrid approach to transfer learning, where instead of reusing the full model architecture, only a portion of is resused and the rest is redesigned to better suit the target task. They say that it confers most of the benefits of transfer learning while further enabling flexible model design. “Transfer learning is a central technique for many domain,” wrote Google Research scientists Maithra Raghu and Chiyuan Zhang in a <a href="https://ai.googleblog.com/2019/12/understanding-transfer-learning-for.html">blog post</a>. “Many interesting open questions remain, [and we] look forward to tackling these questions in future work.”</p>
      <p>The work comes shortly after Google detailed an AI capable of <a href="https://venturebeat.com/2019/12/03/google-details-ai-that-classifies-chest-x-rays-with-human-level-accuracy/">classifying chest X-rays with human-level accuracy</a>. In another recent study, teams from the tech giant claimed to have developed a machine learning model that detects <a href="https://venturebeat.com/2019/09/13/googles-ai-detects-26-skin-conditions-as-accurately-as-dermatologists/">26 skin conditions as accurately as dermatologists</a> and a <a href="https://venturebeat.com/2019/05/20/googles-lung-cancer-detection-ai-outperforms-6-human-radiologists/">lung cancer detection AI</a> that outperformed six human radiologists.</p>
    </article>
  </body>
</html>