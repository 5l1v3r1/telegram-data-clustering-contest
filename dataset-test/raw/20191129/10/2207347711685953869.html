<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://elpais.com/tecnologia/2019/11/29/actualidad/1575023261_514971.html"/>
    <meta property="og:site_name" content="EL PAÍS"/>
    <meta property="article:published_time" content="2019-11-29T10:27:41+00:00"/>
    <meta property="og:title" content="Facebook potencia el arma de la Inteligencia Artificial para eliminar contenidos nocivos"/>
    <meta property="og:description" content="La red bloquea 1.700 millones de cuentas en un trimestre y el 90% de estas no llega a activarse"/>
  </head>
  <body>
    <article>
      <h1>Facebook potencia el arma de la Inteligencia Artificial para eliminar contenidos nocivos</h1>
      <h2>La red bloquea 1.700 millones de cuentas en un trimestre y el 90% de estas no llega a activarse</h2>
      <address><time datetime="2019-11-29T10:27:41+00:00">29 Nov 2019, 10:27</time> by <a rel="author">Patricia Tubella</a></address>
      <p><a href="https://elpais.com/elpais/2019/11/22/gente/1574414022_857097.html">Acoso, incitación al odio, propaganda terrorista</a>, cuentas falsas que manipulan la conversación entre internautas … Rastrear y bloquear los contenidos abusivos y peligrosos que buscan su canal de circulación en Facebook es la declarada prioridad de la red social con más usuarios del mundo, cuya imagen se ha visto lastrada por “errores” y “malas experiencias” como la emisión en directo de una masacre antimusulmana en Nueva Zelanda. Mediaron 17 minutos hasta que la filmación del propio asesino fue desactivada. Ocho meses y medio después, los responsables de la plataforma esgrimen el arma de la Inteligencia Artifical (AI) para lograr su objetivo de detectar automáticamente ese y otro tipo de materiales nocivos antes de que sean accesibles a otros usuarios. En el minuto cero.</p>
      <p>De los 1.700 millones de cuentas eliminadas por Facebook en el tercer trimestre del año, el 90% fueron interceptadas antes de que fueran activas, han explicado en Londres un grupo de expertos en producto e ingeniería que desarrollan las <a href="https://elpais.com/elpais/2019/11/14/ciencia/1573728249_279206.html">herramientas tecnológicas</a> de la compañía. Ese dato ha sido posible gracias a los avances en el <i>software</i> a la hora de identificar contenidos que no cumplan los estándares fijados por la plataforma. Si la firma californiana dependía en sus inicios de que los usuarios alertaran sobre contenidos dudosos, a lo que siguió un proceso de revisión por parte de su personal, en el presente son los algoritmos los encargados de la detección automática de los materiales no deseados, aunque el proceso incluye equipos humanos que verifican la aplicación del software.</p>
      <p>"El reto es continuo", subraya Simon Cross, director de producto de Facebook</p>
      <p>Hoy por hoy, la detección de antemano está por encima del 90% en muchos casos, pero resulta menos eficaz en áreas complejas como los discursos que propagan el odio y especialmente el acoso. Uno de los reiterados errores de la “máquina” –como los expertos reunidos en Londres denominan coloquialmente a la IA- es incurrir en “falsos positivos” y “falsos negativos”, es decir, en detectar irregularidades en un contenido cuando no las hay y viceversa. Es difícil que <a href="https://elpais.com/elpais/2019/11/06/opinion/1573053765_189745.html">un algoritmo pueda dirimir</a>, por ejemplo, si ciertos mensajes persiguen la difamación o sólo se trata de un broma, porque su contenido atiende al contexto. Se ha avanzado en la evaluación de otros factores, como los patrones de comportamiento de ciertos usuarios en la plataforma, pero la intervención humana sigue siendo necesaria.</p>
      <p>“El reto es continuo”, subraya Simon Cross (<i>product manager</i>), poniendo como ejemplo, básico pero ilustrativo, ciertas cuentas consagradas a la venta de cánnabis: empezaron obviando el uso de la palabra marihuana para no ser detectados; luego recurrieron a fotografías de la planta que la asemejaban al inocente brócoli, hasta que las mejoras tecnológicas en la plataforma permitieron extraer las imágenes del contexto; entonces optaron por otras alegóricas que imitaban a aquellas fotos del desayuno que tantos usuarios de Facebook son aficionados a colgar en la red…La historia de esos engaños a la “máquina” aparece interminable. Por eso, admite, “cuanto más se sofistica la tecnología más personal humano precisamos”.</p>
      <p>De los 35.000 empleados que trabajan en el ámbito de seguridad de Facebook, 15.000 están volcados en la revisión de contenidos</p>
      <p>De los 35.000 empleados que trabajan en el ámbito de seguridad de Facebook, 15.000 están volcados en la <a href="https://elpais.com/tecnologia/2019/10/31/actualidad/1572527432_620617.html">revisión de contenidos</a>. “La Inteligencia Artificial nos permite lidiar con una escala de cuentas inabarcable para los humanos, pero el fuerte de éstos es el conocimiento del contexto (los matices, los giros del lenguaje coloquial, el manejo de idiomas minoritarios) que hacen inimaginable una IA sin intervención humana”, apostilla Cross.</p>
      <p>Los humanos también son los proveedores de los <i>data</i> que alimentan la tecnología del aprendizaje automático. El pasado 15 de marzo, los algoritmos no reconocieron como material eliminable de inmediato la matanza que un supremacista blanco estaba perpetrando –y emitiendo en directo- en dos mezquitas de la localidad neozelandesa de Chistchurch. Desde entonces Facebook ha recabado la ayuda de fuerzas de seguridad de algunos países. como la Policía Metropolitana de Londres, que les ha facilitado las imágenes capturadas por las cámaras corporales de los agentes durante los entrenamientos de la lucha antiterrorista. Los expertos de Facebook aseguran que la “máquina” detecta hoy el 98% de la propaganda terrorista, pero el reto sigue siendo ese sensible 2% restante.</p>
    </article>
  </body>
</html>